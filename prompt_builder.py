"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BRAJEN PROMPT BUILDER v1.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Converts raw pre_batch data into optimized, readable prompts.

v1.1 changes:
  - _fmt_keywords(): calculates remaining from actual + target_total
    (backend sends these but NOT remaining directly)
  - Shows hard_max_this_batch so Claude knows per-batch limits
  - Clearer MUST/EXTENDED/STOP formatting

Architecture:
  SYSTEM PROMPT = Expert persona + Writing techniques
  USER PROMPT   = Structured instructions from data
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_system_prompt(pre_batch, batch_type):
    """
    Build system prompt = expert persona + writing rules.
    Uses gpt_instructions_v39 (writing techniques from API) as the core,
    with a proper persona wrapper.
    """
    pre_batch = pre_batch or {}
    gpt_instructions = pre_batch.get("gpt_instructions_v39", "")
    gpt_prompt = pre_batch.get("gpt_prompt", "")

    parts = []

    # â”€â”€ Expert persona â”€â”€
    parts.append(
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO z 10-letnim staÅ¼em. "
        "Piszesz naturalnie, merytorycznie i angaÅ¼ujÄ…co. "
        "TwÃ³j tekst nie brzmi jak AI â€” brzmi jak ekspert piszÄ…cy dla ludzi."
    )

    # â”€â”€ Writing techniques from API (if available) â”€â”€
    if gpt_instructions:
        parts.append(gpt_instructions)

    # â”€â”€ Batch context from API (structure, lengths) â”€â”€
    if gpt_prompt:
        parts.append(gpt_prompt)

    # â”€â”€ Core rules (always) â”€â”€
    parts.append("""ZASADY PISANIA:

â€¢ PASSAGE-FIRST: Pod kaÅ¼dym H2 i w intro stosuj wzorzec:
  â†’ Zdanie 1: bezpoÅ›rednia odpowiedÅº/definicja (passage-ready dla Google)
  â†’ Zdanie 2: konkret (liczba, data, przykÅ‚ad, dane)
  â†’ Zdanie 3: doprecyzowanie lub wyjÄ…tek
  Dopiero potem rozwijaj temat.

â€¢ BURSTINESS (cel: CV zdaÅ„ 0.35â€“0.45):
  20% zdaÅ„ krÃ³tkich (do 8 sÅ‚Ã³w) â€” dynamika
  55% zdaÅ„ Å›rednich (9â€“18 sÅ‚Ã³w) â€” rdzeÅ„
  25% zdaÅ„ dÅ‚ugich (19â€“28 sÅ‚Ã³w) â€” gÅ‚Ä™bia
  Mieszaj je nieregularnie, nie twÃ³rz wzorcÃ³w.

â€¢ SPACING â€” minimalna odlegÅ‚oÅ›Ä‡ miÄ™dzy powtÃ³rzeniami frazy:
  MAIN: ~60 sÅ‚Ã³w | BASIC: ~80 sÅ‚Ã³w | EXTENDED: ~120 sÅ‚Ã³w
  Nie klasteruj kilku fraz w jednym zdaniu â€” rozÅ‚Ã³Å¼ je po caÅ‚ej sekcji.

â€¢ FLEKSJA: Odmiany frazy liczÄ… siÄ™ jako jedno uÅ¼ycie!
  â€zespÃ³Å‚ turnera" = â€zespoÅ‚u turnera" = â€zespoÅ‚em turnera"
  Pisz naturalnie, uÅ¼ywaj rÃ³Å¼nych przypadkÃ³w gramatycznych.

â€¢ KAUZALNOÅšÄ†: WyjaÅ›niaj DLACZEGO (przyczynyâ†’skutki), nie tylko CO.
  Wzorce: powoduje, skutkuje, prowadzi do, zapobiega, w wyniku, poniewaÅ¼
  âŒ â€Temperatura wynosi XÂ°C." â†’ âœ… â€Wzrost temperatury powyÅ¼ej 100Â°C powoduje wrzenie, co prowadzi do parowania."

â€¢ ANTI-AI: Unikaj fraz-klisz: "warto zauwaÅ¼yÄ‡", "naleÅ¼y podkreÅ›liÄ‡", "w dzisiejszych czasach", "kluczowe jest", "nie ulega wÄ…tpliwoÅ›ci", "warto podkreÅ›liÄ‡", "naleÅ¼y pamiÄ™taÄ‡", "kluczowym aspektem", "w kontekÅ›cie". Brzmi to sztucznie.

â€¢ ANTY-POWTÃ“RZENIA: NIE powtarzaj tej samej informacji w rÃ³Å¼nych sekcjach!
  JeÅ›li zdefiniowaÅ‚eÅ› pojÄ™cie raz, NIE definiuj go ponownie. OdwoÅ‚uj siÄ™: "wspomniany wczeÅ›niej X".

â€¢ ANTY-PYTANIA-RETORYCZNE: MAX 1 pytanie retoryczne na sekcjÄ™ H2.
  âŒ "Jak to wyglÄ…da w praktyce?", "Co to oznacza?", "Czy zawsze?" â€” to szablony AI.
  âœ… UÅ¼yj zdaÅ„ przejÅ›ciowych (bridge): "To prowadzi do...", "Z tym wiÄ…Å¼e siÄ™..."

â€¢ ANTY-BRAND-STUFFING: NIE powtarzaj nazw firm/marek wiÄ™cej niÅ¼ 2x w artykule.
  JeÅ›li w encjach pojawia siÄ™ firma (np. TAURON, PGE), wspomnij jÄ… MAX 2 razy.

â€¢ ANTY-FILLER: KaÅ¼de zdanie MUSI dodawaÄ‡ nowÄ… informacjÄ™.
  âŒ â€Przewodnik elektryczny przewodzi prÄ…d." â€” truizm, oczywistoÅ›Ä‡
  âŒ â€OpÃ³r elektryczny wpÅ‚ywa na natÄ™Å¼enie." â€” banaÅ‚ bez konkretu
  âŒ â€To kluczowa rÃ³Å¼nica technologiczna." â€” puste podsumowanie
  âœ… â€MiedÅº przewodzi prÄ…d 6Ã— lepiej niÅ¼ Å¼elazo, dlatego stanowi 60% okablowania domowego."
  Zamiast powtarzaÄ‡ definicjÄ™ encji jako truizm, opisz DLACZEGO, JAK, ILE, KIEDY.

â€¢ ANTY-TRANSITIONS-FILLER: NIE uÅ¼ywaj pustych zdaÅ„ przejÅ›ciowych:
  âŒ â€To prowadzi do kolejnego aspektu."
  âŒ â€Z tym wiÄ…Å¼e siÄ™ potrzeba zrozumienia..."
  âŒ â€Wynika z tego, Å¼e..."
  âŒ â€Kolejna czÄ™Å›Ä‡ artykuÅ‚u wyjaÅ›nia..."
  Te zdania marnujÄ… miejsce. Zamiast nich â€” przejdÅº bezpoÅ›rednio do nowego tematu.
  KaÅ¼de zdanie powinno nieÅ›Ä‡ informacjÄ™, a nie zapowiadaÄ‡ jÄ….

â€¢ CYTOWANIE Å¹RÃ“DEÅ: NIE cytuj nazw encji jako ÅºrÃ³deÅ‚ informacji.
  âŒ â€Wikipedia podaje, Å¼e..." (max 1Ã— w caÅ‚ym artykule)
  âŒ â€WedÅ‚ug [nazwa encji z listy]..." â€” encje to pojÄ™cia, nie ÅºrÃ³dÅ‚a
  âŒ â€[cokolwiek] potwierdza / podaje / przywoÅ‚uje..."
  Podawaj fakty bezpoÅ›rednio, bez atrybuowania ich do ÅºrÃ³deÅ‚.
  JeÅ›li musisz wspomnieÄ‡ ÅºrÃ³dÅ‚o â€” zrÃ³b to MAX 1 raz na caÅ‚y artykuÅ‚.

â€¢ ANTY-HALUCYNACJA: NIE wymyÅ›laj danych, ktÃ³rych nie jesteÅ› pewien.
  âŒ WymyÅ›lone statystyki: â€WedÅ‚ug GUS w 2022 roku doszÅ‚o do 300 wypadkÃ³w..."
  âŒ WymyÅ›lone rozporzÄ…dzenia: â€RozporzÄ…dzenie Ministra X z dnia Y..."
  âŒ WymyÅ›lone daty/ceny/normy: â€od 1 stycznia 2026 stawka wynosi..."
  âœ… Podawaj TYLKO fakty, ktÃ³re znasz z pewnÄ… wiedzÄ….
  âœ… JeÅ›li chcesz daÄ‡ przykÅ‚ad â€” napisz ogÃ³lnie: â€np. w Polsce napiÄ™cie sieciowe wynosi 230 V"
  âœ… Zamiast wymyÅ›lonych przepisÃ³w â€” opisz zasadÄ™ ogÃ³lnÄ… bez podawania numerÃ³w ustaw.

â€¢ POLSZCZYZNA (dane NKJP â€” Narodowy Korpus JÄ™zyka Polskiego, 1,8 mld segmentÃ³w):
  â†’ PRZECINKI â€” OBOWIÄ„ZKOWE przed: Å¼e, ktÃ³ry/a/e, poniewaÅ¼, gdyÅ¼, aby, Å¼eby, jednak, lecz, ale.
    Brak przecinka przed "Å¼e" to NATYCHMIASTOWY sygnaÅ‚ sztucznoÅ›ci.
    W polszczyÅºnie przecinek wystÄ™puje CZÄ˜ÅšCIEJ niÅ¼ litera "b" (>1,47% znakÃ³w).
  â†’ KOLOKACJE â€” uÅ¼ywaj POPRAWNYCH poÅ‚Ä…czeÅ„:
    podjÄ…Ä‡ decyzjÄ™ (NIE: zrobiÄ‡ decyzjÄ™), odnieÅ›Ä‡ sukces (NIE: mieÄ‡ sukces),
    popeÅ‚niÄ‡ bÅ‚Ä…d (NIE: zrobiÄ‡ bÅ‚Ä…d), ponieÅ›Ä‡ konsekwencje (NIE: mieÄ‡ konsekwencje),
    wysoki poziom (NIE: duÅ¼y poziom), silny bÃ³l (NIE: duÅ¼y bÃ³l),
    wysokie ryzyko (NIE: duÅ¼e ryzyko), mocna kawa (NIE: silna kawa),
    rzÄ™sisty deszcz (NIE: duÅ¼y deszcz), wysunÄ…Ä‡ propozycjÄ™ (NIE: daÄ‡ propozycjÄ™),
    odgrywaÄ‡ rolÄ™ (NIE: peÅ‚niÄ‡ rolÄ™), osiÄ…gnÄ…Ä‡ porozumienie (NIE: zrobiÄ‡ porozumienie).
  â†’ DÅUGOÅšÄ† ZDAÅƒ â€” Å›rednio 10â€“15 sÅ‚Ã³w (styl publicystyczny).
    NIE pisz wszystkich zdaÅ„ jednej dÅ‚ugoÅ›ci â€” to sygnaÅ‚ AI.
  â†’ ÅšREDNIA DÅUGOÅšÄ† WYRAZU â€” 6 znakÃ³w (Â±0,5). Publicystyka=6,0, naukowe=6,4.
    Nie naduÅ¼ywaj nominalizacji ("przeprowadzanie systematycznego monitorowania").
    Mieszaj krÃ³tkie sÅ‚owa (3-4 znaki) z dÅ‚uÅ¼szymi (8-10).
  â†’ DIAKRYTYKI â€” naturalny tekst ma ~7% znakÃ³w Ä…,Ä™,Ä‡,Å‚,Å„,Ã³,Å›,Åº,Å¼.
    Tekst <5% lub >9% diakrytykÃ³w = statystycznie nienaturalny.
  â†’ DWUZNAKI â€” ch, cz, rz, sz, dz, dÅº, dÅ¼ stanowiÄ… ~3% tekstu.
  â†’ SAMOGÅOSKI â€” A,I,O,E,U,Y = 35-38% tekstu.
  â†’ Unikaj pleonazmÃ³w: "wzajemna wspÃ³Å‚praca", "aktualna sytuacja na dziÅ›", "krÃ³tkie streszczenie".
  â†’ Mieszaj przypadki gramatyczne â€” nie powtarzaj frazy w mianowniku.

â€¢ NATURALNOÅšÄ†: Pisz jak ekspert tÅ‚umaczÄ…cy temat znajomemu â€” konkretnie, bez lania wody.

â€¢ FORMAT: UÅ¼ywaj wyÅ‚Ä…cznie formatu h2:/h3: dla nagÅ‚Ã³wkÃ³w. Å»adnego markdown, HTML ani gwiazdek.""")

    return "\n\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USER PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import logging as _logging
_pb_logger = _logging.getLogger("prompt_builder")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCHEMA GUARD â€” validates critical pre_batch fields
# Ensures backend sent everything needed. Logs warnings for
# missing fields so we catch backend API changes early.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_CRITICAL_FIELDS = [
    "keywords",             # keyword list â€” without this, article has no SEO
    "main_keyword",         # primary keyword
    "batch_number",         # batch sequencing
]
_IMPORTANT_FIELDS = [
    "gpt_instructions_v39", # backend writing instructions
    "enhanced",             # enhanced_pre_batch AI data
    "h2_remaining",         # H2 structure
    "article_memory",       # context from previous batches
    "keyword_limits",       # STOP/EXCEEDED rules
    "coverage",             # keyword coverage state
]

def _schema_guard(pre_batch):
    """Validate pre_batch has critical fields. Log warnings for missing."""
    missing_critical = [f for f in _CRITICAL_FIELDS if f not in pre_batch or pre_batch[f] is None]
    missing_important = [f for f in _IMPORTANT_FIELDS if f not in pre_batch or pre_batch[f] is None]

    if missing_critical:
        _pb_logger.warning(
            f"âš ï¸ SCHEMA GUARD: Missing CRITICAL fields: {missing_critical}. "
            f"Backend may have changed API. Article quality will be degraded."
        )
    if missing_important:
        _pb_logger.info(
            f"â„¹ï¸ Schema guard: Missing optional fields: {missing_important} "
            f"(batch {pre_batch.get('batch_number', '?')})"
        )

    # Validate enhanced sub-fields if enhanced exists
    enhanced = pre_batch.get("enhanced") or {}
    if enhanced:
        expected_enhanced = [
            "smart_instructions_formatted", "causal_context",
            "information_gain", "relations_to_establish"
        ]
        missing_enh = [f for f in expected_enhanced if not enhanced.get(f)]
        if missing_enh:
            _pb_logger.info(f"â„¹ï¸ Enhanced missing: {missing_enh}")


def build_user_prompt(pre_batch, h2, batch_type, article_memory=None):
    """
    Main user prompt builder.
    Converts ALL pre_batch fields into readable, actionable instructions.
    Each section is wrapped in try/except so one bad field won't crash generation.
    """
    pre_batch = pre_batch or {}
    sections = []

    # â”€â”€ SCHEMA GUARD: validate critical fields from backend â”€â”€
    _schema_guard(pre_batch)

    formatters = [
        # â”€â”€ TIER 1: NON-NEGOTIABLE (backend hard rules) â”€â”€
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_keywords(pre_batch),           # MUST/STOP/EXCEEDED â€” hardest constraints
        lambda: _fmt_smart_instructions(pre_batch),  # enhanced_pre_batch AI instructions
        lambda: _fmt_legal_medical(pre_batch),        # YMYL â€” legal compliance, non-negotiable

        # â”€â”€ TIER 2: BACKEND WRITE INSTRUCTIONS (gpt_instructions_v39 etc.) â”€â”€
        lambda: _fmt_semantic_plan(pre_batch, h2),
        lambda: _fmt_coverage_density(pre_batch),
        lambda: _fmt_phrase_hierarchy(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_h2_remaining(pre_batch),

        # â”€â”€ TIER 3: CONTENT CONTEXT (enrichment data) â”€â”€
        lambda: _fmt_entity_salience(pre_batch),     # entity positioning rules (salience only)
        # _fmt_entities REMOVED v45.4.1 â€” gpt_instructions_v39 already contains
        # curated "ğŸ§  ENCJE:" section (max 3/batch, importanceâ‰¥0.7, with HOW hints).
        # Our version duplicated it with dirtier, unfiltered data from S1.
        # _fmt_ngrams REMOVED v45.4.1 â€” raw statistical n-grams from competitor
        # pages often contain CSS/JS artifacts ("button button", "block embed").
        # Custom GPT never sees these and produces better text without them.
        lambda: _fmt_serp_enrichment(pre_batch),
        lambda: _fmt_causal_context(pre_batch),
        lambda: _fmt_depth_signals(pre_batch),       # depth signals when previous batch scored low
        lambda: _fmt_experience_markers(pre_batch),
        lambda: _fmt_natural_polish(pre_batch),      # v50: fleksja, spacing, anti-stuffing

        # â”€â”€ TIER 4: SOFT GUIDELINES (format, style, intro) â”€â”€
        lambda: _fmt_intro_guidance(pre_batch, batch_type),
        lambda: _fmt_style(pre_batch),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception:
            pass

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION FORMATTERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_batch_header(pre_batch, h2, batch_type):
    batch_number = pre_batch.get("batch_number", 1)
    total_batches = pre_batch.get("total_planned_batches", 1)
    batch_length = pre_batch.get("batch_length") or {}

    min_w = batch_length.get("min_words", 350)
    max_w = batch_length.get("max_words", 500)

    section_length = pre_batch.get("section_length_guidance") or {}
    length_hint = ""
    if section_length:
        suggested = section_length.get("suggested_words") or section_length.get("target_words")
        if suggested:
            length_hint = f"\nSugerowana dÅ‚ugoÅ›Ä‡ tej sekcji: ~{suggested} sÅ‚Ã³w."

    h2_instruction = ""
    if batch_type not in ("INTRO", "intro"):
        h2_instruction = f"\nZaczynaj DOKÅADNIE od: h2: {h2}"

    return f"""â•â•â• BATCH {batch_number}/{total_batches} â€” {batch_type} â•â•â•
Sekcja H2: "{h2}"
DÅ‚ugoÅ›Ä‡: {min_w}-{max_w} sÅ‚Ã³w{length_hint}{h2_instruction}"""


def _fmt_intro_guidance(pre_batch, batch_type):
    if batch_type not in ("INTRO", "intro"):
        return ""
    guidance = pre_batch.get("intro_guidance", "")

    main_kw = pre_batch.get("main_keyword") or {}
    kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)

    parts = ["â•â•â• WPROWADZENIE (WSTÄ˜P ARTYKUÅU) â•â•â•",
             "To jest PIERWSZY batch â€” piszesz WSTÄ˜P artykuÅ‚u.",
             "MUSISZ:",
             f'  1. WpleÄ‡ frazÄ™ gÅ‚Ã³wnÄ… ("{kw_name}") w PIERWSZE zdanie' if kw_name else "  1. FrazÄ™ gÅ‚Ã³wnÄ… umieÅ›Ä‡ w pierwszym zdaniu",
             "  2. ZaczÄ…Ä‡ od angaÅ¼ujÄ…cego haka (hook) â€” pytanie, statystyka, scenariusz",
             "  3. PrzedstawiÄ‡ GÅÃ“WNÄ„ TEZÄ˜ artykuÅ‚u w 1-2 zdaniach",
             "  4. ZapowiedzieÄ‡ co czytelnik znajdzie dalej (bez listy H2!)",
             "  5. NIE zaczynaÄ‡ od definicji ani od 'W dzisiejszych czasach...'",
             "  6. NIE dodawaÄ‡ nagÅ‚Ã³wka h2: â€” wstÄ™p nie ma nagÅ‚Ã³wka",
             "  7. UtrzymaÄ‡ zwiÄ™zÅ‚oÅ›Ä‡ â€” wstÄ™p to 80-150 sÅ‚Ã³w"]

    if guidance:
        if isinstance(guidance, dict):
            hook = guidance.get("hook", "")
            angle = guidance.get("angle", "")
            if hook:
                parts.append(f"\nHak otwierajÄ…cy: {hook}")
            if angle:
                parts.append(f"KÄ…t artykuÅ‚u: {angle}")
        else:
            parts.append(f"\n{guidance}")

    return "\n".join(parts)


def _fmt_smart_instructions(pre_batch):
    """Smart instructions from enhanced_pre_batch â€” THE most valuable field."""
    enhanced = pre_batch.get("enhanced") or {}
    smart = enhanced.get("smart_instructions_formatted", "")
    if smart:
        return f"â•â•â• INSTRUKCJE DLA TEGO BATCHA â•â•â•\n{smart[:1000]}"
    return ""


def _parse_target_max(target_total_str):
    """
    Parse target_max from backend's target_total field.
    Backend sends target_total as "min-max" string (e.g., "2-6").
    Returns max value as int, or 0 if unparseable.
    """
    if not target_total_str:
        return 0
    if isinstance(target_total_str, (int, float)):
        return int(target_total_str)
    try:
        parts = str(target_total_str).replace("x", "").split("-")
        if len(parts) >= 2:
            return int(parts[-1].strip())
        return int(parts[0].strip())
    except (ValueError, IndexError):
        return 0


def _fmt_keywords(pre_batch):
    """
    Format keywords section with CALCULATED remaining_max.
    
    v1.1: Backend sends actual (current uses) and target_total ("min-max")
    but NOT remaining. We calculate: remaining = target_max - actual.
    Also shows hard_max_this_batch so Claude knows per-batch limits.
    """
    keywords_info = pre_batch.get("keywords") or {}
    keyword_limits = pre_batch.get("keyword_limits") or {}
    soft_caps = pre_batch.get("soft_cap_recommendations") or {}

    # â”€â”€ MUST USE (with calculated remaining) â”€â”€
    must_raw = keywords_info.get("basic_must_use", [])
    must_lines = []
    for kw in must_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            
            # Calculate remaining from actual + target_total
            actual = kw.get("actual", kw.get("actual_uses", kw.get("current_count", 0)))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            hard_max = kw.get("hard_max_this_batch", "")
            use_range = kw.get("use_this_batch", "")
            
            # Explicit remaining from backend (if sent), otherwise calculate
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            
            # Build descriptive line
            parts_line = [f'"{name}"']
            if remaining:
                parts_line.append(f"zostaÅ‚o {remaining}Ã— ogÃ³Å‚em")
            if hard_max:
                parts_line.append(f"max {hard_max}Ã— w tym batchu")
            elif use_range:
                parts_line.append(f"cel: {use_range}Ã— w tym batchu")
            
            must_lines.append(f'  â€¢ {" â€” ".join(parts_line)}')
        else:
            must_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ EXTENDED (with remaining) â”€â”€
    ext_raw = keywords_info.get("extended_this_batch", [])
    ext_lines = []
    for kw in ext_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            actual = kw.get("actual", kw.get("actual_uses", 0))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            
            line = f'  â€¢ "{name}"'
            if remaining:
                line += f" â€” zostaÅ‚o {remaining}Ã—"
            ext_lines.append(line)
        else:
            ext_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ STOP â”€â”€
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_lines = []
    for s in stop_raw:
        if isinstance(s, dict):
            name = s.get("keyword", "")
            current = s.get("current_count", s.get("current", s.get("actual", "?")))
            max_c = s.get("max_count", s.get("max", s.get("target_max", "?")))
            stop_lines.append(f'  â€¢ "{name}" (juÅ¼ {current}Ã—, limit {max_c}) â€” STOP!')
        else:
            stop_lines.append(f'  â€¢ "{s}"')

    # â”€â”€ CAUTION â”€â”€
    caution_raw = keyword_limits.get("caution_keywords") or []
    caution_lines = []
    for c in caution_raw:
        if isinstance(c, dict):
            name = c.get("keyword", "")
            current = c.get("current_count", c.get("current", c.get("actual", "")))
            max_c = c.get("max_count", c.get("max", c.get("target_max", "")))
            line = f'  â€¢ "{name}"'
            if current and max_c:
                line += f" ({current}/{max_c})"
            line += " â€” max 1Ã— w tym batchu"
            caution_lines.append(line)
        else:
            caution_lines.append(f'  â€¢ "{c}" â€” max 1Ã—')

    # â”€â”€ SOFT CAPS â”€â”€
    soft_notes = []
    if soft_caps:
        for kw_name, info in soft_caps.items():
            if isinstance(info, dict):
                action = info.get("action", "")
                if action and action != "OK":
                    soft_notes.append(f'  â„¹ï¸ "{kw_name}": {action}')

    # â”€â”€ Build section â”€â”€
    parts = ["â•â•â• FRAZY KLUCZOWE â•â•â•"]

    if must_lines:
        parts.append("ğŸ”´ OBOWIÄ„ZKOWE (wpleÄ‡ naturalnie w tekst):")
        parts.extend(must_lines)

    if ext_lines:
        parts.append("\nğŸŸ¡ ROZSZERZONE (uÅ¼yj jeÅ›li pasujÄ… do kontekstu):")
        parts.extend(ext_lines)

    if stop_lines:
        parts.append("\nğŸ›‘ STOP â€” NIE UÅ»YWAJ (przekroczone limity!):")
        parts.extend(stop_lines)

    if caution_lines:
        parts.append("\nâš ï¸ OSTROÅ»NIE â€” uÅ¼yj max 1Ã— lub pomiÅ„:")
        parts.extend(caution_lines)

    if soft_notes:
        parts.append("")
        parts.extend(soft_notes)

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_semantic_plan(pre_batch, h2):
    plan = pre_batch.get("semantic_batch_plan") or {}
    if not plan:
        return ""

    parts = ["â•â•â• CO PISAÄ† W TEJ SEKCJI â•â•â•"]

    h2_coverage = plan.get("h2_coverage") or {}
    for h2_name, info in h2_coverage.items():
        if isinstance(info, dict):
            angle = info.get("semantic_angle", "")
            must = info.get("must_phrases", [])
            if angle:
                parts.append(f'KÄ…t semantyczny: {angle}')
            if must:
                phrases = ", ".join(f'"{p}"' for p in must[:5])
                parts.append(f'ObowiÄ…zkowe frazy w tej sekcji: {phrases}')

    density_targets = plan.get("density_targets") or {}
    overall = density_targets.get("overall")
    if overall:
        parts.append(f'Docelowa gÄ™stoÅ›Ä‡ fraz: {overall}%')

    direction = plan.get("content_direction") or plan.get("writing_direction", "")
    if direction:
        parts.append(f'Kierunek treÅ›ci: {direction}')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_entity_salience(pre_batch):
    """Entity salience instructions â€” grammatical positioning, hierarchy.
    
    Based on:
    - Patent US10235423B2 (entity metrics)
    - Patent US9251473B2 (salient items in documents)
    - Dunietz & Gillick (2014) entity salience research
    - Google Cloud NLP API salience scoring
    
    v47.0: Also includes backend placement instructions from competitor analysis
    (entity_salience.py in gpt-ngram-api: salience scoring, co-occurrence, placement)
    
    Data sources:
    - pre_batch["_entity_salience_instructions"] â€” local positioning rules (from entity_salience.py frontend)
    - pre_batch["_backend_placement_instruction"] â€” backend placement from competitor analysis
    - pre_batch["_concept_instruction"] â€” topical concepts agent instruction
    - pre_batch["_must_cover_concepts"] â€” concept entities that must be covered
    """
    parts = []
    
    # 1. Local salience positioning rules
    local_instructions = pre_batch.get("_entity_salience_instructions", "")
    if local_instructions:
        parts.append(local_instructions)
    
    # 2. v47.0: Backend placement instructions (from gpt-ngram-api competitor analysis)
    backend_placement = pre_batch.get("_backend_placement_instruction", "")
    if backend_placement:
        parts.append("â•â•â• ROZMIESZCZENIE ENCJI (z analizy konkurencji) â•â•â•")
        parts.append(backend_placement)
    
    # 3. v47.0: Concept instruction + must-cover concepts
    concept_instr = pre_batch.get("_concept_instruction", "")
    must_concepts = pre_batch.get("_must_cover_concepts", [])
    if concept_instr:
        parts.append(concept_instr)
    elif must_concepts:
        # Build instruction from concept list if no agent instruction provided
        concept_names = [c.get("text", c) if isinstance(c, dict) else str(c) for c in must_concepts[:10]]
        parts.append(
            "â•â•â• POJÄ˜CIA TEMATYCZNE (z analizy konkurencji) â•â•â•\n"
            f"NastÄ™pujÄ…ce pojÄ™cia pojawiajÄ… siÄ™ u konkurencji â€” wpleÄ‡ naturalnie w tekst:\n"
            f"{', '.join(concept_names)}"
        )
    
    # 4. v50: Co-occurrence pairs â€” encje ktÃ³re MUSZÄ„ byÄ‡ blisko siebie
    cooc_pairs = pre_batch.get("_cooccurrence_pairs") or []
    if cooc_pairs:
        cooc_lines = []
        for pair in cooc_pairs[:8]:
            if isinstance(pair, dict):
                e1 = pair.get("entity1", pair.get("source", ""))
                e2 = pair.get("entity2", pair.get("target", ""))
                if e1 and e2:
                    cooc_lines.append(f'  â€¢ "{e1}" + "{e2}" â€” w tym samym akapicie')
            elif isinstance(pair, str) and "+" in pair:
                cooc_lines.append(f"  â€¢ {pair} â€” w tym samym akapicie")
        if cooc_lines:
            parts.append(
                "â•â•â• WSPÃ“ÅWYSTÄ˜POWANIE ENCJI (co-occurrence) â•â•â•\n"
                "NastÄ™pujÄ…ce pary encji czÄ™sto pojawiajÄ… siÄ™ RAZEM u konkurencji.\n"
                "UmieÅ›Ä‡ je W TYM SAMYM AKAPICIE â€” bliskoÅ›Ä‡ buduje kontekst semantyczny:\n"
                + "\n".join(cooc_lines)
            )
    
    # 5. v50: First paragraph entities â€” encje z pierwszego akapitu top10
    first_para_ents = pre_batch.get("_first_paragraph_entities") or []
    if first_para_ents:
        fp_names = []
        for ent in first_para_ents[:6]:
            name = ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent)
            if name:
                fp_names.append(f'"{name}"')
        if fp_names:
            parts.append(
                "PIERWSZY AKAPIT â€” encje tematyczne:\n"
                f"WprowadÅº w pierwszym akapicie: {', '.join(fp_names)}.\n"
                "âš ï¸ To POJÄ˜CIA do opisania, NIE ÅºrÃ³dÅ‚a do cytowania. Nie pisz '[encja] podaje/potwierdza...'."
            )
    
    # 6. v50: H2 entities â€” encje tematyczne do rozmieszczenia w H2
    h2_ents = pre_batch.get("_h2_entities") or []
    if h2_ents:
        h2_names = []
        for ent in h2_ents[:8]:
            name = ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent)
            if name:
                h2_names.append(f'"{name}"')
        if h2_names:
            parts.append(
                "ENCJE TEMATYCZNE W H2:\n"
                f"RozÅ‚Ã³Å¼ w tekÅ›cie: {', '.join(h2_names)}.\n"
                "âš ï¸ To POJÄ˜CIA do opisania, NIE ÅºrÃ³dÅ‚a. Nie pisz '[encja] podaje...'."
            )
    
    return "\n\n".join(parts) if parts else ""


# _fmt_entities REMOVED v45.4.1 â†’ v50 cleanup: function deleted.
# gpt_instructions_v39 already contains curated "ğŸ§  ENCJE:" section
# (max 3/batch, importanceâ‰¥0.7, with HOW hints). Our version duplicated it
# with dirtier, unfiltered data from S1.

# _fmt_ngrams REMOVED v45.4.1 â†’ v50 cleanup: function deleted.
# Raw statistical n-grams from competitor pages often contain CSS/JS artifacts
# ("button button", "block embed"). Custom GPT produces better text without them.


def _fmt_serp_enrichment(pre_batch):
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}

    paa = (serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or [])
    lsi = (serp.get("lsi_keywords") or [])

    if not paa and not lsi:
        return ""

    parts = ["â•â•â• WZBOGACENIE Z SERP â•â•â•"]

    if paa:
        parts.append("Pytania ktÃ³re ludzie zadajÄ… w Google (PAA) â€” odpowiedz na 1-2 w tekÅ›cie:")
        for q in paa[:5]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                parts.append(f'  â“ {q_text}')

    if lsi:
        lsi_names = [l.get("keyword", l) if isinstance(l, dict) else l for l in lsi[:8]]
        parts.append(f'\nFrazy LSI (bliskoznaczne, wpleÄ‡ naturalnie): {", ".join(lsi_names)}')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_continuation(pre_batch):
    continuation = pre_batch.get("continuation_v39") or {}
    enhanced = pre_batch.get("enhanced") or {}
    cont_ctx = enhanced.get("continuation_context") or {}

    last_h2 = cont_ctx.get("last_h2") or continuation.get("last_h2", "")
    last_ending = cont_ctx.get("last_paragraph_ending") or continuation.get("last_paragraph_ending", "")
    last_topic = cont_ctx.get("last_topic") or continuation.get("last_topic", "")
    transition_hint = continuation.get("transition_hint", "")

    if not last_h2 and not last_ending:
        return ""

    parts = ["â•â•â• KONTYNUACJA â•â•â•",
             "Poprzedni batch zakoÅ„czyÅ‚ siÄ™ na:"]

    if last_h2:
        parts.append(f'  Ostatni H2: "{last_h2}"')
    if last_ending:
        ending_preview = last_ending[:150] + ("..." if len(last_ending) > 150 else "")
        parts.append(f'  Ostatnie zdanie: "{ending_preview}"')
    if last_topic:
        parts.append(f'  Temat: {last_topic}')

    parts.append("\nZacznij PÅYNNIE â€” nawiÄ…Å¼ do poprzedniego wÄ…tku, ale nie powtarzaj zakoÅ„czenia.")
    if transition_hint:
        parts.append(f'Sugerowane przejÅ›cie: {transition_hint}')

    return "\n".join(parts)


def _fmt_article_memory(article_memory):
    if not article_memory:
        return ""

    parts = ["â•â•â• PAMIÄ˜Ä† ARTYKUÅU (KRYTYCZNE â€” nie powtarzaj!) â•â•â•"]

    if isinstance(article_memory, dict):
        topics = article_memory.get("topics_covered") or article_memory.get("covered_topics") or []
        if topics:
            parts.append("Sekcje juÅ¼ napisane:")
            for t in topics[:10]:
                if isinstance(t, str):
                    parts.append(f'  âœ“ {t}')
                elif isinstance(t, dict):
                    parts.append(f'  âœ“ {t.get("topic", t.get("h2", ""))}')

        facts = article_memory.get("key_facts_used") or article_memory.get("facts", [])
        # v50.5 FIX 30: Also extract key_points and avoid_repetition from AI memory
        key_points = article_memory.get("key_points") or []
        avoid_rep = article_memory.get("avoid_repetition") or []
        
        all_facts = list(facts) + list(key_points)
        if all_facts:
            parts.append("\nFakty/definicje juÅ¼ podane (NIE POWTARZAJ â€” odwoÅ‚uj siÄ™: 'wspomniany wczeÅ›niej'):")
            for f in all_facts[:12]:
                parts.append(f'  â€¢ {f}' if isinstance(f, str) else f'  â€¢ {json.dumps(f, ensure_ascii=False)[:100]}')

        if avoid_rep:
            parts.append("\nâ›” KONKRETNE TEMATY DO UNIKANIA (AI memory):")
            for r in avoid_rep[:8]:
                parts.append(f'  âŒ {r}')

        phrases_used = article_memory.get("phrases_used") or {}
        if phrases_used:
            high_use = [(k, v) for k, v in phrases_used.items()
                        if isinstance(v, (int, float)) and v >= 3]
            if high_use:
                parts.append("\nFrazy juÅ¼ czÄ™sto uÅ¼yte (ogranicz):")
                for name, count in high_use[:8]:
                    parts.append(f'  â€¢ "{name}" â€” juÅ¼ {count}Ã—')
        
        # v50.5 FIX 30: Add strong anti-repetition instruction
        if topics and len(topics) >= 2:
            parts.append(
                "\nâš ï¸ ZASADA ANTY-POWTÃ“RZEÅƒ: JeÅ›li pojÄ™cie (np. prawo Ohma, definicja ampera) "
                "zostaÅ‚o ZDEFINIOWANE w poprzedniej sekcji, NIE definiuj go ponownie. "
                "Zamiast tego: uÅ¼yj go w nowym kontekÅ›cie lub odnieÅ› siÄ™ krÃ³tko: "
                "'zgodnie z omÃ³wionym wczeÅ›niej prawem Ohma'. "
                "PowtÃ³rzenie definicji = utrata punktÃ³w jakoÅ›ci."
            )
    elif isinstance(article_memory, str):
        parts.append(article_memory[:1500])

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_coverage_density(pre_batch):
    coverage = pre_batch.get("coverage") or {}
    density = pre_batch.get("density") or {}
    main_kw = pre_batch.get("main_keyword") or {}
    keyword_tracking = pre_batch.get("keyword_tracking") or {}

    if not coverage and not density and not main_kw:
        return ""

    parts = ["â•â•â• STATUS POKRYCIA FRAZ â•â•â•"]

    if main_kw:
        kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
        synonyms = main_kw.get("synonyms", []) if isinstance(main_kw, dict) else []
        if kw_name:
            parts.append(f'HasÅ‚o gÅ‚Ã³wne: "{kw_name}"')
        if synonyms:
            parts.append(f'Synonimy (uÅ¼ywaj zamiennie): {", ".join(synonyms[:5])}')

    current_cov = coverage.get("current", coverage.get("current_coverage"))
    target_cov = coverage.get("target", coverage.get("target_coverage"))
    if current_cov is not None and target_cov is not None:
        parts.append(f'\nPokrycie fraz: {current_cov}% z docelowych {target_cov}%')

    missing = coverage.get("missing_phrases") or coverage.get("uncovered") or []
    if missing:
        parts.append("âš ï¸ BRAKUJÄ„CE FRAZY â€” wpleÄ‡ w tym batchu:")
        for m in missing[:8]:
            name = m.get("keyword", m) if isinstance(m, dict) else m
            parts.append(f'  â†’ "{name}"')

    if density:
        current_d = density.get("current")
        target_range = density.get("target_range") or []
        if current_d is not None:
            range_str = f'{target_range[0]}-{target_range[1]}%' if len(target_range) >= 2 else "1.5-2.5%"
            status = "âœ… w normie" if target_range and len(target_range) >= 2 and target_range[0] <= current_d <= target_range[1] else "âš ï¸ do korekty"
            parts.append(f'\nGÄ™stoÅ›Ä‡ fraz: {current_d}% (cel: {range_str}) {status}')

        overused_d = density.get("overused") or []
        if overused_d:
            over_names = ", ".join(f'"{o}"' if isinstance(o, str) else f'"{o.get("keyword", "")}"' for o in overused_d[:5])
            parts.append(f'NaduÅ¼ywane: {over_names} â€” uÅ¼yj synonimÃ³w')

    if keyword_tracking:
        total_kw = keyword_tracking.get("total_keywords", 0)
        covered_kw = keyword_tracking.get("covered", 0)
        if total_kw and covered_kw:
            parts.append(f'\nTracking: {covered_kw}/{total_kw} fraz pokrytych')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_style(pre_batch):
    style = pre_batch.get("style_instructions") or pre_batch.get("style_instructions_v39") or {}

    if not style:
        return ""

    parts = ["â•â•â• STYL â•â•â•"]

    if isinstance(style, dict):
        tone = style.get("tone", "")
        if tone:
            parts.append(f'Ton: {tone}')

        para_len = style.get("paragraph_length", "")
        if para_len:
            parts.append(f'DÅ‚ugoÅ›Ä‡ akapitÃ³w: {para_len} sÅ‚Ã³w')

        forbidden = style.get("forbidden_phrases") or style.get("avoid_phrases") or []
        if forbidden:
            parts.append(f'ZAKAZANE zwroty: {", ".join(f"{f}" for f in forbidden[:8])}')

        preferred = style.get("preferred_phrases") or style.get("use_phrases") or []
        if preferred:
            parts.append(f'Preferowane zwroty: {", ".join(preferred[:5])}')

        persona = style.get("persona", "")
        if persona:
            parts.append(f'Perspektywa: {persona}')
    elif isinstance(style, str):
        parts.append(style[:500])

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_legal_medical(pre_batch):
    legal_ctx = pre_batch.get("legal_context") or {}
    medical_ctx = pre_batch.get("medical_context") or {}
    ymyl_enrich = pre_batch.get("_ymyl_enrichment") or {}
    ymyl_intensity = pre_batch.get("_ymyl_intensity", "full")

    parts = []

    # v50: For "light" YMYL â€” DON'T inject full legal/medical framework
    if ymyl_intensity == "light":
        light_note = pre_batch.get("_light_ymyl_note", "")
        if light_note:
            parts.append("â•â•â• ASPEKT REGULACYJNY (peryferyjny â€” NIE gÅ‚Ã³wny temat!) â•â•â•")
            parts.append(f"  {light_note}")
            parts.append("  âš ï¸ OGRANICZENIE: Wspomnij o regulacjach MAX 1-2 razy w CAÅYM artykule.")
            parts.append("  NIE cytuj artykuÅ‚Ã³w ustaw, NIE dodawaj sygnatur orzeczeÅ„,")
            parts.append("  NIE dodawaj disclaimera o konsultacji z prawnikiem/lekarzem.")
            parts.append("  ArtykuÅ‚ jest EDUKACYJNY/TECHNICZNY, nie prawniczy/medyczny.")
        return "\n".join(parts) if parts else ""

    if legal_ctx and legal_ctx.get("active"):
        parts.append("â•â•â• KONTEKST PRAWNY (YMYL) â•â•â•")
        parts.append("Ten artykuÅ‚ dotyczy tematyki prawnej. MUSISZ:")
        parts.append("  1. CytowaÄ‡ realne przepisy i orzeczenia (podane niÅ¼ej)")
        parts.append("  2. DodaÄ‡ disclaimer o konsultacji z prawnikiem")
        parts.append("  3. NIE wymyÅ›laÄ‡ sygnatur ani dat orzeczeÅ„")
        
        # v47.2: Claude's enrichment â€” specific articles and concepts
        legal_enrich = ymyl_enrich.get("legal", {})
        if legal_enrich.get("articles"):
            parts.append("")
            parts.append("PODSTAWA PRAWNA (kluczowe przepisy):")
            for art in legal_enrich["articles"][:5]:
                parts.append(f"  â€¢ {art}")
        if legal_enrich.get("acts"):
            parts.append(f"  Ustawy: {', '.join(legal_enrich['acts'][:4])}")
        if legal_enrich.get("key_concepts"):
            parts.append(f"  Kluczowe pojÄ™cia: {', '.join(legal_enrich['key_concepts'][:6])}")
        
        parts.append("")
        parts.append("FORMATY CYTOWAÅƒ PRAWNYCH:")
        parts.append('  â€¢ Przepisy: "art. 13 Â§ 1 k.c.", "art. 58 Â§ 2 k.r.o."')
        parts.append('  â€¢ Wyroki: "wyrok SN z 12.03.2021, III CZP 45/19"')
        parts.append('  â€¢ Dziennik Ustaw: "Dz.U. 2023 poz. 1234"')
        parts.append('  Causal legal: "niedopeÅ‚nienie obowiÄ…zku skutkuje...", "brak zgÅ‚oszenia prowadzi do..."')

        instruction = legal_ctx.get("legal_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        judgments = legal_ctx.get("top_judgments") or []
        if judgments:
            parts.append("\nOrzeczenia do zacytowania:")
            for j in judgments[:3]:
                if isinstance(j, dict):
                    sig = j.get("signature", j.get("caseNumber", ""))
                    court = j.get("court", j.get("courtName", ""))
                    date = j.get("date", j.get("judgmentDate", ""))
                    matched = j.get("matched_article", "")
                    line = f'  â€¢ {sig} â€” {court} ({date})'
                    if matched:
                        line += f' [dot. {matched}]'
                    parts.append(line)

        citation_hint = legal_ctx.get("citation_hint", "")
        if citation_hint:
            parts.append(f'\n{citation_hint}')

    if medical_ctx and medical_ctx.get("active"):
        if parts:
            parts.append("")
        parts.append("â•â•â• KONTEKST MEDYCZNY (YMYL) â•â•â•")
        parts.append("Ten artykuÅ‚ dotyczy tematyki zdrowotnej. MUSISZ:")
        parts.append("  1. CytowaÄ‡ ÅºrÃ³dÅ‚a naukowe (podane niÅ¼ej)")
        parts.append("  2. NIE wymyÅ›laÄ‡ statystyk ani nazw badaÅ„")
        parts.append("  3. DodaÄ‡ informacjÄ™ o konsultacji z lekarzem")
        
        # v47.2: Claude's enrichment â€” specialization, evidence guidelines
        med_enrich = ymyl_enrich.get("medical", {})
        if med_enrich.get("specialization"):
            parts.append(f"\n  Specjalizacja: {med_enrich['specialization']}")
        if med_enrich.get("condition"):
            cond = med_enrich["condition"]
            latin = med_enrich.get("condition_latin", "")
            icd = med_enrich.get("icd10", "")
            parts.append(f"  Choroba/stan: {cond}" + (f" ({latin})" if latin else "") + (f" [ICD-10: {icd}]" if icd else ""))
        if med_enrich.get("key_drugs"):
            parts.append(f"  Kluczowe leki: {', '.join(med_enrich['key_drugs'][:5])}")
        if med_enrich.get("evidence_note"):
            parts.append(f"\n  âš ï¸ WYTYCZNE: {med_enrich['evidence_note']}")
        
        parts.append("")
        parts.append("FORMATY CYTOWAÅƒ MEDYCZNYCH:")
        parts.append('  â€¢ "Smith i wsp. (2023)", "Kowalski et al. (2024)"')
        parts.append('  â€¢ "PMID:12345678", "DOI:10.1000/xyz"')
        parts.append("")
        parts.append("HIERARCHIA DOWODÃ“W (cytuj najwyÅ¼szy dostÄ™pny):")
        parts.append("  1. Meta-analiza / PrzeglÄ…d systematyczny (najsilniejszy)")
        parts.append("  2. RCT (badanie randomizowane)")
        parts.append("  3. Badanie kohortowe")
        parts.append("  4. Opis przypadku")
        parts.append("  5. Opinia eksperta (najsÅ‚abszy)")
        parts.append('  Causal medical: "nieleczone prowadzi do...", "brak terapii skutkuje..."')

        instruction = medical_ctx.get("medical_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        publications = medical_ctx.get("top_publications") or []
        if publications:
            parts.append("\nPublikacje do zacytowania:")
            for p in publications[:5]:
                if isinstance(p, dict):
                    title = p.get("title", "")[:80]
                    authors = p.get("authors", "")[:40]
                    year = p.get("year", "")
                    pmid = p.get("pmid", "")
                    parts.append(f'  â€¢ {authors} ({year}): "{title}" PMID:{pmid}')

    return "\n".join(parts) if parts else ""


def _fmt_experience_markers(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    markers = enhanced.get("experience_markers") or []

    if not markers:
        return ""

    parts = ["â•â•â• SYGNAÅY DOÅšWIADCZENIA (E-E-A-T) â•â•â•",
             "WpleÄ‡ min 1 sygnaÅ‚, Å¼e autor MA doÅ›wiadczenie z tematem:"]

    for m in markers[:5]:
        if isinstance(m, str):
            parts.append(f'  â€¢ {m}')
        elif isinstance(m, dict):
            parts.append(f'  â€¢ {m.get("marker", m.get("text", ""))}')

    return "\n".join(parts)


def _fmt_causal_context(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    causal = enhanced.get("causal_context", "")
    info_gain = enhanced.get("information_gain", "")

    parts = []

    if causal:
        parts.append("â•â•â• KONTEKST PRZYCZYNOWO-SKUTKOWY â•â•â•")
        parts.append(f'{causal[:500]}')

    if info_gain:
        if parts:
            parts.append("")
        parts.append("â•â•â• INFORMATION GAIN (przewaga nad konkurencjÄ…) â•â•â•")
        parts.append(f'{info_gain[:500]}')

    return "\n".join(parts) if parts else ""


def _fmt_depth_signals(pre_batch):
    """Depth signals â€” inject when previous batch scored low on depth
    or always for FULL YMYL content.
    
    v50: Only force for full YMYL intensity, not light.
    Based on 10 depth signals from GPT prompt with weights.
    """
    last_depth = pre_batch.get("_last_depth_score")
    is_ymyl = pre_batch.get("_is_ymyl", False)
    ymyl_intensity = pre_batch.get("_ymyl_intensity", "none")
    is_full_ymyl = is_ymyl and ymyl_intensity == "full"
    
    # Only force depth for FULL YMYL, not light
    threshold = 40 if is_full_ymyl else 30
    if last_depth is not None and last_depth >= threshold and not is_full_ymyl:
        return ""
    
    # If no depth data at all and not full YMYL, skip
    if last_depth is None and not is_full_ymyl:
        return ""
    
    parts = ["â•â•â• SYGNAÅY GÅÄ˜BOKOÅšCI (dodaj od najwyÅ¼szej wagi) â•â•â•"]
    
    if last_depth is not None:
        parts.append(f"âš ï¸ Ostatni batch: depth {last_depth}/100 (prÃ³g: {threshold}). Dodaj wiÄ™cej konkretÃ³w!")
    
    parts.append("")
    # v50: Legal references only for FULL YMYL
    if is_full_ymyl:
        parts.append("WAGA 2.5: referencje prawne (art. k.c., wyroki SN, Dz.U.) + naukowe (PMID, DOI, badania)")
    parts.append('WAGA 2.0: konkretne liczby (kwoty PLN, %, okresy â€” NIE "okoÅ‚o")')
    parts.append('WAGA 1.8: nazwane instytucje (konkretny sÄ…d/urzÄ…d, NIE "wÅ‚aÅ›ciwy sÄ…d") + praktyczne porady (w praktyce, czÄ™sty bÅ‚Ä…d)')
    parts.append("WAGA 1.5: wyjaÅ›nienia przyczynowe (poniewaÅ¼, w wyniku) + wyjÄ…tki (z wyjÄ…tkiem, chyba Å¼e) + konkretne daty")
    parts.append("WAGA 1.2: porÃ³wnania (w odrÃ³Å¼nieniu od) | WAGA 1.0: kroki procedur (najpierw/nastÄ™pnie)")
    
    return "\n".join(parts)


def _fmt_natural_polish(pre_batch):
    """v50: Natural Polish writing instructions â€” fleksja, spacing, anti-stuffing.

    Based on natural_polish_instructions.py (master-seo-api-main).
    Inlined here because prompt_builder runs in Brajn, not master.
    
    Prevents keyword stuffing by teaching Claude that:
    1. Polish inflected forms count as the same keyword
    2. Minimum spacing between repetitions is required
    3. Max 2 uses of same phrase per paragraph
    """
    # Get keywords from pre_batch
    keywords_info = pre_batch.get("keywords") or {}
    must_kw = keywords_info.get("basic_must_use") or []
    ext_kw = keywords_info.get("extended_this_batch") or []

    all_kw = []
    for kw in must_kw + ext_kw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            kw_type = kw.get("type", "BASIC").upper()
        elif isinstance(kw, str):
            name = kw
            kw_type = "BASIC"
        else:
            continue
        if name:
            all_kw.append((name, kw_type))

    if not all_kw:
        return ""

    # Spacing rules
    SPACING = {"MAIN": 60, "BASIC": 80, "EXTENDED": 120}

    parts = ["â•â•â• NATURALNY POLSKI â€” ANTY-STUFFING â•â•â•"]

    parts.append(
        "ğŸ”„ FLEKSJA: Odmiany frazy liczÄ… siÄ™ jako jedno uÅ¼ycie!\n"
        '   "zespÃ³Å‚ turnera" = "zespoÅ‚u turnera" = "zespoÅ‚em turnera"\n'
        "   Pisz naturalnie, uÅ¼ywaj rÃ³Å¼nych przypadkÃ³w gramatycznych.\n"
        "   NIE MUSISZ powtarzaÄ‡ frazy w mianowniku â€” system zaliczy kaÅ¼dÄ… odmianÄ™."
    )

    spacing_lines = []
    for name, kw_type in all_kw[:8]:
        spacing = SPACING.get(kw_type, 80)
        spacing_lines.append(f'  â€¢ "{name}" ({kw_type}) â€” min {spacing} sÅ‚Ã³w miÄ™dzy powtÃ³rzeniami')
    if spacing_lines:
        parts.append("ğŸ“ ODSTÄ˜PY MIÄ˜DZY POWTÃ“RZENIAMI:\n" + "\n".join(spacing_lines))

    parts.append(
        "âš ï¸ ZASADY:\n"
        "  â€¢ Max 2Ã— ta sama fraza w jednym akapicie\n"
        "  â€¢ RozkÅ‚adaj frazy RÃ“WNOMIERNIE w tekÅ›cie (nie grupuj na poczÄ…tku/koÅ„cu)\n"
        "  â€¢ Zamiast powtÃ³rzenia uÅ¼yj: synonimu, zaimka, opisu ('ta choroba', 'omawiany zespÃ³Å‚')\n"
        "  â€¢ Podmiot â†’ dopeÅ‚nienie â†’ synonim â†’ kolejny akapit â†’ ponownie fraza"
    )

    return "\n".join(parts)


def _fmt_phrase_hierarchy(pre_batch):
    """Format phrase hierarchy: roots, extensions, strategy.
    
    Data sources (checked in order):
    1. pre_batch["enhanced"]["phrase_hierarchy"] â€” from enhanced_pre_batch.py
    2. pre_batch["_phrase_hierarchy"] â€” injected by app.py from /phrase_hierarchy endpoint
    """
    hier = (pre_batch.get("enhanced") or {}).get("phrase_hierarchy") or pre_batch.get("_phrase_hierarchy") or {}
    if not hier:
        return ""

    parts = ["â•â•â• HIERARCHIA FRAZ â•â•â•"]

    strategies = hier.get("strategies") or {}

    # 1. Extensions sufficient â€” don't repeat root standalone
    ext_suff = strategies.get("extensions_sufficient") or {}
    ext_roots = ext_suff.get("roots") or []
    if ext_roots:
        parts.append("RDZENIE POKRYTE ROZSZERZENIAMI (NIE powtarzaj samodzielnie!):")
        for root_info in ext_roots[:8]:
            if isinstance(root_info, dict):
                root = root_info.get("root", root_info.get("keyword", ""))
                extensions = root_info.get("extensions", [])
                ext_list = ", ".join(f'"{e}"' if isinstance(e, str) else f'"{e.get("keyword", "")}"' for e in extensions[:5])
                parts.append(f'  â€¢ "{root}" â†’ uÅ¼ywaj rozszerzeÅ„: {ext_list}')
            elif isinstance(root_info, str):
                parts.append(f'  â€¢ "{root_info}" â†’ uÅ¼ywaj rozszerzeÅ„ zamiast rdzenia')

    # 2. Mixed â€” some standalone + extensions
    mixed = strategies.get("mixed") or {}
    mixed_roots = mixed.get("roots") or []
    if mixed_roots:
        parts.append("RDZENIE MIESZANE (kilka samodzielnych uÅ¼yÄ‡ + rozszerzenia):")
        for root_info in mixed_roots[:8]:
            if isinstance(root_info, dict):
                root = root_info.get("root", root_info.get("keyword", ""))
                standalone = root_info.get("standalone_uses", "1-2")
                extensions = root_info.get("extensions", [])
                ext_list = ", ".join(f'"{e}"' if isinstance(e, str) else f'"{e.get("keyword", "")}"' for e in extensions[:5])
                parts.append(f'  â€¢ "{root}" â†’ {standalone}Ã— samodzielnie + rozszerzenia: {ext_list}')
            elif isinstance(root_info, str):
                parts.append(f'  â€¢ "{root_info}" â†’ kilka samodzielnie + rozszerzenia')

    # 3. Need standalone â€” extensions insufficient
    standalone = strategies.get("need_standalone") or {}
    standalone_roots = standalone.get("roots") or []
    if standalone_roots:
        parts.append("RDZENIE WYMAGAJÄ„CE SAMODZIELNYCH UÅ»YÄ†:")
        for root_info in standalone_roots[:8]:
            if isinstance(root_info, dict):
                root = root_info.get("root", root_info.get("keyword", ""))
                target = root_info.get("remaining", root_info.get("target", "?"))
                parts.append(f'  â€¢ "{root}" â†’ uÅ¼yj samodzielnie jeszcze ~{target}Ã—')
            elif isinstance(root_info, str):
                parts.append(f'  â€¢ "{root_info}" â†’ uÅ¼yj samodzielnie')

    # 4. Entity phrases (if available)
    entity_phrases = hier.get("entity_phrases") or []
    if entity_phrases:
        ep_list = ", ".join(f'"{e}"' if isinstance(e, str) else f'"{e.get("keyword", "")}"' for e in entity_phrases[:6])
        parts.append(f"FRAZY ENCYJNE (wpleÄ‡ naturalnie): {ep_list}")

    # 5. Triplet phrases (if available)
    triplet_phrases = hier.get("triplet_phrases") or []
    if triplet_phrases:
        tp_list = ", ".join(f'"{t}"' if isinstance(t, str) else f'"{t.get("keyword", "")}"' for t in triplet_phrases[:6])
        parts.append(f"FRAZY TRIPLETOWE (relacje do wplecenia): {tp_list}")

    if len(parts) <= 1:
        return ""

    return "\n".join(parts)


def _fmt_h2_remaining(pre_batch):
    h2_remaining = pre_batch.get("h2_remaining") or []
    if not h2_remaining:
        return ""

    h2_list = ", ".join(f'"{h}"' for h in h2_remaining[:6])
    return f"â•â•â• PLAN â•â•â•\nPozostaÅ‚e sekcje H2 w artykule: {h2_list}\nNie zachodÅº na ich tematy â€” zostanÄ… pokryte pÃ³Åºniej."


def _fmt_output_format(h2, batch_type):
    if batch_type in ("INTRO", "intro"):
        return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ wstÄ™pu. NIE zaczynaj od "h2:" â€” wstÄ™p nie ma nagÅ‚Ã³wka.
80-150 sÅ‚Ã³w. FrazÄ™ gÅ‚Ã³wnÄ… wpleÄ‡ w PIERWSZE zdanie.
NIE dodawaj komentarzy, wyjaÅ›nieÅ„ â€” TYLKO treÅ›Ä‡ wstÄ™pu."""
    
    return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ tego batcha. Zaczynaj dokÅ‚adnie od:

h2: {h2}

Potem: akapity tekstu (40-150 sÅ‚Ã³w kaÅ¼dy), opcjonalnie h3: [podsekcja].
NIE dodawaj komentarzy, wyjaÅ›nieÅ„, podsumowaÅ„ â€” TYLKO treÅ›Ä‡ artykuÅ‚u."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FAQ PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_faq_system_prompt(pre_batch=None):
    """System prompt for FAQ generation."""
    base = (
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO. "
        "Piszesz sekcjÄ™ FAQ â€” zwiÄ™zÅ‚e, konkretne odpowiedzi na pytania uÅ¼ytkownikÃ³w. "
        "KaÅ¼da odpowiedÅº ma szansÄ™ trafiÄ‡ do Google Featured Snippet â€” pisz bezpoÅ›rednio i merytorycznie."
    )

    gpt_instructions = ""
    if pre_batch:
        gpt_instructions = pre_batch.get("gpt_instructions_v39", "")

    if gpt_instructions:
        return base + "\n\n" + gpt_instructions
    return base


def build_faq_user_prompt(paa_data, pre_batch=None):
    """User prompt for FAQ generation."""
    # Normalize: if paa_data is a list (raw PAA questions), wrap it
    if isinstance(paa_data, list):
        paa_data = {"serp_paa": paa_data}
    elif not isinstance(paa_data, dict):
        paa_data = {}
    paa_questions = paa_data.get("serp_paa") or []
    unused = paa_data.get("unused_keywords") or {}
    avoid = paa_data.get("avoid_in_faq") or []
    if isinstance(avoid, dict):
        avoid = avoid.get("topics") or []
    elif not isinstance(avoid, list):
        avoid = []
    instructions_raw = paa_data.get("instructions", "")
    if isinstance(instructions_raw, dict):
        parts = []
        for k, v in instructions_raw.items():
            if isinstance(v, str):
                parts.append(f"â€¢ {v}")
            elif isinstance(v, dict):
                for sk, sv in v.items():
                    if isinstance(sv, str):
                        parts.append(f"â€¢ {sk}: {sv}")
        instructions = "\n".join(parts)
    elif isinstance(instructions_raw, str):
        instructions = instructions_raw
    else:
        instructions = ""

    enhanced_paa = []
    if pre_batch:
        enhanced = pre_batch.get("enhanced") or {}
        if not isinstance(enhanced, dict):
            enhanced = {}
        enhanced_paa = enhanced.get("paa_from_serp") or []
        if not isinstance(enhanced_paa, list):
            enhanced_paa = []

    keyword_limits = {}
    if pre_batch:
        keyword_limits = pre_batch.get("keyword_limits") or {}
        if not isinstance(keyword_limits, dict):
            keyword_limits = {}
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_names = [s.get("keyword", s) if isinstance(s, dict) else s for s in stop_raw]

    style = {}
    if pre_batch:
        style = pre_batch.get("style_instructions") or {}

    sections = []

    sections.append("""â•â•â• SEKCJA FAQ â•â•â•
Napisz sekcjÄ™ FAQ. Zaczynaj DOKÅADNIE od:
h2: NajczÄ™Å›ciej zadawane pytania""")

    all_paa = list(dict.fromkeys(paa_questions + enhanced_paa))
    if all_paa:
        sections.append("Pytania z Google (People Also Ask) â€” to NAPRAWDÄ˜ pytajÄ… uÅ¼ytkownicy:")
        for i, q in enumerate(all_paa[:8], 1):
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text and q_text.strip():
                sections.append(f'  {i}. {q_text}')
        sections.append("Wybierz 4-6 najlepszych. MoÅ¼esz przeformuÅ‚owaÄ‡, ale zachowaj sens.")

    if unused:
        if isinstance(unused, dict):
            unused_list = []
            for cat, items in unused.items():
                if isinstance(items, list):
                    unused_list.extend(items[:5])
                elif isinstance(items, str):
                    unused_list.append(items)
            if unused_list:
                names = ", ".join(f'"{u}"' if isinstance(u, str) else f'"{u.get("keyword", "")}"' for u in unused_list[:8])
                sections.append(f'\nFrazy jeszcze nieuÅ¼yte â€” wpleÄ‡ w odpowiedzi: {names}')
        elif isinstance(unused, list):
            names = ", ".join(f'"{u}"' for u in unused[:8])
            sections.append(f'\nFrazy jeszcze nieuÅ¼yte â€” wpleÄ‡ w odpowiedzi: {names}')

    if avoid:
        topics = ", ".join(f'"{a}"' if isinstance(a, str) else f'"{a.get("topic", "")}"' for a in avoid[:8])
        sections.append(f'\nNIE powtarzaj tematÃ³w juÅ¼ pokrytych w artykule: {topics}')

    if stop_names:
        sections.append(f'\nğŸ›‘ STOP â€” NIE UÅ»YWAJ: {", ".join(f"{s}" for s in stop_names[:5])}')

    if style:
        forbidden = style.get("forbidden_phrases") or []
        if forbidden:
            sections.append(f'ZAKAZANE zwroty: {", ".join(forbidden[:5])}')

    if pre_batch and pre_batch.get("article_memory"):
        mem = pre_batch["article_memory"]
        if isinstance(mem, dict):
            topics = mem.get("topics_covered") or []
            if topics:
                topic_names = [t if isinstance(t, str) else t.get("topic", "") for t in topics[:6]]
                sections.append(f'\nTematy z artykuÅ‚u (nie powtarzaj): {", ".join(topic_names)}')

    if instructions:
        sections.append(f'\n{instructions}')

    sections.append("""
â•â•â• FORMAT â•â•â•
h2: NajczÄ™Å›ciej zadawane pytania

h3: [Pytanie â€” 5-10 sÅ‚Ã³w, zaczynaj od Jak/Czy/Co/Dlaczego/Ile]
[OdpowiedÅº 60-120 sÅ‚Ã³w]
â†’ Zdanie 1: BEZPOÅšREDNIA odpowiedÅº
â†’ Zdanie 2-3: rozwiniÄ™cie z konkretem
â†’ Zdanie 4: praktyczna wskazÃ³wka lub wyjÄ…tek

Napisz 4-6 pytaÅ„. Pisz TYLKO treÅ›Ä‡, bez komentarzy.""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# H2 PLAN PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_h2_plan_system_prompt():
    """System prompt for H2 plan generation."""
    return (
        "JesteÅ› ekspertem SEO z 10-letnim doÅ›wiadczeniem w planowaniu architektury treÅ›ci. "
        "Tworzysz logiczne, wyczerpujÄ…ce struktury nagÅ‚Ã³wkÃ³w H2, ktÃ³re pokrywajÄ… temat kompleksowo "
        "i dajÄ… przewagÄ™ nad konkurencjÄ… dziÄ™ki pokryciu luk treÅ›ciowych."
    )


def build_h2_plan_user_prompt(main_keyword, mode, s1_data, all_user_phrases, user_h2_hints=None):
    """Build readable H2 plan prompt from S1 analysis data."""
    s1_data = s1_data or {}
    competitor_h2 = s1_data.get("competitor_h2_patterns") or []
    suggested_h2s = (s1_data.get("content_gaps") or {}).get("suggested_new_h2s", [])
    content_gaps = s1_data.get("content_gaps") or {}
    causal_triplets = s1_data.get("causal_triplets") or {}
    paa = s1_data.get("paa") or s1_data.get("paa_questions") or []

    sections = []

    mode_desc = "standard = peÅ‚ny artykuÅ‚" if mode == "standard" else "fast = krÃ³tki artykuÅ‚, max 3 sekcje"
    sections.append(f"""HASÅO GÅÃ“WNE: {main_keyword}
TRYB: {mode} ({mode_desc})""")

    if competitor_h2:
        lines = ["â•â•â• WZORCE H2 KONKURENCJI (najczÄ™stsze tematy sekcji) â•â•â•"]
        for i, h in enumerate(competitor_h2[:20], 1):
            if isinstance(h, dict):
                pattern = h.get("pattern", h.get("h2", str(h)))
                count = h.get("count", "")
                lines.append(f"  {i}. {pattern}" + (f" ({count}Ã—)" if count else ""))
            elif isinstance(h, str):
                lines.append(f"  {i}. {h}")
        sections.append("\n".join(lines))

    if suggested_h2s:
        lines = ["â•â•â• SUGEROWANE NOWE H2 (luki â€” tego NIKT z konkurencji nie pokrywa) â•â•â•"]
        for h in suggested_h2s[:10]:
            h_text = h if isinstance(h, str) else h.get("h2", h.get("title", str(h)))
            lines.append(f"  â€¢ {h_text}")
        sections.append("\n".join(lines))

    # Content gaps â€” ordered by priority (GPT prompt: PAA_UNANSWERED > DEPTH_MISSING > SUBTOPIC_MISSING)
    gap_priority_map = {
        "paa_unanswered": ("ğŸ”´ HIGH", "PAA bez odpowiedzi"),
        "depth_missing": ("ğŸŸ¡ MED-HIGH", "Brak gÅ‚Ä™bi"),
        "subtopic_missing": ("ğŸŸ¢ MED", "BrakujÄ…cy podtemat"),
        "gaps": ("", "Luka"),
    }
    all_gaps = []
    for key in ("paa_unanswered", "depth_missing", "subtopic_missing", "gaps"):
        priority, label = gap_priority_map.get(key, ("", ""))
        items = content_gaps.get(key) or []
        for item in items[:5]:
            gap_text = item if isinstance(item, str) else item.get("gap", item.get("topic", str(item)))
            if gap_text and gap_text not in [g[0] for g in all_gaps]:
                all_gaps.append((gap_text, priority, label))
    if all_gaps:
        lines = ["â•â•â• LUKI TREÅšCIOWE (tematy do pokrycia â€” priorytet od najwyÅ¼szego) â•â•â•"]
        for gap_text, priority, label in all_gaps[:10]:
            prefix = f"[{priority}] " if priority else ""
            lines.append(f"  â€¢ {prefix}{gap_text}")
        sections.append("\n".join(lines))

    if paa:
        lines = ["â•â•â• PYTANIA PAA (People Also Ask z Google) â•â•â•"]
        for q in paa[:8]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                lines.append(f"  â“ {q_text}")
        sections.append("\n".join(lines))

    triplet_list = (causal_triplets.get("chains") or causal_triplets.get("singles")
                    or causal_triplets.get("triplets") or [])[:8]
    if triplet_list:
        lines = ["â•â•â• PRZYCZYNOWE ZALEÅ»NOÅšCI (causeâ†’effect z konkurencji) â•â•â•",
                 "Confidence: ğŸ”´ â‰¥0.9 UÅ»YJ | ğŸŸ¡ â‰¥0.6 gdy pasuje | ğŸŸ¢ <0.6 opcjonalnie",
                 "is_chain=True (Aâ†’Bâ†’C) = najcenniejsze â€” buduj logiczny przepÅ‚yw"]
        for t in triplet_list:
            if isinstance(t, dict):
                cause = t.get("cause", t.get("subject", ""))
                effect = t.get("effect", t.get("object", ""))
                conf = t.get("confidence", 0)
                is_chain = t.get("is_chain", False)
                
                # Priority indicator
                if conf >= 0.9:
                    ind = "ğŸ”´"
                elif conf >= 0.6:
                    ind = "ğŸŸ¡"
                else:
                    ind = "ğŸŸ¢"
                chain_tag = " [CHAIN]" if is_chain else ""
                conf_str = f" ({conf:.1f})" if conf else ""
                lines.append(f"  {ind} {cause} â†’ {effect}{conf_str}{chain_tag}")
            elif isinstance(t, str):
                lines.append(f"  â€¢ {t}")
        sections.append("\n".join(lines))

    if user_h2_hints:
        h2_hints_list = "\n".join(f'  â€¢ "{h}"' for h in user_h2_hints[:10])
        sections.append(f"""â•â•â• FRAZY H2 UÅ»YTKOWNIKA â•â•â•

UÅ¼ytkownik podaÅ‚ te frazy z myÅ›lÄ… o nagÅ‚Ã³wkach H2.
Wykorzystaj je w nagÅ‚Ã³wkach tam, gdzie brzmiÄ… naturalnie po polsku.
Nie musisz uÅ¼yÄ‡ kaÅ¼dej â€” ale nie ignoruj ich. Dopasuj z wyczuciem.

JeÅ›li fraza brzmi sztucznie jako nagÅ‚Ã³wek â€” przeformuÅ‚uj lub pomiÅ„ (trafi do treÅ›ci).

FRAZY H2:
{h2_hints_list}""")

    if all_user_phrases:
        phrases_text = ", ".join(f'"{p}"' for p in all_user_phrases[:15])
        sections.append(f"""â•â•â• KONTEKST TEMATYCZNY (frazy BASIC/EXTENDED) â•â•â•

PoniÅ¼sze frazy bÄ™dÄ… uÅ¼yte W TREÅšCI artykuÅ‚u (nie w nagÅ‚Ã³wkach).
PodajÄ™ je Å¼ebyÅ› wiedziaÅ‚ jaki zakres tematyczny artykuÅ‚ musi pokryÄ‡
i zaplanowaÅ‚ H2 tak, by kaÅ¼da fraza miaÅ‚a naturalnÄ… sekcjÄ™:

{phrases_text}""")

    fast_note = "Tryb fast: DOKÅADNIE 3 sekcje + FAQ (4 H2 Å‚Ä…cznie)." if mode == "fast" else ""
    
    # v50.5 FIX 29: Dynamic H2 count based on recommended article length
    # Instead of hard-coded "6-9 H2", scale H2 count to match content needs.
    # Each H2 section generates ~200-400 words. Too many H2s â†’ article bloat.
    length_analysis = s1_data.get("length_analysis") or {}
    rec_length = length_analysis.get("recommended") or s1_data.get("recommended_length") or 0
    median_length = length_analysis.get("median") or s1_data.get("median_length") or 0
    
    if mode != "fast":
        # Use recommended length (or median Ã— 2 as fallback) to determine H2 count
        target = rec_length or (median_length * 2) or 1500
        if target <= 500:
            h2_range = "2-3"
            h2_min, h2_max = 2, 3
        elif target <= 1000:
            h2_range = "3-4"
            h2_min, h2_max = 3, 4
        elif target <= 2000:
            h2_range = "4-6"
            h2_min, h2_max = 4, 6
        elif target <= 3500:
            h2_range = "5-7"
            h2_min, h2_max = 5, 7
        else:
            h2_range = "6-9"
            h2_min, h2_max = 6, 9
        
        fast_note = (
            f"Tryb standard: {h2_range} sekcji + FAQ ({h2_min+1}-{h2_max+1} H2 Å‚Ä…cznie).\n"
            f"   UWAGA: Rekomendowana dÅ‚ugoÅ›Ä‡ artykuÅ‚u: ~{target} sÅ‚Ã³w (mediana konkurencji: {median_length}).\n"
            f"   KaÅ¼da sekcja H2 = ~{target // (h2_max + 1)}-{target // h2_min} sÅ‚Ã³w.\n"
            f"   NIE GENERUJ wiÄ™cej niÅ¼ {h2_max + 1} H2 (wliczajÄ…c FAQ)!"
        )
    
    h2_hint_rule = ("UwzglÄ™dnij frazy H2 uÅ¼ytkownika w nagÅ‚Ã³wkach, o ile brzmiÄ… naturalnie."
                    if user_h2_hints else "Dobierz nagÅ‚Ã³wki na podstawie S1 i luk treÅ›ciowych.")

    sections.append(f"""â•â•â• ZASADY â•â•â•

1. LICZBA H2: {fast_note}
2. OSTATNI H2 MUSI byÄ‡: "NajczÄ™Å›ciej zadawane pytania"
3. Pokryj najwaÅ¼niejsze wzorce z konkurencji + luki treÅ›ciowe (przewaga nad konkurencjÄ…)
4. {h2_hint_rule}
5. Logiczna narracja â€” od ogÃ³Å‚u do szczegÃ³Å‚u, chronologicznie, lub problemowo
6. NIE powtarzaj hasÅ‚a gÅ‚Ã³wnego dosÅ‚ownie w kaÅ¼dym H2
7. H2 muszÄ… brzmieÄ‡ naturalnie po polsku â€” Å¼adnego keyword stuffingu

â•â•â• FORMAT ODPOWIEDZI â•â•â•

Odpowiedz TYLKO JSON array, bez markdown, bez komentarzy:
["H2 pierwszy", "H2 drugi", ..., "NajczÄ™Å›ciej zadawane pytania"]""")

    return "\n\n".join(sections)
