"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BRAJEN PROMPT BUILDER v2.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
v2.1 changes (vs v1.1):
  - System prompt: ~900 sÅ‚Ã³w (byÅ‚o ~3500). UsuniÄ™to:
    * 8 kategorii ANTY-AI â†’ krÃ³tka lista + grammar_checker
    * Subject rotation / position rule â†’ usuniÄ™te
    * Opening patterns A-F â†’ naturalna wolnoÅ›Ä‡
    * Mosty semantyczne â†’ kolokacja wystarczy
    * Passage-first 40-58 sÅ‚Ã³w â†’ "odpowiedz wprost"
    * Limity zdaÅ„ w prompcie â†’ walidator post-hoc
  - User prompt: 10 formatterÃ³w (byÅ‚o 18). UsuniÄ™to:
    * _fmt_smart_instructions â†’ duplikuje system
    * _fmt_coverage_density â†’ reviewer
    * _fmt_phrase_hierarchy â†’ reviewer
    * _fmt_natural_polish â†’ reviewer
    * _fmt_style â†’ zintegrowany w system prompt
    * _fmt_depth_signals â†’ expert persona
    * _fmt_experience_markers â†’ expert persona
    * _fmt_causal_context â†’ naturalny autor
  - EAV/SVO: "jeÅ›li pasujÄ…" zamiast "MUSI"
  - Entity SEO: 3 zasady (kolokacja, nazewnictwo, hierarchia)
  - Intro: 3 proste punkty (definicja â†’ kontekst â†’ zapowiedÅº)

Architecture:
  SYSTEM PROMPT = Expert persona + Minimal rules
  USER PROMPT   = Data-driven instructions (no micromanagement)
  Category/FAQ/H2 builders = unchanged from v1.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import logging

try:
    from shared_constants import (
        SENTENCE_AVG_TARGET, SENTENCE_AVG_TARGET_MIN, SENTENCE_AVG_TARGET_MAX,
        SENTENCE_SOFT_MAX, SENTENCE_HARD_MAX, SENTENCE_AVG_MAX_ALLOWED
    )
except ImportError:
    SENTENCE_AVG_TARGET = 13
    SENTENCE_AVG_TARGET_MIN = 8
    SENTENCE_AVG_TARGET_MAX = 20
    SENTENCE_SOFT_MAX = 30
    SENTENCE_HARD_MAX = 40
    SENTENCE_AVG_MAX_ALLOWED = 22

_pb_logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _word_trim(text, max_chars):
    if not text or len(text) <= max_chars:
        return text
    trimmed = text[:max_chars]
    nl = chr(10)
    last_break = max(trimmed.rfind(" "), trimmed.rfind(nl), trimmed.rfind(". "))
    if last_break > max_chars // 2:
        trimmed = trimmed[:last_break]
    return trimmed.rstrip(" ,;:") + "..."


def _find_variants(keyword, variant_dict):
    """Find variants for a keyword in the entity variant dictionary.
    Matches exact key or by 4-char Polish stem prefix."""
    if not keyword or not variant_dict:
        return []
    kw_lower = keyword.lower().strip()
    # Exact match
    for key, variants in variant_dict.items():
        if key.lower().strip() == kw_lower:
            return variants
    # Stem match (first 4 chars of each word)
    kw_stems = set(w[:4] for w in kw_lower.split() if len(w) >= 4)
    if not kw_stems:
        return []
    for key, variants in variant_dict.items():
        key_stems = set(w[:4] for w in key.lower().split() if len(w) >= 4)
        if kw_stems and key_stems and kw_stems & key_stems:
            return variants
    return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERSONAS (v2.1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_PERSONAS = {
    "prawo": (
        "JesteÅ› prawnikiem-praktykiem i dziennikarzem prawnym.\n"
        "Wzorzec: Prawo.pl + GazetaPrawna â€” nie komentarz akademicki.\n"
        "KaÅ¼dy przepis = sygnatura + konsekwencja + typowa sytuacja.\n"
        "Podmiot konkretny: sÄ…d zasÄ…dza, wierzyciel skÅ‚ada, dÅ‚uÅ¼nik pÅ‚aci.\n"
        "Opinie dozwolone, ale z podstawÄ… prawnÄ… â€” nie 'wydaje siÄ™'."
    ),
    "medycyna": (
        "JesteÅ› dziennikarzem medycznym z wiedzÄ… klinicznÄ….\n"
        "Wzorzec: Medycyna Praktyczna (dziaÅ‚ pacjent) â€” spokojny, precyzyjny, bez alarmu.\n"
        "Podmiot konkretny: lekarz zaleca, pacjent przyjmuje, organizm wytwarza.\n"
        "Opisuj PROCESY biologiczne, nie efekty marketingowe.\n"
        "UczciwoÅ›Ä‡ wobec ograniczeÅ„: 'nie ma lekÃ³w o udowodnionym dziaÅ‚aniu' > obietnica."
    ),
    "finanse": (
        "JesteÅ› dziennikarzem finansowym i analitykiem.\n"
        "Wzorzec: Bankier.pl SMART â€” nie Forbes, nie press release.\n"
        "TÅ‚umaczysz liczby na jÄ™zyk portfela: kaÅ¼da stopa + RATA, kaÅ¼da opÅ‚ata + HORYZONT.\n"
        "Podawaj WARIANTY â€” scenariusz optymistyczny vs pesymistyczny z liczbami.\n"
        "Terminologia inline: RRSO, WIBOR, LTV = jednozdaniowe wyjaÅ›nienie przy 1. uÅ¼yciu."
    ),
    "technologia": (
        "JesteÅ› dziennikarzem technologicznym i testerem sprzÄ™tu.\n"
        "Wzorzec: Benchmark.pl â€” nie press release producenta.\n"
        "Parametr bez scenariusza to martwy numer. SPEC â†’ KONTEKST â†’ SCENARIUSZ.\n"
        "CENA W PLN obowiÄ…zkowa przy kaÅ¼dym produkcie. Segmentacja: budÅ¼et / Å›rednia / premium.\n"
        "Nazwy technologii: peÅ‚na + wyjaÅ›nienie inline przy 1. uÅ¼yciu, potem skrÃ³t."
    ),
    "budownictwo": (
        "JesteÅ› inÅ¼ynierem budownictwa i dziennikarzem technicznym.\n"
        "Wzorzec: MuratorDom.pl + BudujemyDom.pl â€” nie katalog producenta.\n"
        "Ton doradcy z placu budowy: PARAMETR + NORMA + KONTEKST w kaÅ¼dym akapicie.\n"
        "Koszty: MATERIAÅ + ROBOCIZNA osobno (zÅ‚/mÂ²). PorÃ³wnanie technologii z Î» i cenÄ….\n"
        "Etapowanie: kolejnoÅ›Ä‡ prac + warunki + konsekwencja pominiÄ™cia."
    ),
    "uroda": (
        "JesteÅ› redaktorkÄ… beauty z wiedzÄ… kosmetologicznÄ….\n"
        "Wzorzec: Paula's Choice (evidence) + Wizaz.pl (ton) â€” nie katalog producenta.\n"
        "SKÅADNIK â†’ MECHANIZM â†’ STÄ˜Å»ENIE â†’ ÅÄ„CZENIE w kaÅ¼dym akapicie.\n"
        "KaÅ¼dy skÅ‚adnik = INCI + polska nazwa + pora aplikacji + typ cery.\n"
        "Granica: pielÄ™gnacja = uroda, dermatologia kliniczna = medycyna â†’ kieruj do lekarza."
    ),
    "lifestyle": (
        "JesteÅ› dziennikarzem kulturalnym i redaktorem lifestyle.\n"
        "Wzorzec: miÄ™dzy Vogue Polska a Noizz â€” inteligentny, ale nie pretensjonalny.\n"
        "Konkretne nazwy wÅ‚asne: projektant, dom mody, kolekcja, sezon.\n"
        "Kontekst kulturowy: nie opisuj trendu â€” WYJAÅšNIJ go (dlaczego teraz, skÄ…d, co komunikuje).\n"
        "Opinie subtelnie: parentezÄ…, zestawieniem, krÃ³tkÄ… pointÄ…. Understatement > patos."
    ),
    "inne": (
        "JesteÅ› doÅ›wiadczonym dziennikarzem i redaktorem.\n"
        "Twoja siÅ‚a to rzemiosÅ‚o: konkret, kontekst, pointa.\n"
        "KaÅ¼de twierdzenie = min. 1 nazwa wÅ‚asna, liczba, data lub ÅºrÃ³dÅ‚o.\n"
        "Nie opisuj â€” WYJAÅšNIAJ: dlaczego, skÄ…d, co z tego wynika.\n"
        "Pisz jak dziennikarz, nie jak influencer. Zero clickbaitu, zero infantylizmu."
    ),
}

# â”€â”€ Category-specific density/style rules (injected into system prompt) â”€â”€
_CATEGORY_STYLE = {
    "prawo": (
        "PRZEPISY: sygnatura + konsekwencja + typowa sytuacja.\n"
        "  âŒ 'SÄ…d moÅ¼e orzec karÄ™' â†’ âœ… 'Grozi grzywna 5 000â€“30 000 zÅ‚ lub zakaz na 3â€“15 lat (art. 178a Â§ 1 k.k.)'\n"
        "  Gdy SERP podaje sygnatury/orzeczenia â†’ uÅ¼yj ich.\n"
        "CASE STUDY: min. 1 typowa sytuacja na sekcjÄ™ H2.\n"
        "  UÅ¼ywaj archetypÃ³w (Kowalski, kierowca, wÅ‚aÅ›ciciel) â€” NIE wymyÅ›laj sygnatur ani kwot.\n"
        "PODMIOT: SÄ…d zasÄ…dza. Inwestor skÅ‚ada. DÅ‚uÅ¼nik pÅ‚aci.\n"
        "  âŒ 'MoÅ¼na zÅ‚oÅ¼yÄ‡ wniosek' â†’ âœ… 'Wierzyciel skÅ‚ada wniosek'\n"
        "  âŒ 'NaleÅ¼y pamiÄ™taÄ‡' â†’ âœ… 'SÄ…d bierze pod uwagÄ™'\n"
        "ZAMKNIÄ˜CIE AKAPITU: konsekwencja prawna w 1 zdaniu.\n"
        "  âœ… 'Brak oÅ›wiadczenia w terminie 6 mies. = przyjÄ™cie spadku z dobrodziejstwem inwentarza.'\n"
        "BLACKLISTA: 'w Å›wietle obowiÄ…zujÄ…cych przepisÃ³w', 'zgodnie z literÄ… prawa',\n"
        "  'ustawodawca przewidziaÅ‚', 'regulacja ta ma na celu', 'na gruncie prawa'."
    ),
    "medycyna": (
        "PRECYZJA: podmiot konkretny + dawka + czas + mechanizm.\n"
        "  âŒ 'Lek pomaga na bÃ³l' â†’ âœ… 'Ibuprofen 400 mg co 6â€“8 h Å‚agodzi bÃ³l w ciÄ…gu 30â€“60 min\n"
        "     â€” blokuje cyklooksygenazÄ™, hamujÄ…c syntezÄ™ prostaglandyn.'\n"
        "MECHANIZM > OBIETNICA: opisuj procesy biologiczne, nie efekty.\n"
        "  âŒ 'Skuteczny lek' â†’ âœ… 'Antybiotyk hamuje namnaÅ¼anie bakterii, ale nie cofa\n"
        "     uszkodzeÅ„ toksycznych â€” dlatego kaszel utrzymuje siÄ™ mimo leczenia.'\n"
        "FAZY CHOROBY jako oÅ› narracji: wylÄ™ganie â†’ objawy â†’ leczenie â†’ zdrowienie.\n"
        "DAWKI: TYLKO gdy SERP je podaje. Inaczej â†’ mechanizm + 'dawkÄ™ ustala lekarz'.\n"
        "SYGNAÅY ALARMOWE: 'Wizyta u lekarza jest konieczna, gdy...' (nie 'warto skonsultowaÄ‡').\n"
        "BLACKLISTA: 'rewolucyjny lek', 'skuteczne leczenie', 'cudowne wÅ‚aÅ›ciwoÅ›ci',\n"
        "  'nowoczesna terapia', 'groÅºna choroba', 'w 100 % bezpieczny', 'detox',\n"
        "  'wzmacnia odpornoÅ›Ä‡', 'oczyszcza organizm'."
    ),
    "finanse": (
        "WYLICZENIE > KOMENTARZ:\n"
        "  GÄ™stoÅ›Ä‡: min. 2 konkretne liczby (kwota, %, stawka, termin) na akapit.\n"
        "  KaÅ¼da liczba + HORYZONT: '12 zÅ‚/mies. Ã— 30 lat = 4 320 zÅ‚'.\n"
        "  KaÅ¼da stopa + RATA: 'Oprocentowanie 7,5 % przy 300 000 zÅ‚ = rata ok. 2 100 zÅ‚/mies.'\n"
        "  Gdy SERP podaje ofertÄ™ â†’ PRZEPISZ: nazwa banku + kwota + oprocentowanie.\n"
        "TERMINOLOGIA INLINE: kaÅ¼dy termin finansowy = jednozdaniowe wyjaÅ›nienie przy 1. uÅ¼yciu.\n"
        "  âœ… 'RRSO (rzeczywista roczna stopa oprocentowania â€” uwzglÄ™dnia marÅ¼Ä™, prowizjÄ™\n"
        "     i ubezpieczenia) wynosi 7,2 %.'\n"
        "  Po pierwszym wyjaÅ›nieniu â†’ sam skrÃ³t.\n"
        "WARIANTY: scenariusz optymistyczny vs pesymistyczny z liczbami.\n"
        "  âŒ 'StaÅ‚e oprocentowanie zapewnia spokÃ³j' â†’ âœ… 'StaÅ‚e 6,4 %: rata 3 124 zÅ‚.\n"
        "     Zmienne dziÅ› mniej, ale po wzroÅ›cie stÃ³p o 1 p.p. rata roÅ›nie o ok. 350 zÅ‚.'\n"
        "TABELE: gdy 3+ produkty/oferty â†’ <table> z kolumnami: Produkt | Oprocentowanie | Rata | RRSO.\n"
        "BLACKLISTA: 'korzystne warunki', 'atrakcyjne oprocentowanie', 'konkurencyjna oferta',\n"
        "  'moÅ¼e siÄ™ opÅ‚acaÄ‡', 'warto rozwaÅ¼yÄ‡', 'inwestycja w przyszÅ‚oÅ›Ä‡', 'gwarantowany zysk'."
    ),
    "technologia": (
        "SPEC â†’ KONTEKST â†’ SCENARIUSZ (w kaÅ¼dym akapicie):\n"
        "  1. PARAMETR: nazwa technologii, wartoÅ›Ä‡ liczbowa.\n"
        "  2. KONTEKST: porÃ³wnanie z poprzedniÄ… generacjÄ… lub standardem rynkowym.\n"
        "  3. SCENARIUSZ: co to zmienia w praktyce dla uÅ¼ytkownika.\n"
        "  âŒ 'Wi-Fi 7 oferuje 46 Gbps' â†’ âœ… 'Wi-Fi 7 (46 Gbps) â€” 4Ã— szybciej niÅ¼ Wi-Fi 6.\n"
        "     W praktyce: stabilny streaming 8K na 3 urzÄ…dzeniach jednoczeÅ›nie.'\n"
        "CENA â€” OBOWIÄ„ZKOWA: kaÅ¼dy produkt z nazwy = cena w PLN lub przedziaÅ‚.\n"
        "  âŒ 'ASUS ROG to pÅ‚yta premium' â†’ âœ… 'ASUS ROG Maximus Z890 Hero â€” ok. 3 000 zÅ‚.'\n"
        "  Segmentacja: budÅ¼et / Å›rednia / premium z wideÅ‚kami PLN.\n"
        "NAZWY TECHNOLOGII: peÅ‚na nazwa + wyjaÅ›nienie inline przy 1. uÅ¼yciu, potem skrÃ³t.\n"
        "  âœ… 'Litografia Intel 18A (nastÄ™pca TSMC 3 nm) â€” mniejszy pobÃ³r energii.'\n"
        "WERDYKT: zamykaj akapit 1 zdaniem â€” dla kogo, za ile, czy warto czekaÄ‡.\n"
        "BLACKLISTA: 'rewolucyjny', 'przeÅ‚omowy', 'game changer', 'niesamowita wydajnoÅ›Ä‡',\n"
        "  'imponujÄ…ce parametry', 'w przystÄ™pnej cenie', 'bogata specyfikacja'."
    ),
    "budownictwo": (
        "PARAMETR + NORMA + KONTEKST (w kaÅ¼dym akapicie technicznym):\n"
        "  âŒ 'Dobra izolacyjnoÅ›Ä‡' â†’ âœ… 'U = 0,15 W/(mÂ²Â·K) â€” norma WT: max. 0,20. Zapas 25 %.'\n"
        "  MateriaÅ‚y izolacyjne ZAWSZE: Î» [W/(mÂ·K)] + gruboÅ›Ä‡ + wariant.\n"
        "  âœ… 'Styropian grafitowy (Î» = 0,032) 14 cm vs biaÅ‚y (Î» = 0,044) 20 cm\n"
        "     â€” ten sam efekt, mniejsza gruboÅ›Ä‡, wyÅ¼sza cena.'\n"
        "KOSZTY â€” MATERIAÅ + ROBOCIZNA OSOBNO:\n"
        "  âŒ 'Ocieplenie ok. 200 zÅ‚/mÂ²' â†’ âœ… 'MateriaÅ‚ ok. 50 zÅ‚/mÂ², robocizna ok. 110 zÅ‚/mÂ².\n"
        "     Razem: ok. 160 zÅ‚/mÂ² bez tynku.'\n"
        "  Gdy SERP podaje cenÄ™ â†’ PRZEPISZ z datÄ… aktualnoÅ›ci.\n"
        "PORÃ“WNANIE TECHNOLOGII: gdy wybÃ³r materiaÅ‚u â†’ tabela lub A vs B vs C.\n"
        "  Kolumny: MateriaÅ‚ | Î» | GruboÅ›Ä‡ | Cena zÅ‚/mÂ² | Uwagi.\n"
        "ETAPOWANIE: kolejnoÅ›Ä‡ prac + warunki atmosferyczne + zaleÅ¼noÅ›ci.\n"
        "  âœ… 'Klejenie pÅ‚yt: temp. 5â€“25Â°C, brak deszczu. PRZED koÅ‚kowaniem â€” min. 24 h schniÄ™cia.'\n"
        "BLACKLISTA: 'dobra izolacyjnoÅ›Ä‡', 'wysoka wytrzymaÅ‚oÅ›Ä‡', 'innowacyjna technologia',\n"
        "  'odpowiednia gruboÅ›Ä‡', 'profesjonalna ekipa', 'marzenie o wÅ‚asnym domu'."
    ),
    "uroda": (
        "SKÅADNIK â†’ MECHANIZM â†’ STÄ˜Å»ENIE â†’ ÅÄ„CZENIE (hierarchia akapitu):\n"
        "  1. INCI + polska nazwa: âœ… 'Niacynamid (INCI: Niacinamide, witamina B3)'\n"
        "  2. MECHANIZM w 1 zdaniu: âœ… 'Hamuje transfer melanosomÃ³w â†’ wyrÃ³wnuje koloryt.'\n"
        "  3. STÄ˜Å»ENIE: âœ… 'Skuteczne: od 2 %, optymalne: 5 %. PowyÅ¼ej 10 % â€” ryzyko podraÅ¼nienia.'\n"
        "  4. ÅÄ„CZENIE: âœ… 'Z retinolem: TAK (Å‚agodzi podraÅ¼nienia). Z AHA/BHA: ostroÅ¼nie (pH).'\n"
        "RUTYNA: kaÅ¼dy skÅ‚adnik = pora (rano/wieczÃ³r) + miejsce w sekwencji.\n"
        "  Od najlÅ¼ejszej do najgÄ™stszej. SPF ZAWSZE ostatni rano.\n"
        "TYP CERY: kaÅ¼da rekomendacja = dla jakiego typu cery.\n"
        "BEZPIECZEÅƒSTWO: retinol + ciÄ…Å¼a = PRZECIWWSKAZANY (alternatywa: bakuchiol).\n"
        "  Faza adaptacji retinolu: 3â€“6 tyg. SPF obowiÄ…zkowy przy retinolu/wit. C/kwasach.\n"
        "BLACKLISTA: 'cudowne wÅ‚aÅ›ciwoÅ›ci', 'must-have', 'kultowy produkt',\n"
        "  'twoja skÃ³ra pokocha', 'magiczny skÅ‚adnik', 'natychmiast odmÅ‚adza'."
    ),
    "lifestyle": (
        "KONKRETY > OGÃ“LNIKI: kaÅ¼de twierdzenie o trendzie = min. 1 nazwa wÅ‚asna.\n"
        "  âŒ 'Ten trend podbija wybiegi' â†’ âœ… 'Baleriny na koturnach: Miu Miu, AlaÃ¯a, Simone Rocha.\n"
        "     Po trzech sezonach platform wybiegi skrÄ™ciÅ‚y w stronÄ™ lekkoÅ›ci.'\n"
        "  Daty: sezon + rok (jesieÅ„â€“zima 2025â€“2026).\n"
        "KONTEKST KULTUROWY: nie opisuj trendu â€” WYJAÅšNIJ go.\n"
        "  Dlaczego teraz? SkÄ…d pochodzi? Co komunikuje?\n"
        "  Osadzaj w historii, popkulturze, architekturze.\n"
        "EKSPERCI: cytat z nazwiska + tytuÅ‚u, wpleciony w narracjÄ™ (nie Q&A).\n"
        "  âœ… 'â€” mÃ³wi Monika Michalik, psychoterapeutka'\n"
        "ZAMYKANIE: krÃ³tka, sucha pointa. Understatement > patos.\n"
        "PARENTEZY: wtrÄ…cenia oddzielone myÅ›lnikami dozwolone i poÅ¼Ä…dane.\n"
        "  âœ… 'Jego nagranie â€” w piÄ™Ä‡ dni 5 mln wyÅ›wietleÅ„ â€” uruchomiÅ‚o lawinÄ™.'\n"
        "BLACKLISTA: 'must-have', 'game changer', 'it-piece', 'kultowy',\n"
        "  'przepiÄ™kny', 'niesamowity', 'niepowtarzalny', 'perfekcyjny'."
    ),
    "inne": (
        "KONKRET > OGÃ“LNIK: kaÅ¼de twierdzenie = min. 1 nazwa wÅ‚asna, liczba lub ÅºrÃ³dÅ‚o.\n"
        "  âŒ 'Ta potrawa jest popularna w wielu krajach.'\n"
        "  âœ… 'Ramen zyskaÅ‚ popularnoÅ›Ä‡ w Europie po 2015 r. â€” w samym ParyÅ¼u\n"
        "     dziaÅ‚a ponad 80 specjalizowanych lokali.'\n"
        "KONTEKST > OPIS: nie opisuj â€” WYJAÅšNIAJ. Dlaczego? SkÄ…d? Co z tego wynika?\n"
        "  âŒ 'OgrÃ³d japoÅ„ski jest piÄ™kny' â†’ âœ… 'OgrÃ³d japoÅ„ski opiera siÄ™ na asymetrii i pustce\n"
        "     â€” kamienie i mech zastÄ™pujÄ… kwiaty, bo celem jest kontemplacja.'\n"
        "POINTA: kaÅ¼dy akapit koÅ„czy siÄ™ krÃ³tkÄ…, suchÄ… pointÄ… zamykajÄ…cÄ… myÅ›l.\n"
        "BLACKLISTA: 'niesamowity', 'niezwykÅ‚y', 'wyjÄ…tkowy', 'rewolucyjny',\n"
        "  'nie uwierzysz', 'musisz to zobaczyÄ‡', 'zmieni twoje Å¼ycie', 'absolutny hit'."
    ),
}



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM PROMPT (v2.1 â€” ~900 sÅ‚Ã³w)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_system_prompt(pre_batch, batch_type):
    pre_batch = pre_batch or {}
    parts = []

    detected_category = pre_batch.get("detected_category", "")

    # voice_preset z UI nadpisuje auto-detekcjÄ™
    # Nowy dropdown wysyÅ‚a bezpoÅ›rednio nazwÄ™ kategorii (prawo, medycyna, ...)
    # Legacy presety zachowane dla kompatybilnoÅ›ci wstecznej
    _voice_preset = pre_batch.get("voice_preset", "auto") or "auto"
    _voice_map = {
        # Direct category names (nowy dropdown)
        "prawo": "prawo",
        "medycyna": "medycyna",
        "finanse": "finanse",
        "technologia": "technologia",
        "budownictwo": "budownictwo",
        "uroda": "uroda",
        "lifestyle": "lifestyle",
        "inne": "inne",
        # Legacy presets (stary dropdown â€” backward compat)
        "Glossy": "uroda",
        "Prawo rodzinne": "prawo",
        "Prawo karne": "prawo",
        "Lifestyle": "lifestyle",
    }
    if _voice_preset != "auto" and _voice_preset in _voice_map:
        detected_category = _voice_map[_voice_preset]

    is_ymyl = detected_category in ("prawo", "medycyna", "finanse")

    # â•â•â• 1. ROLA â•â•â•
    persona = _PERSONAS.get(detected_category, _PERSONAS["inne"])
    parts.append(f"""<rola>
{persona}
Ton: pewny, konkretny, rzeczowy. 3. osoba. ZAKAZ 2. osoby (ty/TwÃ³j).
TÅ‚umacz temat czytelnikowi â€” nie pisz jak encyklopedia.
</rola>""")

    # â•â•â• 2. ZASADY PISANIA â•â•â•
    parts.append(f"""<zasady>
KaÅ¼de zdanie = nowa informacja. Fakt podany raz â€” potem skrÃ³t lub pomiÅ„.

DANE > OPINIA: konkretne liczby, wideÅ‚ki, stawki, wymiary.
  âœ… â€Malowanie z gÅ‚adziami: 60â€“120 zÅ‚/mÂ². Deska z montaÅ¼em: 150â€“250 zÅ‚/mÂ²."
  Gdy SERP podaje cenÄ™ â†’ PRZEPISZ wideÅ‚ki. 3+ pozycji z cenami â†’ tabela HTML (<table>).

STYL: fakt + co to znaczy dla czytelnika (portfel, kalendarz, zdrowie).
  âœ… â€NajczÄ™Å›ciej grzywna i zakaz na 3 lata â€” recydywa oznacza wiÄ™zienie bez zawieszenia."

RYTM: mieszaj dÅ‚ugoÅ›Ä‡ zdaÅ„. Czasem 5 sÅ‚Ã³w. Czasem 25. Trzy zbliÅ¼one pod rzÄ…d = monotonia.

ZDANIA: max 2 przecinki. Zdanie > 22 sÅ‚Ã³w â†’ rozbij. Jedno zdanie = jedna myÅ›l.
  Cel czytelnoÅ›ci: FOG-PL 8â€“9 (liceum). Wyrazy trudne = 4+ sylab â€” ograniczaj.

PODMIOT KONKRETNY: inwestor, lekarz, sÄ…d, ekipa â€” zamiast â€moÅ¼na", â€naleÅ¼y", â€warto".
  âœ… â€Wierzyciel skÅ‚ada wniosek"  âœ… â€SÄ…d bierze pod uwagÄ™"

OTWIERANIE SEKCJI: kaÅ¼da H2 od INNEGO zdania. Zacznij od: liczby, pytania, nazwy, sytuacji.
  FrazÄ™ kluczowÄ… umieszczaj w ÅšRODKU zdania â€” nie jako opener akapitu.

LISTY: 1â€“2 <ul> w artykule. WiÄ™kszoÅ›Ä‡ treÅ›ci = proza.

JEDNOSTKI: spacja przed jednostkÄ…, tysiÄ…ce ze spacjÄ…: âœ… 10 mÂ², 2 500 zÅ‚  âŒ 10mÂ², 2500zÅ‚

INTERPUNKCJA: przecinek przed: Å¼e, ktÃ³ry, poniewaÅ¼, aby.
  CofniÄ™ty przecinek: âœ… â€ZostanÄ™, mimo Å¼e..." âŒ â€ZostanÄ™ mimo, Å¼e..."
  ImiesÅ‚Ã³w (-Ä…c, -wszy): ten sam podmiot co zdanie gÅ‚Ã³wne + przecinek.

FORMAT: h2:/h3: dla nagÅ‚Ã³wkÃ³w. Zero markdown (**, __, #). KaÅ¼dy h2:/h3: w NOWEJ LINII.

NAZWY FIRM: Nurofen â†’ ibuprofen, OLX â†’ portal ogÅ‚oszeniowy.
</zasady>""")

    # â•â•â• 2b. ANTYREPETYCJE â•â•â•
    parts.append("""<antyrepetycje>
ZASADA PIERWSZEGO UÅ»YCIA: konkretna wartoÅ›Ä‡ (kwota, przepis, data) peÅ‚nÄ… formÄ… TYLKO RAZ.
  Potem: skrÃ³t, zaimek lub pomiÅ„. Trzecie powtÃ³rzenie = za duÅ¼o â†’ przepisz sekcjÄ™.

PRZEPISY: max 2Ã— ten sam artykuÅ‚ w caÅ‚ym tekÅ›cie (1Ã— definicja + 1Ã— sankcja/wyjÄ…tek).

KaÅ¼da sekcja H2 = nowa informacja. Pytaj siÄ™: â€Czego czytelnik dowie siÄ™ z TEJ sekcji,
  czego nie wiedziaÅ‚ po poprzedniej?" JeÅ›li odpowiedÅº siÄ™ pokrywa â€” to powtÃ³rzenie, nie sekcja.
</antyrepetycje>""")

    # â•â•â• 2c. SPÃ“JNOÅšÄ† STRUKTURY â•â•â•
    parts.append("""<spojnosc>
ZDANIE-MOST: sekcja 2+ zaczyna siÄ™ od krÃ³tkiego (max 15 sÅ‚Ã³w) nawiÄ…zania do poprzedniej.
  âœ… â€Skoro warunki speÅ‚nione â€” czas na dokumenty."  âœ… â€Koszty zaleÅ¼Ä… od trybu postÄ™powania."

KIERUNEK: ogÃ³Å‚ â†’ szczegÃ³Å‚ â†’ praktyka â†’ koszty. KaÅ¼dy H2 przesuwa czytelnika naprzÃ³d.

ZAMKNIÄ˜CIE SEKCJI: ostatnie zdanie = fakt lub liczba. MoraÅ‚, podsumowanie = usuÅ„.
  âœ… â€Czas oczekiwania: 14â€“30 dni roboczych."  âŒ â€Dlatego tak waÅ¼ne jest, aby..."
</spojnosc>""")

        # â•â•â• 3. ENTITY SEO â•â•â•
    parts.append("""<encje>
Encja gÅ‚Ã³wna = podmiot zdania, nie dopeÅ‚nienie. Stawiaj jÄ… na poczÄ…tku.
  âœ… â€Jazda po alkoholu skutkuje..."  âŒ â€WaÅ¼nym aspektem jest jazda po alkoholu"

POZYCJA: encja gÅ‚Ã³wna w 1. zdaniu artykuÅ‚u (podmiot). W co 3.â€“4. nagÅ‚Ã³wku H2.
  W kaÅ¼dej sekcji H2 â€” min. 1Ã— encja gÅ‚Ã³wna, rotuj formÄ™ fleksyjnÄ….

KOLOKACJA: powiÄ…zane encje w TYM SAMYM akapicie â€” nie rozrzucone po tekÅ›cie.
  âœ… â€Art. 178a KK penalizuje jazdÄ™ zakazem prowadzenia od 3 lat i Å›wiadczeniem od 5 000 zÅ‚."
  âŒ Lista tagÃ³w: â€art. 178a KK, zakaz prowadzenia, Å›wiadczenie pieniÄ™Å¼ne"

INFORMATION GAIN: w kaÅ¼dej sekcji H2 min. 1 element, ktÃ³rego NIE MA w danych z konkurencji.
CZYSTOÅšÄ†: kaÅ¼da sekcja H2 = JEDEN podtemat, wyczerpany do koÅ„ca.
</encje>""")

    # â•â•â• 4. JÄ˜ZYK: NATURALNOÅšÄ† + KOLOKACJE + ORTOGRAFIA â•â•â•
    parts.append("""<jezyk>
NATURALNY POLSKI â€” pisz jak redaktor, nie jak tÅ‚umacz z angielskiego.
  Podmiot + orzeczenie + dopeÅ‚nienie. Zdanie od podmiotu, nie od okolicznika.
  âœ… â€SÄ…d orzeka zakaz prowadzenia."  âŒ â€W odniesieniu do orzekania â€” sÄ…d moÅ¼e..."
  Puste startery: â€warto zauwaÅ¼yÄ‡", â€naleÅ¼y podkreÅ›liÄ‡", â€kluczowe jest",
    â€istotne jest", â€w tym kontekÅ›cie" â†’ USUÅƒ i zacznij od faktu.

KOLOKACJE POLSKIE (bÅ‚Ä…d = marker AI):
  âœ… podjÄ…Ä‡ decyzjÄ™          âŒ zrobiÄ‡ decyzjÄ™
  âœ… mocna kawa              âŒ silna kawa
  âœ… ponieÅ›Ä‡ konsekwencje    âŒ mieÄ‡ konsekwencje
  âœ… odnieÅ›Ä‡ sukces          âŒ osiÄ…gnÄ…Ä‡ sukces (kalk z EN)
  âœ… prowadziÄ‡ dziaÅ‚alnoÅ›Ä‡   âŒ robiÄ‡ dziaÅ‚alnoÅ›Ä‡
  âœ… zawrzeÄ‡ umowÄ™           âŒ zrobiÄ‡/podpisaÄ‡ umowÄ™ (podpisaÄ‡ = potocznie OK)
  âœ… wyciÄ…gnÄ…Ä‡ wnioski       âŒ zrobiÄ‡ wnioski
  âœ… budziÄ‡ wÄ…tpliwoÅ›ci      âŒ rodziÄ‡ wÄ…tpliwoÅ›ci (rodziÄ‡ = przestarzaÅ‚e, ale akceptowane)
  âœ… speÅ‚niÄ‡ warunki         âŒ wypeÅ‚niÄ‡ warunki (wypeÅ‚niÄ‡ formularz â‰  warunki)
  âœ… odgrywaÄ‡ rolÄ™           âŒ graÄ‡ rolÄ™ (graÄ‡ = teatr)
  âœ… nabraÄ‡ przekonania      âŒ zyskaÄ‡ przekonanie
  âœ… zasiÄ™gnÄ…Ä‡ opinii        âŒ wziÄ…Ä‡ opiniÄ™
  âœ… wyrzÄ…dziÄ‡ szkodÄ™        âŒ zrobiÄ‡ szkodÄ™
  âœ… wywrzeÄ‡ wpÅ‚yw           âŒ zrobiÄ‡ wpÅ‚yw
  âœ… postawiÄ‡ diagnozÄ™       âŒ zrobiÄ‡ diagnozÄ™
  âœ… zÅ‚oÅ¼yÄ‡ wniosek          âŒ zrobiÄ‡ wniosek
  âœ… doznaÄ‡ obraÅ¼eÅ„          âŒ otrzymaÄ‡ obraÅ¼enia
  âœ… peÅ‚niÄ‡ funkcjÄ™          âŒ robiÄ‡ funkcjÄ™
  âœ… braÄ‡ pod uwagÄ™          âŒ braÄ‡ na uwagÄ™
  âœ… w dalszym ciÄ…gu         âŒ w dalszym stopniu

PUSTE PODMIOTNIKI: â€ta sytuacja / ten problem / ten aspekt" â†’ nazwij KTO lub CO.

PRO-DROP: polszczyzna nie potrzebuje zaimkÃ³w osobowych. âœ… â€IdÄ™" âŒ â€Ja idÄ™".
  Strona czynna > bierna: âœ… â€Uchwalono nowe zasady" âŒ â€ZostaÅ‚y wprowadzone zmiany".
  Czasowniki > nominalizacje: âœ… â€wdroÅ¼yÄ‡" âŒ â€dokonanie realizacji procesu wdroÅ¼enia".

ORTOGRAFIA 2026 (reforma RJP weszÅ‚a w Å¼ycie):
  â€¢ nie + przymiotnik/przysÅ‚Ã³wek ÅÄ„CZNIE gdy orzeka cechÄ™: â€niedrogi", â€nielepszy", â€niedaleko".
    ROZDZIELNIE tylko przy przeciwstawieniu z â€lecz/ale": â€nie drogi, lecz tani".
  â€¢ Zapis â€w ogÃ³le" (nie â€wogÃ³le"). â€Z powrotem" (nie â€spowrotem").
  â€¢ â€-by" z osobowymi formami cz. ÅÄ„CZNIE: â€zrobiÅ‚by", â€poszÅ‚aby".
    Z innymi wyrazami ROZDZIELNIE: â€kto by pomyÅ›laÅ‚", â€jakby nie patrzeÄ‡".
  â€¢ PartykuÅ‚a â€by" po â€Å¼e", â€gdy", â€chociaÅ¼" â€” ÅÄ„CZNIE: â€Å¼eby", â€gdyby", â€choÄ‡by".
  â€¢ Wielka litera: nazwy Å›wiÄ…t (BoÅ¼e Narodzenie), nazwy dokumentÃ³w urzÄ™dowych
    (Kodeks karny â€” ale: kodeks karny gdy opisowo).
</jezyk>""")

    # â•â•â• 5. Å¹RÃ“DÅA â•â•â•
    if is_ymyl:
        parts.append("""<zrodla>
YMYL â€” zero tolerancji dla zmyÅ›leÅ„.
Wiedza WYÅÄ„CZNIE z: stron SERP (podane), przepisÃ³w (podane), Wikipedia (podane).
Nie wymyÅ›laj liczb, dat, sygnatur, nazw badaÅ„. Nie znasz â†’ pomiÅ„.
</zrodla>""")
    else:
        parts.append("""<zrodla>
Wiedza z: stron SERP, Wikipedia, danych liczbowych (podane).
Nie wymyÅ›laj liczb, dat, nazw badaÅ„. Brak danych â†’ opisz ogÃ³lnie.
Gdy SERP podaje cenÄ™/stawkÄ™ â†’ PRZEPISZ wideÅ‚ki. Nie streszczaj liczb sÅ‚owami.
</zrodla>""")

    # â•â•â• 5b. STYL KATEGORII â•â•â•
    cat_style = _CATEGORY_STYLE.get(detected_category, "")
    if cat_style:
        parts.append(f"<styl_kategorii>\n{cat_style}\n</styl_kategorii>")

    # â•â•â• 6. PRZYKÅAD (per-kategoria) â•â•â•
    _EXAMPLES = {
        "prawo": (
            'TAK: "Granica jest prosta: do 0,5 promila to wykroczenie, powyÅ¼ej â€” przestÄ™pstwo.\n'
            'Typowy kierowca zÅ‚apany pierwszy raz z wynikiem tuÅ¼ ponad prÃ³g dostanie\n'
            'grzywnÄ™ i zakaz na 3 lata. Brak oÅ›wiadczenia w terminie 6 mies.\n'
            '= przyjÄ™cie spadku z dobrodziejstwem inwentarza (art. 1015 Â§ 2 k.c.)."\n\n'
            'NIE: "Sytuacja prawna kierowcy ulega zmianie w zaleÅ¼noÅ›ci od okolicznoÅ›ci.\n'
            'Ten aspekt jest szczegÃ³lnie istotny w kontekÅ›cie aktualnych regulacji."\n'
            'â†‘ dwa zdania, ZERO konkretÃ³w â€” brak artykuÅ‚u, brak kary, brak scenariusza. UsuÅ„.'
        ),
        "medycyna": (
            'TAK: "Ibuprofen 400 mg co 6â€“8 h Å‚agodzi bÃ³l w ciÄ…gu 30â€“60 min\n'
            'â€” blokuje cyklooksygenazÄ™, hamujÄ…c syntezÄ™ prostaglandyn.\n'
            'PowyÅ¼ej 3 dni gorÄ…czki u dziecka â€” wizyta u pediatry jest konieczna,\n'
            'nie Â«warto siÄ™ skonsultowaÄ‡Â»."\n\n'
            'NIE: "Lek skutecznie pomaga na dolegliwoÅ›ci. Ten problem jest powszechny."\n'
            'â†‘ brak dawki, mechanizmu, nazwy substancji. UsuÅ„.'
        ),
        "finanse": (
            'TAK: "ZdolnoÅ›Ä‡ kredytowa rodziny z dochodem 15 000 zÅ‚ netto:\n'
            'VeloBank â€” ok. 1,1 mln zÅ‚, Millennium â€” ok. 950 000 zÅ‚.\n'
            'Karta kredytowa z limitem 10 000 zÅ‚ obniÅ¼a zdolnoÅ›Ä‡ nawet przy zerowym saldzie\n'
            'â€” bank liczy potencjalne zadÅ‚uÅ¼enie. Zmiana z umowy zlecenia na o pracÄ™\n'
            'podnosi zdolnoÅ›Ä‡ o 15â€“20 % â€” nie przez wyÅ¼sze zarobki, lecz innÄ… wycenÄ™ stabilnoÅ›ci."\n\n'
            'NIE: "Warto rozwaÅ¼yÄ‡ skorzystanie z atrakcyjnej oferty kredytowej."\n'
            'â†‘ ZERO: brak banku, brak kwoty, brak oprocentowania. UsuÅ„.'
        ),
        "technologia": (
            'TAK: "ASRock B860M (ok. 600 zÅ‚) â€” DDR5, M.2 PCIe 5.0, Wi-Fi 6E.\n'
            'Wystarczy do wydajnego komputera bez podkrÄ™cania.\n'
            'Premium: ASUS ROG Maximus Z890 (ok. 3 000 zÅ‚) â€” trzy M.2 PCIe 5.0,\n'
            'Thunderbolt 4, Wi-Fi 7. RÃ³Å¼nica piÄ™ciokrotna w cenie â€” opÅ‚acalna\n'
            'przy topowych Core Ultra 9, zbÄ™dna przy i5."\n\n'
            'NIE: "Ta pÅ‚yta gÅ‚Ã³wna oferuje imponujÄ…ce parametry w przystÄ™pnej cenie."\n'
            'â†‘ ZERO: brak modelu, brak ceny, brak parametru. UsuÅ„.'
        ),
        "budownictwo": (
            'TAK: "Ocieplenie Å›cian â€” norma WT: U â‰¤ 0,20 W/(mÂ²Â·K).\n'
            'Styropian grafitowy (Î» = 0,032): 14 cm. BiaÅ‚y (Î» = 0,038): 18â€“20 cm.\n'
            'Koszt kompletny: materiaÅ‚ 50 zÅ‚/mÂ² + robocizna 110 zÅ‚/mÂ² = 160 zÅ‚/mÂ² bez tynku.\n'
            'Tynk: +40â€“60 zÅ‚/mÂ². Klejenie przy temp. 5â€“25Â°C, dni suche."\n\n'
            'NIE: "WykoÅ„czenie domu zaczyna siÄ™ od sprawdzenia stanu deweloperskiego.\n'
            'Ta sytuacja zmienia budÅ¼et."\n'
            'â†‘ ZERO liczb, brak Î», brak cen materiaÅ‚/robocizna. UsuÅ„.'
        ),
        "uroda": (
            'TAK: "Niacynamid (INCI: Niacinamide, witamina B3) reguluje sebum,\n'
            'hamuje transfer melaniny, wspiera syntezÄ™ ceramidÃ³w.\n'
            'Skuteczne stÄ™Å¼enie: od 2 %, optymalnie 5 %. PowyÅ¼ej 10 % â€” ryzyko zaczerwienienia.\n'
            'ÅÄ…czy siÄ™ z retinolem (Å‚agodzi efekty uboczne). OstroÅ¼noÅ›Ä‡ z AHA/BHA â€” rÃ³Å¼nica pH.\n'
            'Cera tÅ‚usta/mieszana: serum 5 % rano, pod krem + SPF."\n\n'
            'NIE: "Ten kultowy skÅ‚adnik to absolutny must-have w kaÅ¼dej rutynie."\n'
            'â†‘ brak INCI, brak stÄ™Å¼enia, brak mechanizmu, brak typu cery. UsuÅ„.'
        ),
        "lifestyle": (
            'TAK: "Baleriny na koturnach â€” Miu Miu, AlaÃ¯a, Simone Rocha â€” po trzech sezonach\n'
            'platform wybiegi skrÄ™ciÅ‚y w stronÄ™ lekkoÅ›ci. Trend nie jest nowy:\n'
            'Ferragamo eksperymentowaÅ‚ z niskim koturnem juÅ¼ w latach 40.\n'
            'DziÅ› powrÃ³t wiÄ…Å¼e siÄ™ z estetykÄ… quiet luxury â€” mniej platformy, wiÄ™cej proporcji."\n\n'
            'NIE: "Ten niesamowity trend podbija wybiegi na caÅ‚ym Å›wiecie."\n'
            'â†‘ brak projektanta, brak kolekcji, brak kontekstu kulturowego. UsuÅ„.'
        ),
        "inne": (
            'TAK: "Zakwas na chleb Å¼ytni dojrzewa 5â€“7 dni: mÄ…ka razowa + woda 1:1,\n'
            'dokarmianie co 24 h w 24â€“26Â°C. Gotowy zakwas: pH 3,5â€“4,0.\n'
            'Proporcja do wypieku: 20â€“30 % masy mÄ…ki. Przy 500 g mÄ…ki = 100â€“150 g zakwasu."\n\n'
            'NIE: "Pieczenie chleba to niesamowita przygoda kulinarna.\n'
            'Ten proces wymaga cierpliwoÅ›ci."\n'
            'â†‘ ZERO danych â€” brak proporcji, temperatury, czasu. UsuÅ„.'
        ),
    }
    _default_example = (
        'TAK: Zdanie z konkretnÄ… liczbÄ…, nazwÄ… wÅ‚asnÄ…, datÄ… lub ÅºrÃ³dÅ‚em.\n'
        'NIE: Zdanie ogÃ³lnikowe â€” "ta sytuacja", "ten problem", "niesamowity" = do usuniÄ™cia.'
    )
    example_text = _EXAMPLES.get(detected_category, _default_example)
    parts.append(f"<przyklad>\n{example_text}\n</przyklad>")

    return "\n\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCHEMA GUARD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_CRITICAL_FIELDS = ["keywords", "main_keyword", "batch_number"]
_IMPORTANT_FIELDS = [
    "gpt_instructions_v39", "enhanced", "h2_remaining",
    "article_memory", "keyword_limits", "coverage",
]

def _schema_guard(pre_batch):
    missing_critical = [f for f in _CRITICAL_FIELDS if f not in pre_batch or pre_batch[f] is None]
    missing_important = [f for f in _IMPORTANT_FIELDS if f not in pre_batch or pre_batch[f] is None]
    if missing_critical:
        _pb_logger.warning(f"âš ï¸ SCHEMA GUARD: Missing CRITICAL fields: {missing_critical}.")
    if missing_important:
        _pb_logger.info(f"â„¹ï¸ Schema guard: Missing optional: {missing_important}")
    enhanced = pre_batch.get("enhanced") or {}
    if enhanced:
        expected = ["smart_instructions_formatted", "causal_context", "information_gain", "relations_to_establish"]
        missing_enh = [f for f in expected if not enhanced.get(f)]
        if missing_enh:
            _pb_logger.info(f"â„¹ï¸ Enhanced missing: {missing_enh}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USER PROMPT (v2.1 â€” 10 formatterÃ³w)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_user_prompt(pre_batch, h2, batch_type, article_memory=None):
    pre_batch = pre_batch or {}
    sections = []

    _schema_guard(pre_batch)

    formatters = [
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_keywords(pre_batch),
        lambda: _fmt_legal_medical(pre_batch),
        lambda: _fmt_entity_context_v2(pre_batch),
        lambda: _fmt_natural_polish(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_serp_enrichment_v2(pre_batch),
        lambda: _fmt_h2_remaining(pre_batch),
        lambda: _fmt_intro_guidance_v2(pre_batch, batch_type),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception as exc:
            _pb_logger.warning(f"Formatter failed: {exc}")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHARED FORMATTERS (used by article + category prompts)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_batch_header(pre_batch, h2, batch_type):
    batch_number = pre_batch.get("batch_number", 1)
    total_batches = pre_batch.get("total_planned_batches", 1)
    batch_length = pre_batch.get("batch_length") or {}

    # INTRO: fixed length, no section header
    if batch_type in ("INTRO", "intro"):
        return f"""â•â•â• BATCH {batch_number}/{total_batches}: INTRO â•â•â•
DÅ‚ugoÅ›Ä‡: 120-200 sÅ‚Ã³w"""

    min_w = batch_length.get("min_words", 350)
    max_w = batch_length.get("max_words", 500)

    section_length = pre_batch.get("section_length_guidance") or {}
    length_hint = ""
    if section_length:
        suggested = section_length.get("suggested_words") or section_length.get("target_words")
        if suggested:
            length_hint = f"\nSugerowana dÅ‚ugoÅ›Ä‡ tej sekcji: ~{suggested} sÅ‚Ã³w."

    return f"""â•â•â• BATCH {batch_number}/{total_batches}: {batch_type} â•â•â•
Sekcja H2: {h2}
DÅ‚ugoÅ›Ä‡: {min_w}-{max_w} sÅ‚Ã³w{length_hint}
Zaczynaj DOKÅADNIE od: h2: {h2}"""


def _parse_target_max(target_total_str):
    if not target_total_str:
        return 0
    if isinstance(target_total_str, (int, float)):
        return int(target_total_str)
    try:
        parts = str(target_total_str).replace("x", "").split("-")
        if len(parts) >= 2:
            return int(parts[-1].strip())
        return int(parts[0].strip())
    except (ValueError, IndexError):
        return 0


def _get_kw_variants(name, pre_batch):
    """v67: Get fleksyjne + peryfrazy for a keyword from search_variants.
    
    Returns (fleksyjne_list, peryfrazy_list) â€” both may be empty.
    Checks: search_variants.secondary[name], search_variants.fleksyjne (for main kw),
    and entity_variants as fallback.
    """
    sv = pre_batch.get("_search_variants") or {}
    secondary = sv.get("secondary", {})
    
    # 1. Check secondary dict (per-keyword variants)
    name_lower = name.lower().strip()
    for key, variants in secondary.items():
        if key.lower().strip() == name_lower:
            # secondary variants are a mixed list â€” split into fleksyjne/peryfrazy
            # heuristic: short variants (Â±3 words diff) = fleksyjne, longer = peryfrazy
            base_words = len(name.split())
            fleks = [v for v in variants if abs(len(v.split()) - base_words) <= 1]
            peri = [v for v in variants if abs(len(v.split()) - base_words) > 1]
            return fleks[:3], peri[:3]
    
    # 2. For main keyword â€” use top-level fleksyjne/peryfrazy
    _raw_main = pre_batch.get("main_keyword") or {}
    main_kw = _raw_main.get("keyword", "") if isinstance(_raw_main, dict) else str(_raw_main)
    if main_kw and name_lower == main_kw.lower().strip():
        fleks = sv.get("fleksyjne", [])[:3]
        peri = sv.get("peryfrazy", [])[:3]
        return fleks, peri
    
    # 3. Fallback to entity_variants
    entity_variants = pre_batch.get("_entity_variants") or secondary
    variants = _find_variants(name, entity_variants)
    if variants:
        return variants[:2], []
    
    return [], []


def _fmt_keywords(pre_batch):
    keywords_info = pre_batch.get("keywords") or {}
    keyword_limits = pre_batch.get("keyword_limits") or {}
    soft_caps = pre_batch.get("soft_cap_recommendations") or {}

    _kw_global_remaining = pre_batch.get("_kw_global_remaining", None)
    _main_kw_budget_exhausted = (_kw_global_remaining is not None and _kw_global_remaining == 0)
    _raw_main_kw = pre_batch.get("main_keyword") or {}
    main_kw = _raw_main_kw.get("keyword", "") if isinstance(_raw_main_kw, dict) else str(_raw_main_kw)

    # â”€â”€ MUST USE â”€â”€
    must_raw = keywords_info.get("basic_must_use", [])
    must_lines = []
    _budget_exhausted_kws = []
    for kw in must_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            if _main_kw_budget_exhausted and name and main_kw and name.lower() == main_kw.lower():
                _budget_exhausted_kws.append(name)
                continue
            actual = kw.get("actual", kw.get("actual_uses", kw.get("current_count", 0)))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            hard_max = kw.get("hard_max_this_batch", "")
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            line = f'  â€¢ "{name}"'
            if hard_max:
                line += f" (max {hard_max}Ã—)"
            elif remaining and int(remaining) <= 2:
                line += f" (jeszcze {remaining}Ã—)"
            # v67: Add variant hints â€” fleksyjne + peryfrazy
            fleks, peri = _get_kw_variants(name, pre_batch)
            if fleks:
                line += f'\n    odmiany: {", ".join(fleks)}'
            if peri:
                line += f'\n    peryfrazy: {", ".join(peri)}'
            must_lines.append(line)
        else:
            line = f'  â€¢ "{kw}"'
            fleks, peri = _get_kw_variants(str(kw), pre_batch)
            if fleks:
                line += f'\n    odmiany: {", ".join(fleks)}'
            if peri:
                line += f'\n    peryfrazy: {", ".join(peri)}'
            must_lines.append(line)

    # â”€â”€ EXTENDED â”€â”€
    ext_raw = keywords_info.get("extended_this_batch", [])
    ext_lines = []
    for kw in ext_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            line = f'  â€¢ "{name}"'
            # v67: Variant hints for extended too
            fleks, peri = _get_kw_variants(name, pre_batch)
            if peri:
                line += f' (lub: {", ".join(peri[:2])})'
            elif fleks:
                line += f' (lub: {", ".join(fleks[:2])})'
            ext_lines.append(line)
        else:
            line = f'  â€¢ "{kw}"'
            fleks, peri = _get_kw_variants(str(kw), pre_batch)
            if peri:
                line += f' (lub: {", ".join(peri[:2])})'
            elif fleks:
                line += f' (lub: {", ".join(fleks[:2])})'
            ext_lines.append(line)

    # â”€â”€ STOP â”€â”€
    stop_raw = keyword_limits.get("stop_keywords") or []
    entity_variants = pre_batch.get("_entity_variants") or \
        (pre_batch.get("_search_variants") or {}).get("secondary", {})
    stop_lines = []
    for s in stop_raw:
        if isinstance(s, dict):
            name = s.get("keyword", "")
            current = s.get("current_count", s.get("current", s.get("actual", "?")))
            max_c = s.get("max_count", s.get("max", s.get("target_max", "?")))
            line = f'  â€¢ "{name}" (juÅ¼ {current}Ã—, limit {max_c}) STOP!'
            # v2.3: Show variant replacements
            variants = _find_variants(name, entity_variants)
            if variants:
                line += f'\n    â†’ zamiast uÅ¼yj: {", ".join(variants[:4])}'
            stop_lines.append(line)
        else:
            line = f'  â€¢ "{s}"'
            variants = _find_variants(str(s), entity_variants)
            if variants:
                line += f'\n    â†’ zamiast uÅ¼yj: {", ".join(variants[:4])}'
            stop_lines.append(line)
    for exhausted_kw in _budget_exhausted_kws:
        line = f'  â€¢ "{exhausted_kw}" (limit globalny osiÄ…gniÄ™ty â€” NIE UÅ»YWAJ!)'
        variants = _find_variants(exhausted_kw, entity_variants)
        if variants:
            line += f'\n    â†’ zamiast uÅ¼yj: {", ".join(variants[:4])}'
        stop_lines.append(line)

    # â”€â”€ CAUTION â”€â”€
    caution_raw = keyword_limits.get("caution_keywords") or []
    caution_names = []
    caution_variant_hints = []
    for c in caution_raw:
        if isinstance(c, dict):
            name = c.get("keyword", "")
            caution_names.append(name)
        else:
            name = str(c)
            caution_names.append(name)
        if name:
            variants = _find_variants(name, entity_variants)
            if variants:
                caution_variant_hints.append(f'  "{name}" â†’ {", ".join(variants[:3])}')
    caution_names = [n for n in caution_names if n]

    # â”€â”€ SOFT CAPS â”€â”€
    soft_notes = []
    if soft_caps:
        for kw_name, info in soft_caps.items():
            if isinstance(info, dict):
                action = info.get("action", "")
                if action and action != "OK":
                    soft_notes.append(f'  â„¹ï¸ "{kw_name}": {action}')

    _kw_force_ban = pre_batch.get("_kw_force_ban", False)
    if _kw_force_ban and main_kw:
        must_lines = [l for l in must_lines if main_kw.lower() not in l.lower()]

    # â”€â”€ BUILD â”€â”€
    parts = ["â•â•â• FRAZY KLUCZOWE â•â•â•"]
    parts.append("âš¡ ROTACJA FORM: Google liczy odmiany jako to samo slowo (lematyzacja).\n"
                 "  'wykroczenie' + 'wykroczenia' + 'wykroczeniem' = 3 uzycia jednego lematu.\n"
                 "  Dlatego: NIE powtarzaj exact match â€” rotuj przez odmiany i peryfrazy.\n"
                 "  Jesli fraza ma podane odmiany/peryfrazy â€” UZYWAJ ICH zamiast powtarzac te sama forme.")
    # v67: Anti-paragraph-opener rule â€” prevents MK stuffing pattern
    if main_kw:
        parts.append(f'ğŸš« ZAKAZ ANAFORY: NIGDY nie zaczynaj akapitu od frazy kluczowej "{main_kw}".\n'
                     f'  âŒ "{main_kw} to..." / "{main_kw} bywa..." / "{main_kw} lekami..."\n'
                     f'  âœ… Zacznij od kontekstu, konsekwencji, pytania lub zaimka.\n'
                     f'  Jesli musisz uzyc frazy â€” wstaw ja w SRODEK zdania, nie na poczatku.')

    if _kw_force_ban and main_kw:
        parts.append(f'â›” STOP: Fraza "{main_kw}" jest PRZEKROCZONA â€” nie uÅ¼ywaj w tym batchu.\n')

    if must_lines:
        parts.append("TEMATY OBOWIÄ„ZKOWE (poruszyj w treÅ›ci):")
        parts.extend(must_lines)
    if ext_lines:
        parts.append("\nTEMATY DODATKOWE (wpleÄ‡ jeÅ›li pasujÄ…):")
        parts.extend(ext_lines)
    if stop_lines:
        parts.append("\nğŸ›‘ STOP â€” nie uÅ¼ywaj (przekroczone):")
        parts.extend(stop_lines)
    if caution_names:
        parts.append(f"\nâš ï¸ OSTROÅ»NIE (max 1Ã— kaÅ¼da): {', '.join(caution_names)}")
        if caution_variant_hints:
            parts.extend(caution_variant_hints)
    if soft_notes:
        parts.append("")
        parts.extend(soft_notes)

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_continuation(pre_batch):
    continuation = pre_batch.get("continuation_v39") or {}
    enhanced = pre_batch.get("enhanced") or {}
    cont_ctx = enhanced.get("continuation_context") or {}

    last_h2 = cont_ctx.get("last_h2") or continuation.get("last_h2", "")
    last_ending = cont_ctx.get("last_paragraph_ending") or continuation.get("last_paragraph_ending", "")
    last_topic = cont_ctx.get("last_topic") or continuation.get("last_topic", "")
    transition_hint = continuation.get("transition_hint", "")

    if not last_h2 and not last_ending:
        return ""

    parts = ["â•â•â• KONTYNUACJA â•â•â•", "Poprzedni batch zakoÅ„czyÅ‚ siÄ™ na:"]
    if last_h2:
        parts.append(f'  Ostatni H2: "{last_h2}"')
    if last_ending:
        ending_preview = last_ending[:150] + ("..." if len(last_ending) > 150 else "")
        parts.append(f'  Ostatnie zdanie: "{ending_preview}"')
    if last_topic:
        parts.append(f'  Temat: {last_topic}')
    parts.append("\nZacznij PÅYNNIE: nawiÄ…Å¼ do poprzedniego wÄ…tku, ale nie powtarzaj zakoÅ„czenia.")
    if transition_hint:
        parts.append(f'Sugerowane przejÅ›cie: {transition_hint}')
    return "\n".join(parts)


def _fmt_article_memory(article_memory):
    if not article_memory:
        return ""

    parts = ["â•â•â• PAMIÄ˜Ä† ARTYKUÅU â•â•â•"]

    if isinstance(article_memory, dict):
        topics = article_memory.get("topics_covered") or article_memory.get("covered_topics") or []
        if topics:
            parts.append("Sekcje juÅ¼ napisane:")
            for t in topics[:10]:
                if isinstance(t, str):
                    parts.append(f'  âœ“ {t}')
                elif isinstance(t, dict):
                    parts.append(f'  âœ“ {t.get("topic", t.get("h2", ""))}')

        # â”€â”€ KONKRETNE WARTOÅšCI: zakaz powtarzania â”€â”€
        concrete_facts = article_memory.get("concrete_facts_used") or []
        if concrete_facts:
            parts.append(
                "\nğŸš« WARTOÅšCI JUÅ» UÅ»YTE â€” nie pisz ich ponownie peÅ‚nÄ… formÄ… "
                "(maks. skrÃ³t jeÅ›li absolutnie konieczne, np. \"ww. kwota\", \"wspomniany przepis\"):"
            )
            for v in concrete_facts[:30]:
                parts.append(f'  âŒ {v}')

        facts = article_memory.get("key_facts_used") or article_memory.get("facts", [])
        key_points = article_memory.get("key_points") or []
        avoid_rep = article_memory.get("avoid_repetition") or []

        all_facts = list(facts) + list(key_points)
        if all_facts:
            parts.append("\nFakty juÅ¼ podane (NIE POWTARZAJ):")
            for f in all_facts[:12]:
                parts.append(f'  â€¢ {f}' if isinstance(f, str) else f'  â€¢ {json.dumps(f, ensure_ascii=False)[:100]}')

        if avoid_rep:
            parts.append("\nâ›” UÅ»YTE ZDANIA â€” NIE POWTARZAJ DOSÅOWNIE:")
            for r in avoid_rep[:8]:
                parts.append(f'  âŒ "{r}"')

        # â”€â”€ PRE-ANALIZA (technika #6 z badaÅ„ â€” najskuteczniejsza) â”€â”€
        # Zmuszamy model do wylistowania zakazÃ³w ZANIM zacznie pisaÄ‡.
        # Badania: modele ktÃ³re "widzÄ…" co jest zakazane przed generowaniem
        # produkujÄ… ~90% mniej duplikacji niÅ¼ te z samymi instrukcjami.
        if topics or concrete_facts or all_facts:
            batch_n = len(topics) + 1
            parts.append(
                f"\nğŸ“‹ PRZED NAPISANIEM SEKCJI {batch_n} wykonaj w myÅ›lach analizÄ™:\n"
                "  1. Jakie konkretne wartoÅ›ci (kwoty, przepisy, daty) juÅ¼ padÅ‚y? â†’ nie powtarzaj ich peÅ‚nÄ… formÄ…\n"
                "  2. JakÄ… myÅ›l koÅ„czyÅ‚a poprzednia sekcja? â†’ zacznij od zdania-mostu, nie od tej samej myÅ›li\n"
                "  3. Co NOWEGO wnosi ta sekcja, czego poprzednie nie omÃ³wiÅ‚y? â†’ to jest Twoja teza\n"
                "Dopiero po tej analizie zacznij pisaÄ‡."
            )

    elif isinstance(article_memory, str):
        parts.append(_word_trim(article_memory, 1500))

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_h2_remaining(pre_batch):
    h2_remaining = pre_batch.get("h2_remaining") or []
    if not h2_remaining:
        return ""
    h2_list = ", ".join(f'"{h}"' for h in h2_remaining[:6])
    return f"â•â•â• PLAN â•â•â•\nPozostaÅ‚e sekcje H2: {h2_list}\nNie zachodÅº na ich tematy."


def _fmt_output_format(h2, batch_type):
    if batch_type in ("INTRO", "intro"):
        return """â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ leadu. NIE zaczynaj od "h2:". Lead nie ma nagÅ‚Ã³wka.
120-200 sÅ‚Ã³w. FrazÄ™ gÅ‚Ã³wnÄ… wpleÄ‡ w PIERWSZE zdanie.
NIE dodawaj komentarzy, meta-tekstu. TYLKO treÅ›Ä‡ leadu."""

    return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ tego batcha. Zaczynaj od:

h2: {h2}

Akapity po 3-5 zdaÅ„. Opcjonalnie h3: [podsekcja].
Gdy masz 3+ warunkÃ³w/krokÃ³w/wymagaÅ„ â†’ lista <ul><li> (max 1-2 listy w artykule).
Gdy porÃ³wnujesz dane liczbowe â†’ tabela <table> (max 1 w artykule).
KaÅ¼dy akapit powinien zawieraÄ‡ min. 1 konkretny fakt (liczbÄ™, stawkÄ™, wymiar, termin).
Zdania bez informacji ("Ta sytuacja...", "Ten problem...") = DO USUNIÄ˜CIA.
Zdanie z 3+ przecinkami = za zÅ‚oÅ¼one â†’ rozbij na dwa zdania.
KAÅ»DY h3: na OSOBNEJ linii z pustÄ… liniÄ… powyÅ¼ej i poniÅ¼ej.
Å»ADEN nagÅ‚Ã³wek NIE moÅ¼e byÄ‡ wklejony w Å›rodek akapitu.
Zero markdown (**, __, #). Zero tagÃ³w HTML (<h2>, <h3>, <b>).
NIE dodawaj komentarzy. TYLKO treÅ›Ä‡ artykuÅ‚u."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEW v2 FORMATTERS (article only)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_entity_context_v2(pre_batch):
    """v2.3: Smart S1 context â€” per-H2 filtered data from _build_batch_s1_context."""
    parts = []
    s1_ctx = pre_batch.get("_s1_context") or {}

    _raw_main = pre_batch.get("main_keyword") or {}
    main_name = _raw_main.get("keyword", "") if isinstance(_raw_main, dict) else str(_raw_main)
    _entity_seo = (pre_batch.get("s1_data") or {}).get("entity_seo") or \
        pre_batch.get("entity_seo") or {}

    # â”€â”€ Block 1: Synonyms (from search_variants or fallback to entity_synonyms) â”€â”€
    if main_name:
        sv = pre_batch.get("_search_variants") or {}
        peryfrazy = sv.get("peryfrazy", [])
        if peryfrazy:
            parts.append(f"â•â•â• ENCJE â•â•â•\nSynonimy: {', '.join(peryfrazy[:5])}")
        else:
            synonyms = _entity_seo.get("entity_synonyms", [])[:5]
            if synonyms:
                parts.append(f"â•â•â• ENCJE â•â•â•\nSynonimy: {', '.join(str(s) for s in synonyms)}")
            else:
                parts.append("â•â•â• ENCJE â•â•â•")

    # â”€â”€ Block 2: Lead entity + concepts for THIS section â”€â”€
    lead = s1_ctx.get("lead_entity")
    concepts = s1_ctx.get("concepts", [])
    e_gaps = s1_ctx.get("entity_gaps", [])

    concept_parts = []
    if lead and lead.lower() != main_name.lower():
        concept_parts.append(f"ğŸ¯ Encja wiodÄ…ca sekcji: {lead}")
    all_to_weave = concepts[:]
    for g in e_gaps:
        if g not in all_to_weave:
            all_to_weave.append(f"{g} [luka]")
    if all_to_weave:
        concept_parts.append(f"WpleÄ‡: {', '.join(all_to_weave[:6])}")
    if concept_parts:
        parts.append("\n".join(concept_parts))

    # â”€â”€ Block 3: EAV facts (filtered per H2) â”€â”€
    eav = s1_ctx.get("eav", [])
    if eav:
        eav_lines = ["Fakty (wpleÄ‡ w zdania, nie listuj):"]
        for e in eav[:5]:
            marker = "ğŸ¯" if e.get("is_primary") else "â€¢"
            eav_lines.append(f'  {marker} {e.get("entity","")} â†’ {e.get("attribute","")} â†’ {e.get("value","")}')
        parts.append("\n".join(eav_lines))

    # â”€â”€ Block 4: SVO relations (filtered per H2 â€” NEW in article prompt) â”€â”€
    svo = s1_ctx.get("svo", [])
    if svo:
        svo_lines = ["Relacje (opisz swoimi sÅ‚owami):"]
        for t in svo[:3]:
            ctx = f' [{t.get("context","")}]' if t.get("context") else ""
            svo_lines.append(f'  â€¢ {t.get("subject","")} â†’ {t.get("verb","")} â†’ {t.get("object","")}{ctx}')
        parts.append("\n".join(svo_lines))

    # â”€â”€ Block 5: Causal chains (NEW â€” first time in article prompt) â”€â”€
    causal = s1_ctx.get("causal", [])
    if causal:
        causal_lines = ["ÅaÅ„cuchy przyczynowe (uÅ¼yj do wyjaÅ›niania DLACZEGO):"]
        for c in causal[:2]:
            if isinstance(c, dict):
                text = c.get("chain", c.get("text", str(c)))
            else:
                text = str(c)
            causal_lines.append(f"  â›“ï¸ {_word_trim(text, 150)}")
        parts.append("\n".join(causal_lines))

    # â”€â”€ Block 6: Content gaps for THIS section (NEW) â”€â”€
    gaps = s1_ctx.get("gaps", [])
    if gaps:
        parts.append(f"Luki TOP10 (information gain): {', '.join(gaps[:3])}")

    # â”€â”€ Block 7: Co-occurrence pairs for THIS section â”€â”€
    cooc = s1_ctx.get("cooc", [])
    if cooc:
        parts.append(f"Encje razem w akapicie: {' | '.join(cooc[:4])}")

    # â”€â”€ Block 8: Information gain (from master API, per-batch) â”€â”€
    enhanced = pre_batch.get("enhanced") or {}
    info_gain = enhanced.get("information_gain", "")
    if info_gain:
        parts.append(f"Przewaga nad konkurencjÄ…: {_word_trim(info_gain, 200)}")

    # â”€â”€ Block 9: Semantic angle (from master API, per-batch) â”€â”€
    plan = pre_batch.get("semantic_batch_plan") or {}
    if plan:
        h2_coverage = plan.get("h2_coverage") or {}
        for h2_name, info in h2_coverage.items():
            if isinstance(info, dict):
                angle = info.get("semantic_angle", "")
                if angle:
                    parts.append(f"KÄ…t sekcji: {angle}")
                    break

    # â”€â”€ Fallback: if _s1_context empty, use old static fields â”€â”€
    if not s1_ctx:
        must_concepts = pre_batch.get("_must_cover_concepts") or []
        old_eav = pre_batch.get("_eav_triples") or []
        old_gaps = pre_batch.get("_entity_gaps") or []
        if must_concepts:
            names = [c.get("text", c) if isinstance(c, dict) else str(c) for c in must_concepts[:8]]
            parts.append(f"WpleÄ‡: {', '.join(n for n in names if n)}")
        if old_eav:
            eav_lines = ["Fakty (wpleÄ‡ w zdania):"]
            for e in old_eav[:4]:
                eav_lines.append(f'  â€¢ {e.get("entity","")} â†’ {e.get("attribute","")} â†’ {e.get("value","")}')
            parts.append("\n".join(eav_lines))
        if old_gaps:
            gap_names = [g.get("entity", "") for g in old_gaps if g.get("priority") == "high"][:3]
            if gap_names:
                parts.append(f"Luki: {', '.join(gap_names)}")

    return "\n\n".join(parts) if parts else ""


def _fmt_serp_enrichment_v2(pre_batch):
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}

    paa = serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or []
    lsi = serp.get("lsi_keywords") or []
    chips = serp.get("refinement_chips") or []

    if not paa and not lsi and not chips:
        return ""

    parts = ["â•â•â• SERP â•â•â•"]
    if chips:
        parts.append(f"Podtematy Google: {', '.join(str(c) for c in chips[:8])}")
    if paa:
        q_strs = []
        for q in paa[:4]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                q_strs.append(str(q_text))
        if q_strs:
            parts.append("Pytania PAA (odpowiedz na 1-2):\n  " + "\n  ".join(q_strs))
    if lsi:
        # Deduplicate: skip LSI keywords already in EXTENDED
        _ext_kws = pre_batch.get("keywords", {}).get("extended_this_batch", [])
        _ext_names = {(k.get("keyword", k) if isinstance(k, dict) else str(k)).lower().strip()
                      for k in _ext_kws}
        lsi_names = []
        for l in lsi[:8]:
            name = l.get("keyword", l) if isinstance(l, dict) else l
            if str(name).lower().strip() not in _ext_names:
                lsi_names.append(str(name))
        if lsi_names:
            parts.append(f"LSI: {', '.join(lsi_names)}")

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_intro_guidance_v2(pre_batch, batch_type):
    if batch_type not in ("INTRO", "intro"):
        return ""

    main_kw = pre_batch.get("main_keyword") or {}
    kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
    serp = pre_batch.get("serp_enrichment") or {}

    parts = ["â•â•â• LEAD (WSTÄ˜P) â•â•â•"]
    parts.append("120-200 sÅ‚Ã³w. NIE zaczynaj od h2:. Lead nie ma nagÅ‚Ã³wka.")
    if kw_name:
        parts.append(f'Zacznij od sedna: czym jest "{kw_name}" i dlaczego czytelnik powinien czytaÄ‡ dalej.')
    parts.append("Kontekst praktyczny + konkretny fakt liczbowy w PIERWSZYM akapicie.")
    parts.append("NIE zapowiadaj co bÄ™dzie dalej. NIE pisz 'w tym artykule dowiesz siÄ™'.")

    search_intent = serp.get("search_intent", "")
    if search_intent:
        parts.append(f"Intencja wyszukiwania: {search_intent}")

    # â”€â”€ Priority 1: Featured Snippet â”€â”€
    fs = serp.get("featured_snippet", "")
    fs_text = ""
    if fs:
        fs_text = fs if isinstance(fs, str) else (fs.get("text", "") if isinstance(fs, dict) else str(fs))
    if fs_text and len(fs_text) > 20:
        parts.append(f"\nğŸ“‹ Google Featured Snippet (PRZELICYTUJ tÄ™ odpowiedÅº â€” daj wiÄ™cej faktÃ³w i konkretÃ³w):")
        parts.append(f"  \"{fs_text[:300]}\"")

    # â”€â”€ Priority 2: AI Overview â”€â”€
    aio = serp.get("ai_overview", "")
    aio_text = ""
    if aio:
        aio_text = aio if isinstance(aio, str) else (aio.get("text", "") if isinstance(aio, dict) else str(aio))
    if aio_text and len(aio_text) > 20:
        parts.append(f"\nğŸ¤– Google AI Overview (TwÃ³j lead MUSI byÄ‡ bardziej konkretny):")
        parts.append(f"  \"{aio_text[:400]}\"")

    # â”€â”€ Fallback: when NO snippet AND NO AI overview â”€â”€
    if not (fs_text and len(fs_text) > 20) and not (aio_text and len(aio_text) > 20):
        parts.append("\nâš ï¸ Brak Featured Snippet i AI Overview â€” zbuduj lead z tych danych:")

        # v2.3: Competitor first paragraphs â€” strongest fallback signal
        comp_intros = serp.get("competitor_intros", [])
        if comp_intros:
            parts.append("  ğŸ“– Pierwsze akapity konkurencji (PRZELICYTUJ â€” daj wiÄ™cej konkretÃ³w):")
            for ci in comp_intros[:3]:
                _title = ci.get("title", "")[:50]
                _intro = ci.get("intro", "")[:250]
                if _intro:
                    parts.append(f"    [{_title}]: \"{_intro}\"")
            parts.append("  â†’ TwÃ³j lead musi byÄ‡ LEPSZY: bardziej konkretny, z liczbami, od razu do sedna.")

        # Competitor titles â†’ what angle works
        comp_titles = serp.get("competitor_titles", [])
        if comp_titles:
            titles_str = ", ".join(
                str(t.get("title", t) if isinstance(t, dict) else t)[:60]
                for t in comp_titles[:5] if t
            )
            if titles_str:
                parts.append(f"  ğŸ“° Top wyniki Google: {titles_str}")
                parts.append("  â†’ TwÃ³j lead musi odpowiedzieÄ‡ na pytanie lepiej niÅ¼ te tytuÅ‚y.")

        # Competitor snippets â†’ what Google shows
        comp_snippets = serp.get("competitor_snippets", [])
        if comp_snippets:
            snippet_texts = []
            for sn in comp_snippets[:3]:
                txt = sn.get("snippet", sn) if isinstance(sn, dict) else str(sn)
                if txt and len(str(txt)) > 20:
                    snippet_texts.append(str(txt)[:100])
            if snippet_texts:
                parts.append(f"  ğŸ“ Meta opisy konkurencji:")
                for st in snippet_texts:
                    parts.append(f"    â€¢ {st}")

        # PAA â†’ the first question is often the core user intent
        paa = serp.get("paa_questions", [])
        if paa:
            paa_texts = []
            for q in paa[:3]:
                qt = q.get("question", q) if isinstance(q, dict) else str(q)
                if qt and len(str(qt)) > 5:
                    paa_texts.append(str(qt))
            if paa_texts:
                parts.append(f"  â“ Ludzie pytajÄ…: {' | '.join(paa_texts)}")
                parts.append("  â†’ Lead powinien odpowiedzieÄ‡ na PIERWSZE pytanie w 1-2 zdaniach.")

        # S1 context â€” key facts
        s1_ctx = pre_batch.get("_s1_context") or {}
        eav = s1_ctx.get("eav", [])
        if eav:
            facts = []
            for e in eav[:3]:
                if isinstance(e, dict):
                    facts.append(f"{e.get('entity','')}: {e.get('value','')}")
            if facts:
                parts.append(f"  ğŸ“Š Fakty do wplecenia: {', '.join(facts)}")

    # â”€â”€ Custom intro guidance (from master API) â”€â”€
    guidance = pre_batch.get("intro_guidance", "")
    if guidance:
        if isinstance(guidance, dict):
            hook = guidance.get("hook", "")
            if hook:
                parts.append(f"Hak: {hook}")
        elif isinstance(guidance, str) and len(str(guidance)) > 10:
            parts.append(str(guidance)[:300])

    return "\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LEGAL / MEDICAL (used by article v2 â€” kept in full)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_legal_medical(pre_batch):
    legal_ctx = pre_batch.get("legal_context") or {}
    medical_ctx = pre_batch.get("medical_context") or {}
    ymyl_enrich = pre_batch.get("_ymyl_enrichment") or {}
    ymyl_intensity = pre_batch.get("_ymyl_intensity", "full")

    parts = []

    if ymyl_intensity == "light":
        light_note = pre_batch.get("_light_ymyl_note", "")
        if light_note:
            parts.append("â•â•â• ASPEKT REGULACYJNY (peryferyjny) â•â•â•")
            parts.append(f"  {light_note}")
            parts.append("  âš ï¸ Wspomnij o regulacjach MAX 1-2 razy w CAÅYM artykule.")
        return "\n".join(parts) if parts else ""

    if legal_ctx and legal_ctx.get("active"):
        parts.append("â•â•â• KONTEKST PRAWNY (YMYL) â•â•â•")
        parts.append("NIE wymyÅ›laj sygnatur, dat orzeczeÅ„ ani numerÃ³w artykuÅ‚Ã³w.")
        parts.append("Placeholder 'odpowiednich przepisÃ³w' â†’ zawsze podaj konkretny art.")
        parts.append("""âš ï¸ KRYTYCZNE ZASADY DLA TREÅšCI PRAWNYCH:
  1. SPRAWDÅ¹ NAZWÄ˜ USTAWY â€” nie mylij ustaw:
     âŒ â€Art. 87 ustawy o ochronie konkurencji i konsumentÃ³w" â† TO INNA USTAWA
     âœ… â€Art. 87 Â§ 1 Kodeksu wykroczeÅ„"
  2. SPRAWDÅ¹ NUMER ARTYKUÅU â€” nie zaokrÄ…glaj:
     âŒ â€Art. 178 k.k." â† to zaostrzenie karalnoÅ›ci, nie samodzielny typ czynu
     âœ… â€Art. 178a Â§ 1 k.k." â† prowadzenie w stanie nietrzeÅºwoÅ›ci
  3. PODAWAJ PEÅNÄ„ SYGNATURÄ˜ z paragrafem (Â§):
     âŒ â€Art. 178 Kodeksu karnego"
     âœ… â€Art. 178a Â§ 1 k.k."
  4. NIE MIESZAJ JEDNOSTEK: promile (â€°) = krew, mg/dmÂ³ = wydychane powietrze.
  5. JeÅ›li NIE masz pewnoÅ›ci co do numeru artykuÅ‚u â€” POMIÅƒ go. Lepiej ogÃ³lnik niÅ¼ bÅ‚Ä…d.
  6. KaÅ¼dÄ… podstawÄ™ prawnÄ… podawaj w formacie: â€Art. X Â§ Y [skrÃ³t ustawy]".""")


        wiki_arts = pre_batch.get("legal_wiki_articles") or []
        if wiki_arts:
            parts.append("\nWIKIPEDIA:")
            for w in wiki_arts[:4]:
                if w.get("found"):
                    parts.append(f"  [{w['article_ref']}] {w['title']}:")
                    parts.append(f"  {w['extract'][:300]}")
                    parts.append(f"  Å¹rÃ³dÅ‚o: {w['url']}")
                    parts.append("")

        legal_enrich = ymyl_enrich.get("legal", {})
        if legal_enrich.get("articles"):
            parts.append("\nPODSTAWA PRAWNA:")
            for art in legal_enrich["articles"][:5]:
                parts.append(f"  â€¢ {art}")
        if legal_enrich.get("acts"):
            parts.append(f"  Ustawy: {', '.join(legal_enrich['acts'][:4])}")
        if legal_enrich.get("key_concepts"):
            parts.append(f"  PojÄ™cia: {', '.join(legal_enrich['key_concepts'][:6])}")

        instruction = legal_ctx.get("legal_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        judgments = legal_ctx.get("top_judgments") or []
        if judgments:
            parts.append("\nOrzeczenia (dostÄ™pne, ale NIE musisz cytowaÄ‡):")
            parts.append("  âš ï¸ UÅ¼yj MAX 1 orzeczenia i TYLKO gdy bezpoÅ›rednio dotyczy tematu sekcji.")
            parts.append("  âš ï¸ NIE cytuj wyroku cywilnego (sygn. I C, III RC) w tekÅ›cie o odpowiedzialnoÅ›ci karnej.")
            parts.append("  âš ï¸ Lepiej pominÄ…Ä‡ orzeczenie niÅ¼ wcisnÄ…Ä‡ nieadekwatne.")
            for j in judgments[:3]:
                if isinstance(j, dict):
                    sig = j.get("signature", j.get("caseNumber", ""))
                    court = j.get("court", j.get("courtName", ""))
                    date = j.get("date", j.get("judgmentDate", ""))
                    matched = j.get("matched_article", "")
                    line = f'  â€¢ {sig}, {court} ({date})'
                    if matched:
                        line += f' [dot. {matched}]'
                    parts.append(line)

        citation_hint = legal_ctx.get("citation_hint", "")
        if citation_hint:
            parts.append(f'\n{citation_hint}')

    if medical_ctx and medical_ctx.get("active"):
        if parts:
            parts.append("")
        parts.append("â•â•â• KONTEKST MEDYCZNY (YMYL) â•â•â•")
        parts.append("MUSISZ:")
        parts.append("  1. CytowaÄ‡ ÅºrÃ³dÅ‚a naukowe (podane niÅ¼ej)")
        parts.append("  2. NIE wymyÅ›laÄ‡ statystyk ani nazw badaÅ„")

        med_enrich = ymyl_enrich.get("medical", {})
        if med_enrich.get("specialization"):
            parts.append(f"\n  Specjalizacja: {med_enrich['specialization']}")
        if med_enrich.get("condition"):
            cond = med_enrich["condition"]
            latin = med_enrich.get("condition_latin", "")
            icd = med_enrich.get("icd10", "")
            parts.append(f"  Choroba/stan: {cond}" + (f" ({latin})" if latin else "") + (f" [ICD-10: {icd}]" if icd else ""))
        if med_enrich.get("key_drugs"):
            parts.append(f"  Leki: {', '.join(med_enrich['key_drugs'][:5])}")
        if med_enrich.get("evidence_note"):
            parts.append(f"\n  âš ï¸ WYTYCZNE: {med_enrich['evidence_note']}")

        parts.append("")
        parts.append("HIERARCHIA DOWODÃ“W:")
        parts.append("  1. Meta-analiza > 2. RCT > 3. Kohortowe > 4. Opis przypadku > 5. Opinia")

        instruction = medical_ctx.get("medical_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        publications = medical_ctx.get("top_publications") or []
        if publications:
            parts.append("\nPublikacje:")
            for p in publications[:5]:
                if isinstance(p, dict):
                    title = p.get("title", "")[:80]
                    authors = p.get("authors", "")[:40]
                    year = p.get("year", "")
                    pmid = p.get("pmid", "")
                    parts.append(f'  â€¢ {authors} ({year}): "{title}" PMID:{pmid}')

    return "\n".join(parts) if parts else ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATEGORY-ONLY FORMATTERS
# (used by build_category_user_prompt â€” NOT by article v2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_smart_instructions(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    smart = enhanced.get("smart_instructions_formatted", "")
    if smart:
        return f"â•â•â• INSTRUKCJE DLA TEGO BATCHA â•â•â•\n{smart[:1000]}"
    return ""


def _fmt_semantic_plan(pre_batch, h2):
    plan = pre_batch.get("semantic_batch_plan") or {}
    if not plan:
        return ""
    parts = ["â•â•â• CO PISAÄ† W TEJ SEKCJI â•â•â•"]
    h2_coverage = plan.get("h2_coverage") or {}
    for h2_name, info in h2_coverage.items():
        if isinstance(info, dict):
            angle = info.get("semantic_angle", "")
            must = info.get("must_phrases", [])
            if angle:
                parts.append(f'KÄ…t: {angle}')
            if must:
                parts.append(f'Frazy: {", ".join(f"{p}" for p in must[:5])}')
    direction = plan.get("content_direction") or plan.get("writing_direction", "")
    if direction:
        parts.append(f'Kierunek: {direction}')
    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_coverage_density(pre_batch):
    coverage = pre_batch.get("coverage") or {}
    density = pre_batch.get("density") or {}
    main_kw = pre_batch.get("main_keyword") or {}
    if not coverage and not density and not main_kw:
        return ""
    parts = ["â•â•â• STATUS POKRYCIA FRAZ â•â•â•"]
    if main_kw:
        kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
        synonyms = main_kw.get("synonyms", []) if isinstance(main_kw, dict) else []
        if kw_name:
            parts.append(f'HasÅ‚o gÅ‚Ã³wne: "{kw_name}"')
        if synonyms:
            parts.append(f'Synonimy: {", ".join(synonyms[:5])}')
    current_cov = coverage.get("current", coverage.get("current_coverage"))
    target_cov = coverage.get("target", coverage.get("target_coverage"))
    if current_cov is not None and target_cov is not None:
        parts.append(f'Pokrycie: {current_cov}% z {target_cov}%')
    missing = coverage.get("missing_phrases") or coverage.get("uncovered") or []
    if missing:
        parts.append("âš ï¸ BRAKUJÄ„CE:")
        for m in missing[:8]:
            name = m.get("keyword", m) if isinstance(m, dict) else m
            parts.append(f'  â†’ "{name}"')
    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_style(pre_batch):
    style = pre_batch.get("style_instructions") or pre_batch.get("style_instructions_v39") or {}
    if not style:
        return ""
    parts = ["â•â•â• STYL (dodatkowy) â•â•â•"]
    if isinstance(style, dict):
        # Skip 'tone' â€” system prompt already sets tone to avoid conflicts
        forbidden = style.get("forbidden_phrases") or style.get("avoid_phrases") or []
        if forbidden:
            parts.append(f'Unikaj teÅ¼: {", ".join(f"{f}" for f in forbidden[:8])}')
    elif isinstance(style, str):
        parts.append(_word_trim(style, 500))
    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_entity_salience(pre_batch):
    """Entity salience â€” used by category prompt. Full version kept."""
    parts = []

    local_instructions = pre_batch.get("_entity_salience_instructions", "")
    if local_instructions:
        parts.append(local_instructions)

    backend_placement = pre_batch.get("_backend_placement_instruction", "")
    if backend_placement:
        parts.append("â•â•â• ROZMIESZCZENIE ENCJI â•â•â•")
        parts.append("âš ï¸ WskazÃ³wki techniczne â€” NIE kopiuj dosÅ‚ownie.")
        parts.append(backend_placement)

    FLEXION_NOTE = (
        "\nâš ï¸ FLEKSJA: PojÄ™cia w mianowniku â€” odmieniaj przez przypadki."
    )
    concept_instr = pre_batch.get("_concept_instruction", "")
    must_concepts = pre_batch.get("_must_cover_concepts", [])
    if concept_instr:
        parts.append(concept_instr + FLEXION_NOTE)
    elif must_concepts:
        concept_names = [c.get("text", c) if isinstance(c, dict) else str(c) for c in must_concepts[:10]]
        parts.append(
            "â•â•â• POJÄ˜CIA TEMATYCZNE â•â•â•\n"
            f"WpleÄ‡ naturalnie: {', '.join(concept_names)}"
            + FLEXION_NOTE
        )

    cooc_pairs = pre_batch.get("_cooccurrence_pairs") or []
    if cooc_pairs:
        cooc_lines = []
        for pair in cooc_pairs[:8]:
            if isinstance(pair, dict):
                e1 = pair.get("entity1", pair.get("source", ""))
                e2 = pair.get("entity2", pair.get("target", ""))
                if e1 and e2:
                    cooc_lines.append(f'  â€¢ "{e1}" + "{e2}"')
        if cooc_lines:
            parts.append("â•â•â• WSPÃ“ÅWYSTÄ˜POWANIE â•â•â•\n" + "\n".join(cooc_lines))

    first_para_ents = pre_batch.get("_first_paragraph_entities") or []
    if first_para_ents:
        fp_names = [ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent) for ent in first_para_ents[:6]]
        fp_names = [f'"{n}"' for n in fp_names if n]
        if fp_names:
            parts.append(f"PIERWSZY AKAPIT: {', '.join(fp_names)}")

    h2_ents = pre_batch.get("_h2_entities") or []
    if h2_ents:
        h2_names = [ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent) for ent in h2_ents[:8]]
        h2_names = [f'"{n}"' for n in h2_names if n]
        if h2_names:
            parts.append(f"ENCJE H2: {', '.join(h2_names)}")

    eav_triples = pre_batch.get("_eav_triples") or []
    if eav_triples:
        eav_lines = ["â•â•â• CECHY ENCJI (EAV) â•â•â•"]
        for e in eav_triples[:10]:
            eav_lines.append(f'  â€¢ "{e.get("entity","")}": {e.get("attribute","")} â†’ {e.get("value","")}')
        parts.append("\n".join(eav_lines))

    svo_triples = pre_batch.get("_svo_triples") or []
    if svo_triples:
        svo_lines = ["â•â•â• RELACJE (SVO) â•â•â•"]
        for t in svo_triples[:12]:
            svo_lines.append(f'  {t.get("subject","")} â†’ {t.get("verb","")} â†’ {t.get("object","")}')
        parts.append("\n".join(svo_lines))

    entity_gaps = pre_batch.get("_entity_gaps") or []
    if entity_gaps:
        high_gaps = [g for g in entity_gaps if g.get("priority") == "high"]
        if high_gaps:
            gap_lines = ["â•â•â• LUKI ENCYJNE â•â•â•"]
            for g in high_gaps[:5]:
                reason = f" â€” {g['why']}" if g.get("why") else ""
                gap_lines.append(f'  ğŸ”´ "{g["entity"]}"{reason}')
            parts.append("\n".join(gap_lines))

    return "\n\n".join(parts) if parts else ""


def _fmt_natural_polish(pre_batch):
    """Anti-stuffing + fleksja â€” v2.3: uses search_variants for richer variation."""
    parts = ["â•â•â• ANTY-STUFFING â•â•â•"]

    _batch_type = pre_batch.get("batch_type", "")
    _is_final = _batch_type.upper() in ("FINAL", "CONCLUSION")

    parts.append(
        "FLEKSJA: Odmiany = jedno uÅ¼ycie w oczach Google (lematyzacja).\n"
        "  Max 2Ã— ta sama FORMA frazy w jednym akapicie.\n"
        "  Max 3Ã— ta sama FORMA frazy w caÅ‚ym batchu â€” potem rotuj na odmianÄ™ lub peryfrazÄ™.\n"
        "RozkÅ‚adaj frazy RÃ“WNOMIERNIE po tekÅ›cie â€” nie skupiaj w jednym akapicie.\n"
        "PERYFRAZY > POWTÃ“RZENIA: gdy fraza blisko limitu â€” uÅ¼yj peryfrazy.\n"
        "  âŒ 'Wykroczenie polega... Wykroczenie grozi... Za wykroczenie kara...'\n"
        "  âœ… 'Wykroczenie polega... Czyn karalny grozi... Za ten delikt kara...'"
    )

    # v67: Extra warning for FINAL batches which tend to keyword-stuff
    if _is_final:
        parts.append(
            "âš ï¸ LAST BATCH RULE: To jest koÅ„cowa sekcja artykuÅ‚u.\n"
            "  NIE prÃ³buj 'nadrabiaÄ‡' brakujÄ…cych fraz â€” pisz naturalnie.\n"
            "  NIE zaczynaj kaÅ¼dego akapitu od frazy kluczowej.\n"
            "  UÅ¼yj MAX 2 fraz EXTENDED z listy â€” resztÄ™ pomiÅ„.\n"
            "  Lepszy naturalny tekst bez fraz niÅ¼ sztuczne upychanie."
        )

    # Dynamic anaphora with search variants
    _raw_main = pre_batch.get("main_keyword") or {}
    _main_name = _raw_main.get("keyword", "") if isinstance(_raw_main, dict) else str(_raw_main)
    if _main_name:
        # Try search_variants first (richest source)
        sv = pre_batch.get("_search_variants") or {}
        peryfrazy = sv.get("peryfrazy", [])
        potoczne = sv.get("potoczne", [])
        formalne = sv.get("formalne", [])

        # Build anaphora list from variants (peryfrazy first, then mix)
        anaphora_pool = []
        for v in peryfrazy[:3]:
            anaphora_pool.append(v)
        for v in potoczne[:2]:
            if v not in anaphora_pool:
                anaphora_pool.append(v)
        for v in formalne[:2]:
            if v not in anaphora_pool:
                anaphora_pool.append(v)

        # Fallback to entity_synonyms if no search_variants
        if not anaphora_pool:
            _entity_seo = (pre_batch.get("s1_data") or {}).get("entity_seo") or pre_batch.get("entity_seo") or {}
            _dynamic_synonyms = _entity_seo.get("entity_synonyms", [])
            if _dynamic_synonyms and len(_dynamic_synonyms) >= 2:
                anaphora_pool = [str(s) for s in _dynamic_synonyms[:5]]
            else:
                anaphora_pool = ["konkretny podmiot z kontekstu"]

        synonyms = ", ".join(anaphora_pool[:5])
        parts.append(f"ANTY-ANAPHORA [{_main_name}] MAX 2 ZDANIA Z RZÄ˜DU â†’ zmieÅ„ na: {synonyms}")

        # Add fleksyjne variants hint (helps LLM with case variation)
        fleksyjne = sv.get("fleksyjne", [])
        if fleksyjne:
            parts.append(f"ODMIANY: {', '.join(fleksyjne[:4])}")

    parts.append(
        "FAQ: kaÅ¼de pytanie zaczynaj INNYM sÅ‚owem (Czy, Kiedy, Jak, Co, Ile, Dlaczego).\n"
        "TEST STUFFINGU: usuniÄ™cie frazy NIE zmienia sensu = stuffing â†’ usuÅ„ powtÃ³rzenie."
    )

    return "\n".join(parts)


def _fmt_serp_enrichment(pre_batch):
    """Old SERP enrichment â€” used by category prompt."""
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}
    paa = serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or []
    lsi = serp.get("lsi_keywords") or []
    chips = serp.get("refinement_chips") or []
    if not paa and not lsi and not chips:
        return ""
    parts = ["â•â•â• WZBOGACENIE Z SERP â•â•â•"]
    if chips:
        parts.append(f"Refinement Chips: {', '.join(str(c) for c in chips[:8])}")
    if paa:
        parts.append("PAA:")
        for q in paa[:5]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                parts.append(f'  â“ {q_text}')
    if lsi:
        lsi_names = [l.get("keyword", l) if isinstance(l, dict) else l for l in lsi[:8]]
        parts.append(f'LSI: {", ".join(str(n) for n in lsi_names)}')
    return "\n".join(parts) if len(parts) > 1 else ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FAQ PROMPT BUILDER (unchanged)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_faq_system_prompt(pre_batch=None):
    base = (
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO. "
        "Piszesz sekcjÄ™ FAQ: zwiÄ™zÅ‚e, konkretne odpowiedzi. "
        "KaÅ¼da odpowiedÅº ma szansÄ™ trafiÄ‡ do Google Featured Snippet."
    )
    gpt_instructions = ""
    if pre_batch:
        gpt_instructions = pre_batch.get("gpt_instructions_v39", "")
    if gpt_instructions:
        return base + "\n\n" + gpt_instructions
    return base


def build_faq_user_prompt(paa_data, pre_batch=None):
    if isinstance(paa_data, list):
        paa_data = {"serp_paa": paa_data}
    elif not isinstance(paa_data, dict):
        paa_data = {}
    paa_questions = paa_data.get("serp_paa") or []
    unused = paa_data.get("unused_keywords") or {}
    avoid = paa_data.get("avoid_in_faq") or []
    if isinstance(avoid, dict):
        avoid = avoid.get("topics") or []
    elif isinstance(avoid, str):
        avoid = [avoid] if avoid.strip() else []
    elif not isinstance(avoid, list):
        avoid = []
    instructions_raw = paa_data.get("instructions", "")
    if isinstance(instructions_raw, dict):
        instr_parts = []
        for k, v in instructions_raw.items():
            if isinstance(v, str):
                instr_parts.append(f"â€¢ {v}")
            elif isinstance(v, dict):
                for sk, sv in v.items():
                    if isinstance(sv, str):
                        instr_parts.append(f"â€¢ {sk}: {sv}")
        instructions = "\n".join(instr_parts)
    elif isinstance(instructions_raw, str):
        instructions = instructions_raw
    else:
        instructions = ""

    enhanced_paa = []
    if pre_batch:
        enhanced = pre_batch.get("enhanced") or {}
        if not isinstance(enhanced, dict):
            enhanced = {}
        enhanced_paa = enhanced.get("paa_from_serp") or []
        if not isinstance(enhanced_paa, list):
            enhanced_paa = []

    keyword_limits = {}
    if pre_batch:
        keyword_limits = pre_batch.get("keyword_limits") or {}
        if not isinstance(keyword_limits, dict):
            keyword_limits = {}
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_names = [s.get("keyword", s) if isinstance(s, dict) else s for s in stop_raw]

    style = {}
    if pre_batch:
        style = pre_batch.get("style_instructions") or {}

    sections = []
    sections.append("â•â•â• SEKCJA FAQ â•â•â•\nNapisz sekcjÄ™ FAQ. Zaczynaj od:\nh2: NajczÄ™Å›ciej zadawane pytania")

    all_paa = list(dict.fromkeys(paa_questions + enhanced_paa))
    if all_paa:
        sections.append("Pytania z Google (PAA):")
        for i, q in enumerate(all_paa[:8], 1):
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text and q_text.strip():
                sections.append(f'  {i}. {q_text}')
        sections.append("Wybierz 4-6 najlepszych.")

    if unused:
        if isinstance(unused, dict):
            unused_list = []
            for cat, items in unused.items():
                if isinstance(items, list):
                    unused_list.extend(items[:5])
                elif isinstance(items, str):
                    unused_list.append(items)
            if unused_list:
                names = ", ".join(f'"{u}"' if isinstance(u, str) else f'"{u.get("keyword", "")}"' for u in unused_list[:8])
                sections.append(f'\nFrazy nieuÅ¼yte: {names}')
        elif isinstance(unused, list):
            names = ", ".join(f'"{u}"' for u in unused[:8])
            sections.append(f'\nFrazy nieuÅ¼yte: {names}')

    if avoid:
        topics = ", ".join(f'"{a}"' if isinstance(a, str) else f'"{a.get("topic", "")}"' for a in avoid[:8])
        sections.append(f'\nNIE powtarzaj: {topics}')

    if stop_names:
        sections.append(f'\nğŸ›‘ STOP: {", ".join(f"{s}" for s in stop_names[:5])}')

    if style:
        forbidden = style.get("forbidden_phrases") or []
        if forbidden:
            sections.append(f'ZAKAZANE: {", ".join(forbidden[:5])}')

    if pre_batch and pre_batch.get("article_memory"):
        mem = pre_batch["article_memory"]
        if isinstance(mem, dict):
            topics = mem.get("topics_covered") or []
            if topics:
                topic_names = [t if isinstance(t, str) else t.get("topic", "") for t in topics[:6]]
                sections.append(f'\nTematy z artykuÅ‚u: {", ".join(topic_names)}')

    if instructions:
        sections.append(f'\n{instructions}')

    sections.append("""
â•â•â• FORMAT â•â•â•
h2: NajczÄ™Å›ciej zadawane pytania

h3: [Pytanie, 5-10 sÅ‚Ã³w]
[OdpowiedÅº 60-120 sÅ‚Ã³w]
â†’ Zdanie 1: BEZPOÅšREDNIA odpowiedÅº
â†’ Zdanie 2-3: rozwiniÄ™cie
â†’ Zdanie 4: praktyczna wskazÃ³wka

Zero markdown (**, __, #). Zero tagÃ³w HTML (<h3>, <b>, <strong>).
KaÅ¼dy h3: na OSOBNEJ linii z pustÄ… liniÄ… powyÅ¼ej.
Napisz 4-6 pytaÅ„. TYLKO treÅ›Ä‡.""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# H2 PLAN PROMPT BUILDER (unchanged)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_h2_plan_system_prompt():
    return (
        "JesteÅ› ekspertem SEO z 10-letnim doÅ›wiadczeniem w planowaniu architektury treÅ›ci. "
        "Tworzysz logiczne, wyczerpujÄ…ce struktury nagÅ‚Ã³wkÃ³w H2."
    )


def build_h2_plan_user_prompt(main_keyword, mode, s1_data, all_user_phrases, user_h2_hints=None):
    s1_data = s1_data or {}
    competitor_h2 = s1_data.get("competitor_h2_patterns") or []
    suggested_h2s = (s1_data.get("content_gaps") or {}).get("suggested_new_h2s", [])
    content_gaps = s1_data.get("content_gaps") or {}
    causal_triplets = s1_data.get("causal_triplets") or {}
    paa = s1_data.get("paa") or s1_data.get("paa_questions") or []
    serp_analysis = s1_data.get("serp_analysis") or {}
    related_searches = s1_data.get("related_searches") or serp_analysis.get("related_searches") or []

    sections = []
    mode_desc = "standard = peÅ‚ny artykuÅ‚" if mode == "standard" else "fast = krÃ³tki, max 3 sekcje"
    sections.append(f"HASÅO GÅÃ“WNE: {main_keyword}\nTRYB: {mode} ({mode_desc})")

    if competitor_h2:
        def _h2_count(h):
            return h.get("count", h.get("sources", 0)) if isinstance(h, dict) else 0
        sorted_h2 = sorted(competitor_h2[:30], key=_h2_count, reverse=True)
        lines = ["â•â•â• WZORCE H2 KONKURENCJI â€” posortowane po popularnoÅ›ci â•â•â•",
                 "Liczba przy H2 = ilu konkurentÃ³w uÅ¼ywa tego tematu.",
                 "H2 z wysokÄ… liczbÄ… = MUST HAVE w Twoim artykule (uÅ¼ytkownicy tego szukajÄ…)."]
        for i, h in enumerate(sorted_h2[:20], 1):
            if isinstance(h, dict):
                pattern = h.get("text", h.get("pattern", h.get("h2", str(h))))
                count = _h2_count(h)
                bar = "â–ˆ" * min(count, 8)
                lines.append(f"  {i:2}. [{bar:<8}] {count}Ã— â€” {pattern}")
            elif isinstance(h, str):
                lines.append(f"  {i:2}. {h}")
        sections.append("\n".join(lines))

    if suggested_h2s:
        lines = ["â•â•â• SUGEROWANE NOWE H2 (luki, tego NIKT z konkurencji nie pokrywa) â•â•â•"]
        for h in suggested_h2s[:10]:
            h_text = h if isinstance(h, str) else h.get("h2", h.get("title", str(h)))
            lines.append(f"  â€¢ {h_text}")
        sections.append("\n".join(lines))

    gap_priority_map = {
        "paa_unanswered": ("ğŸ”´ HIGH", "PAA bez odpowiedzi"),
        "depth_missing": ("ğŸŸ¡ MED-HIGH", "Brak gÅ‚Ä™bi"),
        "subtopic_missing": ("ğŸŸ¢ MED", "BrakujÄ…cy podtemat"),
        "gaps": ("", "Luka"),
    }
    all_gaps = []
    for key in ("paa_unanswered", "depth_missing", "subtopic_missing", "gaps"):
        priority, label = gap_priority_map.get(key, ("", ""))
        items = content_gaps.get(key) or []
        for item in items[:5]:
            gap_text = item if isinstance(item, str) else item.get("gap", item.get("topic", str(item)))
            if gap_text and gap_text not in [g[0] for g in all_gaps]:
                all_gaps.append((gap_text, priority, label))
    if all_gaps:
        lines = ["â•â•â• LUKI TREÅšCIOWE (tematy do pokrycia, priorytet od najwyÅ¼szego) â•â•â•"]
        for gap_text, priority, label in all_gaps[:10]:
            prefix = f"[{priority}] " if priority else ""
            lines.append(f"  â€¢ {prefix}{gap_text}")
        sections.append("\n".join(lines))

    if paa:
        lines = ["â•â•â• PAA â•â•â•"]
        for q in paa[:8]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                lines.append(f"  â“ {q_text}")
        sections.append("\n".join(lines))

    if related_searches:
        rs_texts = []
        for rs in related_searches[:12]:
            rs_t = rs if isinstance(rs, str) else (rs.get("query", "") or rs.get("text", ""))
            if rs_t:
                rs_texts.append(rs_t)
        if rs_texts:
            lines = ["â•â•â• RELATED SEARCHES (Google podpowiada po main_keyword) â•â•â•",
                     "UÅ¼yj tych fraz jako wskazÃ³wek tematycznych przy tworzeniu H2.",
                     "Wiele z nich to podtematy ktÃ³rych BRAK u konkurencji â€” Twoja szansa:"]
            for rs_t in rs_texts:
                lines.append(f"  ğŸ” {rs_t}")
            sections.append("\n".join(lines))

    triplet_list = (causal_triplets.get("chains") or causal_triplets.get("singles")
                    or causal_triplets.get("triplets") or [])[:8]
    if triplet_list:
        lines = ["â•â•â• PRZYCZYNOWE ZALEÅ»NOÅšCI (causeâ†’effect z konkurencji) â•â•â•",
                 "Confidence: ğŸ”´ â‰¥0.9 UÅ»YJ | ğŸŸ¡ â‰¥0.6 gdy pasuje | ğŸŸ¢ <0.6 opcjonalnie",
                 "is_chain=True (Aâ†’Bâ†’C) = najcenniejsze. Buduj logiczny przepÅ‚yw"]
        for t in triplet_list:
            if isinstance(t, dict):
                cause = t.get("cause", t.get("subject", ""))
                effect = t.get("effect", t.get("object", ""))
                conf = t.get("confidence", 0)
                is_chain = t.get("is_chain", False)
                ind = "ğŸ”´" if conf >= 0.9 else ("ğŸŸ¡" if conf >= 0.6 else "ğŸŸ¢")
                chain_tag = " [CHAIN]" if is_chain else ""
                lines.append(f"  {ind} {cause} â†’ {effect}{chain_tag}")
            elif isinstance(t, str):
                lines.append(f"  â€¢ {t}")
        sections.append("\n".join(lines))

    if user_h2_hints:
        h2_hints_list = "\n".join(f'  â€¢ "{h}"' for h in user_h2_hints[:10])
        sections.append(f"""â•â•â• FRAZY H2 UÅ»YTKOWNIKA â•â•â•

UÅ¼ytkownik podaÅ‚ te frazy z myÅ›lÄ… o nagÅ‚Ã³wkach H2.
Wykorzystaj je w nagÅ‚Ã³wkach tam, gdzie brzmiÄ… naturalnie po polsku.
Nie musisz uÅ¼yÄ‡ kaÅ¼dej, ale nie ignoruj ich. Dopasuj z wyczuciem.

FRAZY H2:
{h2_hints_list}""")

    if all_user_phrases:
        phrases_text = ", ".join(f'"{p}"' for p in all_user_phrases[:15])
        sections.append(f"""â•â•â• KONTEKST TEMATYCZNY (frazy BASIC/EXTENDED) â•â•â•

PoniÅ¼sze frazy bÄ™dÄ… uÅ¼yte W TREÅšCI artykuÅ‚u (nie w nagÅ‚Ã³wkach).
Zaplanuj H2 tak, by kaÅ¼da fraza miaÅ‚a naturalnÄ… sekcjÄ™:

{phrases_text}""")

    # H2 scaling â€” driven by target length, not arbitrary thresholds
    length_analysis = s1_data.get("length_analysis") or {}
    rec_length = length_analysis.get("recommended") or s1_data.get("recommended_length") or 0
    median_length = length_analysis.get("median") or s1_data.get("median_length") or 0

    if mode == "fast":
        fast_note = "Tryb fast: DOKÅADNIE 3 sekcje + FAQ."
    else:
        target = rec_length or (median_length * 2) or 1500
        # ~250 words per H2 section + intro â†’ derive count from length
        _raw_h2 = max(3, min(12, target // 250))
        h2_min = max(3, _raw_h2 - 1)
        h2_max = _raw_h2 + 1
        h2_range = f"{h2_min}-{h2_max}"
        fast_note = f"Tryb standard: {h2_range} sekcji + FAQ. Max {h2_max + 1} H2 Å‚Ä…cznie."

    h2_hint_rule = ("UwzglÄ™dnij frazy H2 uÅ¼ytkownika." if user_h2_hints
                    else "Dobierz nagÅ‚Ã³wki na podstawie S1 i luk.")

    sections.append(f"""â•â•â• ZASADY â•â•â•
1. LICZBA H2: {fast_note}
2. OSTATNI H2: "NajczÄ™Å›ciej zadawane pytania"
3. Pokryj wzorce konkurencji + luki
4. {h2_hint_rule}
5. Logiczna narracja
6. NIE powtarzaj hasÅ‚a gÅ‚Ã³wnego w kaÅ¼dym H2
7. Naturalna polszczyzna

â•â•â• FORMAT â•â•â•
JSON array: ["H2 pierwszy", ..., "NajczÄ™Å›ciej zadawane pytania"]""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATEGORY PROMPT BUILDERS (unchanged)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_category_system_prompt(pre_batch, batch_type, category_data=None):
    pre_batch = pre_batch or {}
    category_data = category_data or {}
    parts = []

    store_name = category_data.get("store_name") or "sklep"
    store_desc = category_data.get("store_description") or ""
    brand_voice = category_data.get("brand_voice") or ""

    store_ctx = f" dla {store_name}" if store_name != "sklep" else ""
    store_desc_line = f"\n{store_desc}" if store_desc else ""
    parts.append(f"""<role>
JesteÅ› doÅ›wiadczonym copywriterem e-commerce{store_ctx}{store_desc_line}.
Specjalizujesz siÄ™ w opisach kategorii sklepÃ³w internetowych.
Nie jesteÅ› blogerem â€” piszesz tekst sprzedaÅ¼owy.
</role>""")

    parts.append("""<goal>
Opis kategorii e-commerce, ktÃ³ry:
  â€¢ wspiera intencjÄ™ transakcyjnÄ…,
  â€¢ naturalnie zawiera sÅ‚owa kluczowe (gÄ™stoÅ›Ä‡ 1,0â€“2,0%),
  â€¢ buduje entity salience >0,30,
  â€¢ uÅ¼ywa konkretnych nazw produktÃ³w, cen, cech,
  â€¢ pomaga kupujÄ…cemu podjÄ…Ä‡ decyzjÄ™.
80% transakcyjnych, 20% informacyjnych.
</goal>""")

    target = category_data.get("target_audience") or ""
    target_line = f"\nGrupa docelowa: {target}" if target else ""
    parts.append(f"""<audience>
KupujÄ…cy z intencjÄ… zakupowÄ….{target_line}
</audience>""")

    voice_line = f"\nBrand voice: {brand_voice}" if brand_voice else ""
    parts.append(f"""<tone>
Ton: autorytatywny, pomocny, zwiÄ™zÅ‚y.{voice_line}
Unikaj: â€szeroki wybÃ³r", â€coÅ› dla kaÅ¼dego", â€nie szukaj dalej".
</tone>""")

    parts.append("""<epistemology>
Å¹RÃ“DÅA: dane wejÅ›ciowe, konkurencja z SERP, wiedza produktowa.
âŒ ZAKAZ: nie wymyÅ›laj produktÃ³w, cen, recenzji, certyfikatÃ³w.
</epistemology>""")

    cat_type = category_data.get("category_type", "subcategory")
    if cat_type == "parent":
        struct_desc = """KATEGORIA NADRZÄ˜DNA (200â€“500 sÅ‚Ã³w):
  Blok 1 â€” INTRO (50â€“100 sÅ‚Ã³w): keyword + opis + USP + linki podkategorii
  Blok 2 â€” SEO (100â€“300 sÅ‚Ã³w): 1â€“2 H2, przeglÄ…d, dlaczego u nas
  Blok 3 â€” FAQ (2â€“3 pytania)"""
    else:
        struct_desc = """PODKATEGORIA (500â€“1200 sÅ‚Ã³w):
  Blok 1 â€” INTRO (50â€“150 sÅ‚Ã³w): keyword + opis + USP
  Blok 2 â€” SEO (400â€“800 sÅ‚Ã³w): 2â€“4 H2 (jak wybraÄ‡, rodzaje, dlaczego u nas)
  Blok 3 â€” FAQ (3â€“6 pytaÅ„)"""

    parts.append(f"<category_structure>\n{struct_desc}\n</category_structure>")

    parts.append("""<rules>
KEYWORD DENSITY: 1,0â€“2,0%.
ENTITY SALIENCE: cel >0,30. Entity-rich: typy, materiaÅ‚y, technologie, marki.
PASSAGE-FIRST: intro = standalone summary.
LISTY HTML: 3+ elementÃ³w â†’ lista.
SPACING: MAIN ~60 sÅ‚Ã³w, BASIC ~80, EXTENDED ~120.
ANTI-AI: zakaz fraz kliszowych.
LINKI: 3â€“8 kontekstowych na 300â€“500 sÅ‚Ã³w.
FORMAT: h2:/h3:. Zero markdown (**, __, #). Zero tagÃ³w HTML (<h2>, <h3>).
  KaÅ¼dy h2:/h3: na OSOBNEJ linii z pustÄ… liniÄ… powyÅ¼ej.
</rules>""")

    parts.append("""<examples>
PRZYKÅAD DOBRY:
<example_good>
Damskie buty do biegania od Nike, ASICS i Brooks â€” od 299 do 1 199 zÅ‚.
Bestseller sezonu: Nike Air Zoom Pegasus 41 (4,7â˜…, 312 recenzji)
Å‚Ä…czy responsywnÄ… piankÄ™ React z siateczkÄ… Flyknit.
Darmowy zwrot 30 dni, wysyÅ‚ka w 24h.
</example_good>
</examples>""")

    return "\n\n".join(parts)


def build_category_user_prompt(pre_batch, h2, batch_type, article_memory=None, category_data=None):
    pre_batch = pre_batch or {}
    category_data = category_data or {}
    sections = []

    sections.append(
        "Piszesz opis kategorii e-commerce â€” ton pomocny, "
        "konkretny, wspierajÄ…cy decyzjÄ™ zakupowÄ…. "
        "Zasady w system prompcie."
    )

    # Opening pattern rotation for category (commercial variants)
    _CAT_PATTERNS = [
        ("A", "KONKRET PRODUKTOWY",
         "Zacznij od konkretnego produktu, ceny lub cechy. "
         "Np: 'Nike Pegasus 41 od 549 zÅ‚ â€” bestseller z 312 recenzjami...'"),
        ("B", "ZAKRES/STATYSTYKA",
         "Zacznij od zakresu, liczby lub faktu. "
         "Np: 'Ponad 200 modeli butÃ³w do biegania od 15 marek...'"),
        ("C", "POTRZEBA KUPUJÄ„CEGO",
         "Zacznij od potrzeby klienta. "
         "Np: 'Szukasz buta na maraton z amortyzacjÄ… na twardym podÅ‚oÅ¼u?'"),
        ("D", "USP/WYRÃ“Å»NIK",
         "Zacznij od przewagi sklepu. "
         "Np: 'Darmowy zwrot 30 dni i dobÃ³r rozmiaru z ekspertem...'"),
    ]
    batch_num = pre_batch.get("batch_number", 1) or 1
    pattern_idx = (batch_num - 1) % len(_CAT_PATTERNS)
    p_letter, p_name, p_desc = _CAT_PATTERNS[pattern_idx]
    sections.append(
        f"OTWARCIE â€” wzorzec {p_letter} ({p_name}):\n{p_desc}"
    )

    # Category context
    cat_ctx_parts = []
    cat_name = category_data.get("category_name") or pre_batch.get("main_keyword", "")
    if isinstance(cat_name, dict):
        cat_name = cat_name.get("keyword", "")
    cat_type = category_data.get("category_type", "subcategory")
    hierarchy = category_data.get("hierarchy") or ""
    store_name = category_data.get("store_name") or ""
    usp = category_data.get("usp") or ""
    products = category_data.get("products") or ""
    bestseller = category_data.get("bestseller") or ""
    price_range = category_data.get("price_range") or ""

    cat_ctx_parts.append(f"Kategoria: {cat_name}")
    cat_ctx_parts.append(f"Typ: {'nadrzÄ™dna' if cat_type == 'parent' else 'podkategoria'}")
    if hierarchy: cat_ctx_parts.append(f"Hierarchia: {hierarchy}")
    if store_name: cat_ctx_parts.append(f"Sklep: {store_name}")
    if usp: cat_ctx_parts.append(f"USP: {usp}")
    if products: cat_ctx_parts.append(f"Produkty:\n{products}")
    if bestseller: cat_ctx_parts.append(f"Bestseller: {bestseller}")
    if price_range: cat_ctx_parts.append(f"Ceny: {price_range}")
    sections.append("â•â•â• DANE KATEGORII â•â•â•\n" + "\n".join(cat_ctx_parts))

    _schema_guard(pre_batch)

    formatters = [
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_keywords(pre_batch),
        lambda: _fmt_smart_instructions(pre_batch),
        lambda: _fmt_semantic_plan(pre_batch, h2),
        lambda: _fmt_coverage_density(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_h2_remaining(pre_batch),
        lambda: _fmt_entity_salience(pre_batch),
        lambda: _fmt_serp_enrichment(pre_batch),
        lambda: _fmt_natural_polish(pre_batch),
        lambda: _fmt_style(pre_batch),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception:
            pass

    return "\n\n".join(sections)
