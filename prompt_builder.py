"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BRAJEN PROMPT BUILDER v2.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
v2.1 changes (vs v1.1):
  - System prompt: ~900 sÅ‚Ã³w (byÅ‚o ~3500). UsuniÄ™to:
    * 8 kategorii ANTY-AI â†’ krÃ³tka lista + grammar_checker
    * Subject rotation / position rule â†’ usuniÄ™te
    * Opening patterns A-F â†’ naturalna wolnoÅ›Ä‡
    * Mosty semantyczne â†’ kolokacja wystarczy
    * Passage-first 40-58 sÅ‚Ã³w â†’ "odpowiedz wprost"
    * Limity zdaÅ„ w prompcie â†’ walidator post-hoc
  - User prompt: 10 formatterÃ³w (byÅ‚o 18). UsuniÄ™to:
    * _fmt_smart_instructions â†’ duplikuje system
    * _fmt_coverage_density â†’ reviewer
    * _fmt_phrase_hierarchy â†’ reviewer
    * _fmt_natural_polish â†’ reviewer
    * _fmt_style â†’ zintegrowany w system prompt
    * _fmt_depth_signals â†’ expert persona
    * _fmt_experience_markers â†’ expert persona
    * _fmt_causal_context â†’ naturalny autor
  - EAV/SVO: "jeÅ›li pasujÄ…" zamiast "MUSI"
  - Entity SEO: 3 zasady (kolokacja, nazewnictwo, hierarchia)
  - Intro: 3 proste punkty (definicja â†’ kontekst â†’ zapowiedÅº)

Architecture:
  SYSTEM PROMPT = Expert persona + Minimal rules
  USER PROMPT   = Data-driven instructions (no micromanagement)
  Category/FAQ/H2 builders = unchanged from v1.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import logging

try:
    from shared_constants import (
        SENTENCE_AVG_TARGET, SENTENCE_AVG_TARGET_MIN, SENTENCE_AVG_TARGET_MAX,
        SENTENCE_SOFT_MAX, SENTENCE_HARD_MAX, SENTENCE_AVG_MAX_ALLOWED
    )
except ImportError:
    SENTENCE_AVG_TARGET = 13
    SENTENCE_AVG_TARGET_MIN = 8
    SENTENCE_AVG_TARGET_MAX = 20
    SENTENCE_SOFT_MAX = 30
    SENTENCE_HARD_MAX = 40
    SENTENCE_AVG_MAX_ALLOWED = 22

_pb_logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _word_trim(text, max_chars):
    if not text or len(text) <= max_chars:
        return text
    trimmed = text[:max_chars]
    nl = chr(10)
    last_break = max(trimmed.rfind(" "), trimmed.rfind(nl), trimmed.rfind(". "))
    if last_break > max_chars // 2:
        trimmed = trimmed[:last_break]
    return trimmed.rstrip(" ,;:") + "..."


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERSONAS (v2.1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_PERSONAS = {
    "prawo": (
        "JesteÅ› prawnikiem oraz dziennikarzem portalu zajmujÄ…cego siÄ™ tematykÄ… prawnÄ….\n"
        "TÅ‚umaczysz przepisy zrozumiaÅ‚ym jÄ™zykiem.\n"
        "Piszesz precyzyjnie, ale przystÄ™pnie."
    ),
    "medycyna": (
        "JesteÅ› lekarzem oraz dziennikarzem portalu zajmujÄ…cego siÄ™ tematykÄ… zdrowotnÄ….\n"
        "Piszesz precyzyjnie, ale przystÄ™pnie â€” bez Å¼argonu lekarskiego.\n"
        "TÅ‚umaczysz mechanizmy, nie recytujesz podrÄ™cznik."
    ),
    "finanse": (
        "JesteÅ› doradcÄ… finansowym oraz dziennikarzem portalu zajmujÄ…cego siÄ™ tematykÄ… finansowÄ….\n"
        "Konkretne liczby, realne scenariusze, wideÅ‚ki cenowe.\n"
        "Pokazujesz co liczby znaczÄ… w portfelu czytelnika â€” wyliczenia > komentarze."
    ),
    "technologia": (
        "JesteÅ› inÅ¼ynierem oraz dziennikarzem portalu zajmujÄ…cego siÄ™ tematykÄ… technologicznÄ….\n"
        "Piszesz precyzyjnie, z danymi, zrozumiale dla praktyka."
    ),
    "budownictwo": (
        "JesteÅ› inÅ¼ynierem budownictwa i kosztorysantem.\n"
        "Piszesz jak ktoÅ›, kto liczy â€” podajesz ceny, wideÅ‚ki, stawki za mÂ².\n"
        "Dane > komentarze. Tabelka > piÄ™Ä‡ zdaÅ„ prozy. Wyliczenie > opinia."
    ),
    "uroda": (
        "JesteÅ› kosmetologiem oraz dziennikarzem portalu zajmujÄ…cego siÄ™ tematykÄ… kosmetycznÄ….\n"
        "Piszesz z pozycji nauki, nie marketingu."
    ),
    "inne": (
        "JesteÅ› ekspertem oraz dziennikarzem portalu zajmujÄ…cego siÄ™ tematykÄ… tego artykuÅ‚u.\n"
        "Piszesz jak ktoÅ›, kto zna temat z praktyki,\n"
        "ma opinie i ulubione przykÅ‚ady."
    ),
}

# â”€â”€ Category-specific density/style rules (injected into system prompt) â”€â”€
_CATEGORY_STYLE = {
    "budownictwo": (
        "KALKULACJE > KOMENTARZE:\n"
        "  Gdy temat dotyczy kosztÃ³w â€” LICZ, nie opisuj.\n"
        "  WeÅº przykÅ‚adowy dom (np. 100 mÂ²) i pokaÅ¼ kalkulacjÄ™ PER POMIESZCZENIE:\n"
        "    salon 30 mÂ² Ã— panele 50 zÅ‚/mÂ² = 1 500 zÅ‚\n"
        "    Å‚azienka 8 mÂ² Ã— pÅ‚ytki 120 zÅ‚/mÂ² = 960 zÅ‚\n"
        "  Na koÅ„cu sekcji PODSUMUJ: 'ÅÄ…cznie podÅ‚ogi: ok. 7 500 zÅ‚'\n"
        "  NIGDY nie zamieniaj '90â€“140 zÅ‚/mÂ²' na 'kilkadziesiÄ…t zÅ‚otych' â€” podaj wideÅ‚ki.\n"
        "\nTABELE â€” min. 1 na artykuÅ‚ kosztowy:\n"
        "  Cennik robocizny â†’ <table>:\n"
        "    <tr><th>UsÅ‚uga</th><th>Cena od</th><th>Cena do</th></tr>\n"
        "    <tr><td>Malowanie Å›cian</td><td>20 zÅ‚/mÂ²</td><td>25 zÅ‚/mÂ²</td></tr>\n"
        "    <tr><td>UkÅ‚adanie pÅ‚ytek</td><td>90 zÅ‚/mÂ²</td><td>140 zÅ‚/mÂ²</td></tr>\n"
        "  PorÃ³wnanie standardÃ³w â†’ <table> (ekonom | Å›redni | premium)\n"
        "\nGÄ˜STOÅšÄ† DANYCH: Min. 2 konkretne liczby na akapit.\n"
        "  Zdanie bez Å¼adnej liczby, materiaÅ‚u lub parametru = slop â†’ usuÅ„.\n"
        "\nDANE Z SERP: Gdy strony konkurencji podajÄ… ceny usÅ‚ug/materiaÅ‚Ã³w,\n"
        "  PRZEPISZ wideÅ‚ki cenowe dosÅ‚ownie. NIE parafrazuj na 'kilkadziesiÄ…t' czy 'sporo'.\n"
        "  âŒ 'Panele potrafiÄ… zamknÄ…Ä‡ siÄ™ w kilkudziesiÄ™ciu zÅ‚otych'\n"
        "  âœ… 'Panele laminowane z montaÅ¼em: 50â€“150 zÅ‚/mÂ²'"
    ),
    "finanse": (
        "GÄ˜STOÅšÄ† DANYCH: Min. 2 konkretne liczby (kwota, %, stawka) na akapit.\n"
        "  Wyliczenie > opis. Tabelka > piÄ™Ä‡ zdaÅ„ prozy.\n"
        "FAKT + INTERPRETACJA: po kaÅ¼dej liczbie dodaj co ona znaczy dla czytelnika.\n"
        "  âŒ 'Oprocentowanie wynosi 7,5 %' â†’ âœ… 'Oprocentowanie 7,5 % â€” przy kredycie 300 000 zÅ‚ to rata ok. 2 100 zÅ‚/mies.'\n"
        "  Interpretuj TYLKO gdy dane wynikajÄ… z kontekstu â€” nie wymyÅ›laj obliczeÅ„.\n"
        "ZAKAZANE: zdania komentujÄ…ce bez danych ('ta sytuacja', 'ten problem')."
    ),
    "prawo": (
        "PRZEPISY: podawaj numery artykuÅ‚Ã³w, wideÅ‚ki kar, konkretne terminy.\n"
        "  âŒ 'SÄ…d moÅ¼e orzec karÄ™' â†’ âœ… 'Grozi grzywna 5 000â€“30 000 zÅ‚ lub zakaz na 3â€“15 lat (art. 178a Â§ 1 k.k.)'\n"
        "  Gdy SERP podaje sygnatury/orzeczenia â†’ uÅ¼yj ich.\n"
        "CASE STUDY: min. 1 typowa sytuacja na sekcjÄ™ H2.\n"
        "  UÅ¼ywaj archetypÃ³w (Kowalski, kierowca, wÅ‚aÅ›ciciel mieszkania) â€” NIE wymyÅ›laj sygnatur ani kwot.\n"
        "  âŒ 'Art. 212 KK penalizuje zniesÅ‚awienie.' â†’ âœ… 'JeÅ›li sÄ…siad napisze pod postem, Å¼e kradniesz prÄ…d â€” ryzykuje zarzut z art. 212 KK.'\n"
        "PODMIOT: SÄ…d zasÄ…dza. Inwestor skÅ‚ada. DÅ‚uÅ¼nik pÅ‚aci.\n"
        "  âŒ 'MoÅ¼na zÅ‚oÅ¼yÄ‡ wniosek' â†’ âœ… 'Wierzyciel skÅ‚ada wniosek'\n"
        "  âŒ 'NaleÅ¼y pamiÄ™taÄ‡' â†’ âœ… 'SÄ…d bierze pod uwagÄ™'"
    ),
    "medycyna": (
        "PRECYZJA: dawki, nazwy substancji, mechanizmy â€” nie ogÃ³lniki.\n"
        "  âŒ 'Lek pomaga na bÃ³l' â†’ âœ… 'Ibuprofen 400 mg co 6â€“8 h Å‚agodzi bÃ³l w ciÄ…gu 30â€“60 min.'\n"
        "MECHANIZM > OBIETNICA: opisuj procesy biologiczne, nie efekty marketingowe.\n"
        "  âŒ 'Krem nawilÅ¼a skÃ³rÄ™' â†’ âœ… 'Kwas hialuronowy wiÄ…Å¼e czÄ…steczki wody w naskÃ³rku, tworzÄ…c barierÄ™ okluzyjnÄ….'\n"
        "  âŒ 'Skuteczny skÅ‚adnik' â†’ âœ… 'Retinol przyspiesza odnowÄ™ komÃ³rkowÄ… naskÃ³rka'\n"
        "ZAKAZ przymiotnikÃ³w oceniajÄ…cych: 'skuteczny', 'najlepszy', 'rewolucyjny', 'cudowny'.\n"
        "  Zamiast oceny â†’ mechanizm dziaÅ‚ania lub dane (czas, dawka, czÄ™stotliwoÅ›Ä‡).\n"
        "  NIE cytuj wynikÃ³w badaÅ„, ktÃ³rych nie masz w ÅºrÃ³dÅ‚ach â€” opisz mechanizm."
    ),
    "uroda": (
        "MECHANIZM > MARKETING: opisuj procesy skÃ³rne, nie obietnice.\n"
        "  âŒ 'Krem cudownie nawilÅ¼a' â†’ âœ… 'Ceramidy odbudowujÄ… barierÄ™ lipidowÄ… naskÃ³rka, ograniczajÄ…c TEWL.'\n"
        "NAZEWNICTWO: przy kaÅ¼dym zabiegu/produkcie podaj substancjÄ™ czynnÄ….\n"
        "  âŒ 'peeling chemiczny' â†’ âœ… 'peeling z kwasem glikolowym 30 %'\n"
        "  âŒ 'serum na zmarszczki' â†’ âœ… 'serum z retinalem 0,05 %'\n"
        "ZAKAZ przymiotnikÃ³w oceniajÄ…cych: 'rewolucyjny', 'cudowny', 'najlepszy', 'kultowy'.\n"
        "  Zamiast oceny â†’ mechanizm + czas efektu: 'Retinol widocznie wygÅ‚adza po 8â€“12 tygodniach.'"
    ),
    "technologia": (
        "PORÃ“WNANIE DO STANDARDU: kaÅ¼dy parametr odnieÅ› do tego, co czytelnik zna.\n"
        "  âŒ 'Wi-Fi 7 oferuje 46 Gbps' â†’ âœ… 'Wi-Fi 7 (46 Gbps) â€” 4Ã— szybciej niÅ¼ popularne Wi-Fi 6.'\n"
        "  âŒ 'Chip ma 3 nm proces' â†’ âœ… '3 nm vs dotychczasowe 5 nm â€” 25 % mniej energii przy tej samej mocy.'\n"
        "  PorÃ³wnuj do POPRZEDNIEJ GENERACJI, nie do abstrakcyjnych liczb.\n"
        "SCENARIUSZ UÅ»YCIA: po specyfikacji pokaÅ¼ co to zmienia w praktyce.\n"
        "  âŒ 'PrzepustowoÅ›Ä‡ 10 Gbps' â†’ âœ… 'Taka przepustowoÅ›Ä‡ pozwala na stabilny streaming 8K na 3 urzÄ…dzeniach jednoczeÅ›nie.'\n"
        "GÄ˜STOÅšÄ†: min. 1 parametr techniczny + 1 scenariusz na akapit."
    ),
}



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM PROMPT (v2.1 â€” ~900 sÅ‚Ã³w)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_system_prompt(pre_batch, batch_type):
    pre_batch = pre_batch or {}
    parts = []

    detected_category = pre_batch.get("detected_category", "")
    is_ymyl = detected_category in ("prawo", "medycyna", "finanse")

    # â•â•â• 1. ROLA â•â•â•
    persona = _PERSONAS.get(detected_category, _PERSONAS["inne"])
    parts.append(f"""<rola>
{persona}
Ton: pewny, konkretny, rzeczowy. 3. osoba. ZAKAZ 2. osoby (ty/TwÃ³j).
TÅ‚umacz temat czytelnikowi â€” nie pisz jak encyklopedia.
</rola>""")

    # â•â•â• 2. ZASADY PISANIA â•â•â•
    parts.append(f"""<zasady>
KaÅ¼de zdanie = nowa informacja. Fakt podany raz â€” nie parafrazuj go dalej.
Nie zapowiadaj, nie streszczaj, nie komentuj. Po prostu pisz.

DANE > OPINIA: Konkretne liczby, wideÅ‚ki cenowe, stawki, wymiary.
  Å¹LE: "Koszt wykoÅ„czenia roÅ›nie, gdy standard jest wyÅ¼szy."
  DOBRZE: "Malowanie z gÅ‚adziami: 60â€“120 zÅ‚/mÂ². Deska warstwowa z montaÅ¼em: 150â€“250 zÅ‚/mÂ²."
  Gdy temat dotyczy kosztÃ³w/cen â€” podawaj wideÅ‚ki, nie metafory.
  Gdy masz 3+ pozycji z cenami â†’ tabela HTML (<table>).
  NIGDY nie zamieniaj konkretnej liczby na ogÃ³lnik:
    âŒ "kilkadziesiÄ…t zÅ‚otych" â† gdy ÅºrÃ³dÅ‚o mÃ³wi "50â€“150 zÅ‚/mÂ²"
    âŒ "sporo kosztuje" â† gdy ÅºrÃ³dÅ‚o mÃ³wi "9 000 zÅ‚"
    âŒ "wciÄ…ga budÅ¼et jak odkurzacz" â† metafora zamiast ceny
  JeÅ›li SERP podaje cenÄ™ â†’ PRZEPISZ wideÅ‚ki. Nie streszczaj liczb sÅ‚owami.

STYL: Fakt + co to znaczy w portfelu/kalendarzu czytelnika.
  Å¹LE: "SÄ…d moÅ¼e orzec grzywnÄ™, ograniczenie wolnoÅ›ci oraz karÄ™ pozbawienia wolnoÅ›ci."
  DOBRZE: "NajczÄ™Å›ciej koÅ„czy siÄ™ grzywnÄ… i zakazem na 3 lata â€” ale recydywa oznacza wiÄ™zienie bez zawieszenia."
  NIE buduj napiÄ™cia dramatycznymi krÃ³tkimi zdaniami. To poradnik, nie thriller.

RYTM: mieszaj dÅ‚ugoÅ›Ä‡ zdaÅ„. Nie pisz trzech zdaÅ„ o podobnej dÅ‚ugoÅ›ci pod rzÄ…d.
  Czasem uÅ¼yj zdania 5-sÅ‚owowego. Czasem rozwiÅ„ myÅ›l na 25 sÅ‚Ã³w.
  Naturalny rytm = rÃ³Å¼norodnoÅ›Ä‡, nie formuÅ‚a.
Podmiot konkretny (inwestor, ekipa, hydraulik) + czynnoÅ›Ä‡ + LICZBA/FAKT.
NIE zaczynaj 2+ zdaÅ„ w akapicie od tego samego wzorca.

JEDNOSTKI: zawsze spacja przed jednostkÄ…. TysiÄ…ce oddzielaj spacjÄ….
  âœ… 10 mÂ², 2 500 zÅ‚, 120 kg, 15 cm  âŒ 10mÂ², 2500zÅ‚, 120kg, 15cm

KOÅƒCZENIE SEKCJI: ostatnie zdanie sekcji H2 = konkretny fakt, NIE moraÅ‚.
  âŒ 'Dlatego tak waÅ¼ne jest, aby...' / 'PamiÄ™tajmy, Å¼e...' / 'Warto zatem...'
  âœ… 'Czas oczekiwania na decyzjÄ™: 14â€“30 dni roboczych.' / 'Koszt Å‚Ä…czny: ok. 8 500 zÅ‚.'

OTWIERANIE SEKCJI: KaÅ¼da sekcja H2 MUSI zaczynaÄ‡ siÄ™ INNYM zdaniem.
  â›” ZAKAZANY WZORZEC: "[Fraza gÅ‚Ã³wna] zaczyna siÄ™ od..." / "[Fraza gÅ‚Ã³wna] rzadko..." / "[Fraza gÅ‚Ã³wna] najÅ‚atwiej..."
  W artykule 5+ sekcji NIE WOLNO zaczynaÄ‡ 2 sekcje od tej samej frazy.
  Zacznij od: konkretnej liczby, pytania, nazwy materiaÅ‚u, sytuacji â€” nie od frazy gÅ‚Ã³wnej.
  Å¹LE: "WykoÅ„czenie domu zaczyna siÄ™ od...", "WykoÅ„czenie domu rzadko trzyma siÄ™..."
  DOBRZE: "Salon 30 mÂ² z panelami zamyka siÄ™ w 1 500â€“3 000 zÅ‚ â€” ale lista na tym siÄ™ nie koÅ„czy."

INTERPUNKCJA: przecinki przed: Å¼e, ktÃ³ry, poniewaÅ¼, aby.
FLEKSJA: odmieniaj frazy przez przypadki â€” to jedno uÅ¼ycie, nie powtÃ³rzenie.

FORMAT: h2:/h3: dla nagÅ‚Ã³wkÃ³w. Zero markdown â€” Å¼adnych **, __, #, <h2>, <h3>.
  KaÅ¼dy h2:/h3: MUSI zaczynaÄ‡ siÄ™ w NOWEJ LINII z pustÄ… liniÄ… powyÅ¼ej.
  Å¹LE: "...decyzje procesowe. H3: Co w praktyce"
  DOBRZE: "...decyzje procesowe.\n\nH3: Co w praktyce"

NAZWY FIRM I PLATFORM: nie uÅ¼ywaj nazw wÅ‚asnych.
  Nurofen â†’ ibuprofen, OLX â†’ portal ogÅ‚oszeniowy.
</zasady>""")

    # â•â•â• 3. ENTITY SEO â•â•â•
    parts.append("""<encje>
SALIENCE â€” encja gÅ‚Ã³wna MUSI dominowaÄ‡ w tekÅ›cie:
  PODMIOT > DOPEÅNIENIE: encja gÅ‚Ã³wna = podmiot zdania (kto/co?), nie peryferia.
    âœ… â€Jazda po alkoholu skutkuje..." / â€Retinol przyspiesza..."
    âŒ â€WaÅ¼nym aspektem jest jazda po alkoholu" / â€W przypadku retinolu..."
  POZYCJA: encja gÅ‚Ã³wna w pierwszym zdaniu akapitu = wyÅ¼sza salience.
  KOLOKACJA: powiÄ…zane encje w TYM SAMYM akapicie.
  SPÃ“JNA FORMA: nie przeskakuj miÄ™dzy wariantami nazwy.

NIE LISTUJ ENCJI â€” OPISUJ RELACJE:
  âŒ â€art. 178a KK, zakaz prowadzenia, Å›wiadczenie pieniÄ™Å¼ne" (lista)
  âœ… â€Art. 178a KK penalizuje jazdÄ™ zakazem prowadzenia od 3 lat i Å›wiadczeniem od 5000 zÅ‚" (relacja)
  UÅ¼ywaj fraz: â€reguluje", â€prowadzi do", â€jest typem", â€zapobiega", â€zostaÅ‚ wprowadzony przez".

CZYSTOÅšÄ† TEMATYCZNA: kaÅ¼da sekcja H2 = JEDEN podtemat, wyczerpany do koÅ„ca.
  Nie mieszaj 2 podtematÃ³w. Nie wracaj do podtematu omÃ³wionego we wczeÅ›niejszym H2.

POLISEMIA: gdy encja wieloznaczna â€” doprecyzuj kontekst przy PIERWSZYM uÅ¼yciu.

INFORMATION GAIN: w kaÅ¼dej sekcji H2 dodaj MIN 1 element ktÃ³rego NIE MA w danych z konkurencji:
  unikatowe porÃ³wnanie, wyjÄ…tek od reguÅ‚y, praktyczna wskazÃ³wka, maÅ‚o znany fakt.
</encje>""")

    # â•â•â• 4. ANTY-AI â•â•â•
    parts.append("""<anty_ai>
ZAKAZANE WZORCE (typowe dla AI):
  Frazesy: "warto zauwaÅ¼yÄ‡/pamiÄ™taÄ‡/podkreÅ›liÄ‡", "naleÅ¼y podkreÅ›liÄ‡",
    "kluczowe jest", "istotne jest", "podsumowujÄ…c", "w tym kontekÅ›cie".
  WypeÅ‚niacze: "w Å›wietle obowiÄ…zujÄ…cych przepisÃ³w", "zgodnie z literÄ… prawa",
    "nie bez znaczenia jest fakt", "trzeba mieÄ‡ na uwadze", "jak sama nazwa wskazuje".
  MoraÅ‚y: "dlatego tak waÅ¼ne jest, aby", "pamiÄ™tajmy, Å¼e", "warto zatem".
  ÅÄ…czniki: "W odniesieniu do", "Ma to na celu", "Proces ten", "Jest to".
  Zombie zdania: "Istotnym elementem jest..." â†’ Kto? Co? Nazwij podmiot.
  Przymiotniki: max 1Ã— "kluczowy/istotny/zasadniczy" na akapit.

PUSTE PRZEBIEGI (AI slop â€” ZERO TOLERANCJI):
  NIGDY "ta sytuacja/ten problem/ta kwestia/ten aspekt/omawiany temat" jako podmiot.
  TEST: Czy zdanie da siÄ™ zastÄ…piÄ‡ sÅ‚owem "coÅ›"? JeÅ›li tak â€” podaj KONKRET.
  âŒ "RÃ³Å¼nica kilku decyzji zmienia budÅ¼et o dziesiÄ…tki procent" â†’ JAKIE? ILE?
</anty_ai>""")

    # â•â•â• 5. Å¹RÃ“DÅA â•â•â•
    if is_ymyl:
        parts.append("""<zrodla>
YMYL â€” zero tolerancji dla zmyÅ›leÅ„.
Wiedza WYÅÄ„CZNIE z: stron SERP (podane), przepisÃ³w (podane), Wikipedia (podane).
Nie wymyÅ›laj liczb, dat, sygnatur, nazw badaÅ„. Nie znasz â†’ pomiÅ„.
</zrodla>""")
    else:
        parts.append("""<zrodla>
Wiedza z: stron SERP, Wikipedia, danych liczbowych (podane).
Nie wymyÅ›laj liczb, dat, nazw badaÅ„. Brak danych â†’ opisz ogÃ³lnie.
Gdy SERP podaje cenÄ™/stawkÄ™ â†’ PRZEPISZ wideÅ‚ki. Nie streszczaj liczb sÅ‚owami.
</zrodla>""")

    # â•â•â• 5b. STYL KATEGORII â•â•â•
    cat_style = _CATEGORY_STYLE.get(detected_category, "")
    if cat_style:
        parts.append(f"<styl_kategorii>\n{cat_style}\n</styl_kategorii>")

    # â•â•â• 6. PRZYKÅAD (per-kategoria) â•â•â•
    _EXAMPLES = {
        "prawo": (
            'TAK: "Granica jest prosta: do 0,5 promila to wykroczenie, powyÅ¼ej â€” przestÄ™pstwo.\n'
            'Typowy kierowca zÅ‚apany pierwszy raz z wynikiem tuÅ¼ ponad prÃ³g dostanie\n'
            'grzywnÄ™ i zakaz na 3 lata."\n\n'
            'NIE: "Sytuacja prawna kierowcy ulega zmianie w zaleÅ¼noÅ›ci od okolicznoÅ›ci.\n'
            'Ten aspekt jest szczegÃ³lnie istotny w kontekÅ›cie aktualnych regulacji."\n'
            'â†‘ dwa zdania, ZERO konkretÃ³w. UsuÅ„.'
        ),
        "budownictwo": (
            'TAK: "Panele laminowane z montaÅ¼em: 50â€“150 zÅ‚/mÂ². Salon 30 mÂ² to 1 500â€“4 500 zÅ‚\n'
            'za samÄ… podÅ‚ogÄ™ plus 300â€“500 zÅ‚ na listwy i podkÅ‚ady."\n\n'
            'TAK (tabela):\n'
            '<table>\n'
            '<tr><th>UsÅ‚uga</th><th>Cena od</th><th>Cena do</th></tr>\n'
            '<tr><td>Malowanie Å›cian</td><td>20 zÅ‚/mÂ²</td><td>25 zÅ‚/mÂ²</td></tr>\n'
            '<tr><td>UkÅ‚adanie pÅ‚ytek</td><td>90 zÅ‚/mÂ²</td><td>140 zÅ‚/mÂ²</td></tr>\n'
            '</table>\n\n'
            'NIE: "WykoÅ„czenie domu zaczyna siÄ™ od sprawdzenia stanu deweloperskiego.\n'
            'Ta sytuacja zmienia budÅ¼et."\n'
            'â†‘ ZERO liczb, pusty zaimek. UsuÅ„.'
        ),
        "medycyna": (
            'TAK: "Ibuprofen 400 mg co 6â€“8 h Å‚agodzi bÃ³l w ciÄ…gu 30â€“60 min.\n'
            'Kwas hialuronowy wiÄ…Å¼e czÄ…steczki wody w naskÃ³rku, tworzÄ…c barierÄ™ okluzyjnÄ…."\n\n'
            'NIE: "Lek skutecznie pomaga na dolegliwoÅ›ci. Ten problem jest powszechny."\n'
            'â†‘ brak dawki, mechanizmu, nazwy substancji. UsuÅ„.'
        ),
    }
    _default_example = (
        'TAK: Zdanie z konkretnÄ… liczbÄ…, stawkÄ… lub faktem.\n'
        'NIE: Zdanie ogÃ³lnikowe bez danych â€” "ta sytuacja", "ten problem" = do usuniÄ™cia.'
    )
    example_text = _EXAMPLES.get(detected_category, _default_example)
    parts.append(f"<przyklad>\n{example_text}\n</przyklad>")

    return "\n\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCHEMA GUARD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_CRITICAL_FIELDS = ["keywords", "main_keyword", "batch_number"]
_IMPORTANT_FIELDS = [
    "gpt_instructions_v39", "enhanced", "h2_remaining",
    "article_memory", "keyword_limits", "coverage",
]

def _schema_guard(pre_batch):
    missing_critical = [f for f in _CRITICAL_FIELDS if f not in pre_batch or pre_batch[f] is None]
    missing_important = [f for f in _IMPORTANT_FIELDS if f not in pre_batch or pre_batch[f] is None]
    if missing_critical:
        _pb_logger.warning(f"âš ï¸ SCHEMA GUARD: Missing CRITICAL fields: {missing_critical}.")
    if missing_important:
        _pb_logger.info(f"â„¹ï¸ Schema guard: Missing optional: {missing_important}")
    enhanced = pre_batch.get("enhanced") or {}
    if enhanced:
        expected = ["smart_instructions_formatted", "causal_context", "information_gain", "relations_to_establish"]
        missing_enh = [f for f in expected if not enhanced.get(f)]
        if missing_enh:
            _pb_logger.info(f"â„¹ï¸ Enhanced missing: {missing_enh}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USER PROMPT (v2.1 â€” 10 formatterÃ³w)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_user_prompt(pre_batch, h2, batch_type, article_memory=None):
    pre_batch = pre_batch or {}
    sections = []

    _schema_guard(pre_batch)

    formatters = [
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_keywords(pre_batch),
        lambda: _fmt_legal_medical(pre_batch),
        lambda: _fmt_entity_context_v2(pre_batch),
        lambda: _fmt_natural_polish(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_serp_enrichment_v2(pre_batch),
        lambda: _fmt_h2_remaining(pre_batch),
        lambda: _fmt_intro_guidance_v2(pre_batch, batch_type),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception as exc:
            _pb_logger.warning(f"Formatter failed: {exc}")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHARED FORMATTERS (used by article + category prompts)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_batch_header(pre_batch, h2, batch_type):
    batch_number = pre_batch.get("batch_number", 1)
    total_batches = pre_batch.get("total_planned_batches", 1)
    batch_length = pre_batch.get("batch_length") or {}
    min_w = batch_length.get("min_words", 350)
    max_w = batch_length.get("max_words", 500)

    section_length = pre_batch.get("section_length_guidance") or {}
    length_hint = ""
    if section_length:
        suggested = section_length.get("suggested_words") or section_length.get("target_words")
        if suggested:
            length_hint = f"\nSugerowana dÅ‚ugoÅ›Ä‡ tej sekcji: ~{suggested} sÅ‚Ã³w."

    h2_instruction = ""
    if batch_type not in ("INTRO", "intro"):
        h2_instruction = f"\nZaczynaj DOKÅADNIE od: h2: {h2}"

    return f"""â•â•â• BATCH {batch_number}/{total_batches}: {batch_type} â•â•â•
Sekcja H2: "{h2}"
DÅ‚ugoÅ›Ä‡: {min_w}-{max_w} sÅ‚Ã³w{length_hint}{h2_instruction}"""


def _parse_target_max(target_total_str):
    if not target_total_str:
        return 0
    if isinstance(target_total_str, (int, float)):
        return int(target_total_str)
    try:
        parts = str(target_total_str).replace("x", "").split("-")
        if len(parts) >= 2:
            return int(parts[-1].strip())
        return int(parts[0].strip())
    except (ValueError, IndexError):
        return 0


def _fmt_keywords(pre_batch):
    keywords_info = pre_batch.get("keywords") or {}
    keyword_limits = pre_batch.get("keyword_limits") or {}
    soft_caps = pre_batch.get("soft_cap_recommendations") or {}

    _kw_global_remaining = pre_batch.get("_kw_global_remaining", None)
    _main_kw_budget_exhausted = (_kw_global_remaining is not None and _kw_global_remaining == 0)
    _raw_main_kw = pre_batch.get("main_keyword") or {}
    main_kw = _raw_main_kw.get("keyword", "") if isinstance(_raw_main_kw, dict) else str(_raw_main_kw)

    # â”€â”€ MUST USE â”€â”€
    must_raw = keywords_info.get("basic_must_use", [])
    must_lines = []
    _budget_exhausted_kws = []
    for kw in must_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            if _main_kw_budget_exhausted and name and main_kw and name.lower() == main_kw.lower():
                _budget_exhausted_kws.append(name)
                continue
            actual = kw.get("actual", kw.get("actual_uses", kw.get("current_count", 0)))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            hard_max = kw.get("hard_max_this_batch", "")
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            line = f'  â€¢ "{name}"'
            if hard_max:
                line += f" (max {hard_max}Ã—)"
            elif remaining and int(remaining) <= 2:
                line += f" (jeszcze {remaining}Ã—)"
            must_lines.append(line)
        else:
            must_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ EXTENDED â”€â”€
    ext_raw = keywords_info.get("extended_this_batch", [])
    ext_lines = []
    for kw in ext_raw:
        if isinstance(kw, dict):
            ext_lines.append(f'  â€¢ "{kw.get("keyword", "")}"')
        else:
            ext_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ STOP â”€â”€
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_lines = []
    for s in stop_raw:
        if isinstance(s, dict):
            name = s.get("keyword", "")
            current = s.get("current_count", s.get("current", s.get("actual", "?")))
            max_c = s.get("max_count", s.get("max", s.get("target_max", "?")))
            stop_lines.append(f'  â€¢ "{name}" (juÅ¼ {current}Ã—, limit {max_c}) STOP!')
        else:
            stop_lines.append(f'  â€¢ "{s}"')
    for exhausted_kw in _budget_exhausted_kws:
        stop_lines.append(f'  â€¢ "{exhausted_kw}" (limit globalny osiÄ…gniÄ™ty â€” NIE UÅ»YWAJ!)')

    # â”€â”€ CAUTION â”€â”€
    caution_raw = keyword_limits.get("caution_keywords") or []
    caution_names = []
    for c in caution_raw:
        if isinstance(c, dict):
            caution_names.append(c.get("keyword", ""))
        else:
            caution_names.append(str(c))
    caution_names = [n for n in caution_names if n]

    # â”€â”€ SOFT CAPS â”€â”€
    soft_notes = []
    if soft_caps:
        for kw_name, info in soft_caps.items():
            if isinstance(info, dict):
                action = info.get("action", "")
                if action and action != "OK":
                    soft_notes.append(f'  â„¹ï¸ "{kw_name}": {action}')

    _kw_force_ban = pre_batch.get("_kw_force_ban", False)
    if _kw_force_ban and main_kw:
        must_lines = [l for l in must_lines if main_kw.lower() not in l.lower()]

    # â”€â”€ BUILD â”€â”€
    parts = ["â•â•â• FRAZY KLUCZOWE â•â•â•"]

    if _kw_force_ban and main_kw:
        parts.append(f'â›” STOP: Fraza "{main_kw}" jest PRZEKROCZONA â€” nie uÅ¼ywaj w tym batchu.\n')

    if must_lines:
        parts.append("TEMATY OBOWIÄ„ZKOWE (poruszyj w treÅ›ci):")
        parts.extend(must_lines)
    if ext_lines:
        parts.append("\nTEMATY DODATKOWE (wpleÄ‡ jeÅ›li pasujÄ…):")
        parts.extend(ext_lines)
    if stop_lines:
        parts.append("\nğŸ›‘ STOP â€” nie uÅ¼ywaj (przekroczone):")
        parts.extend(stop_lines)
    if caution_names:
        parts.append(f"\nâš ï¸ OSTROÅ»NIE (max 1Ã— kaÅ¼da): {', '.join(caution_names)}")
    if soft_notes:
        parts.append("")
        parts.extend(soft_notes)

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_continuation(pre_batch):
    continuation = pre_batch.get("continuation_v39") or {}
    enhanced = pre_batch.get("enhanced") or {}
    cont_ctx = enhanced.get("continuation_context") or {}

    last_h2 = cont_ctx.get("last_h2") or continuation.get("last_h2", "")
    last_ending = cont_ctx.get("last_paragraph_ending") or continuation.get("last_paragraph_ending", "")
    last_topic = cont_ctx.get("last_topic") or continuation.get("last_topic", "")
    transition_hint = continuation.get("transition_hint", "")

    if not last_h2 and not last_ending:
        return ""

    parts = ["â•â•â• KONTYNUACJA â•â•â•", "Poprzedni batch zakoÅ„czyÅ‚ siÄ™ na:"]
    if last_h2:
        parts.append(f'  Ostatni H2: "{last_h2}"')
    if last_ending:
        ending_preview = last_ending[:150] + ("..." if len(last_ending) > 150 else "")
        parts.append(f'  Ostatnie zdanie: "{ending_preview}"')
    if last_topic:
        parts.append(f'  Temat: {last_topic}')
    parts.append("\nZacznij PÅYNNIE: nawiÄ…Å¼ do poprzedniego wÄ…tku, ale nie powtarzaj zakoÅ„czenia.")
    if transition_hint:
        parts.append(f'Sugerowane przejÅ›cie: {transition_hint}')
    return "\n".join(parts)


def _fmt_article_memory(article_memory):
    if not article_memory:
        return ""

    parts = ["â•â•â• PAMIÄ˜Ä† ARTYKUÅU (nie powtarzaj!) â•â•â•"]

    if isinstance(article_memory, dict):
        topics = article_memory.get("topics_covered") or article_memory.get("covered_topics") or []
        if topics:
            parts.append("Sekcje juÅ¼ napisane:")
            for t in topics[:10]:
                if isinstance(t, str):
                    parts.append(f'  âœ“ {t}')
                elif isinstance(t, dict):
                    parts.append(f'  âœ“ {t.get("topic", t.get("h2", ""))}')

        facts = article_memory.get("key_facts_used") or article_memory.get("facts", [])
        key_points = article_memory.get("key_points") or []
        avoid_rep = article_memory.get("avoid_repetition") or []

        all_facts = list(facts) + list(key_points)
        if all_facts:
            parts.append("\nFakty juÅ¼ podane (NIE POWTARZAJ):")
            for f in all_facts[:12]:
                parts.append(f'  â€¢ {f}' if isinstance(f, str) else f'  â€¢ {json.dumps(f, ensure_ascii=False)[:100]}')

        if avoid_rep:
            parts.append("\nâ›” UÅ»YTE ZDANIA â€” NIE POWTARZAJ DOSÅOWNIE:")
            for r in avoid_rep[:8]:
                parts.append(f'  âŒ "{r}"')

    elif isinstance(article_memory, str):
        parts.append(_word_trim(article_memory, 1500))

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_h2_remaining(pre_batch):
    h2_remaining = pre_batch.get("h2_remaining") or []
    if not h2_remaining:
        return ""
    h2_list = ", ".join(f'"{h}"' for h in h2_remaining[:6])
    return f"â•â•â• PLAN â•â•â•\nPozostaÅ‚e sekcje H2: {h2_list}\nNie zachodÅº na ich tematy."


def _fmt_output_format(h2, batch_type):
    if batch_type in ("INTRO", "intro"):
        return """â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ leadu. NIE zaczynaj od "h2:". Lead nie ma nagÅ‚Ã³wka.
120-200 sÅ‚Ã³w. FrazÄ™ gÅ‚Ã³wnÄ… wpleÄ‡ w PIERWSZE zdanie.
NIE dodawaj komentarzy, meta-tekstu. TYLKO treÅ›Ä‡ leadu."""

    return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ tego batcha. Zaczynaj od:

h2: {h2}

Akapity po 3-5 zdaÅ„. Opcjonalnie h3: [podsekcja].
KaÅ¼dy akapit powinien zawieraÄ‡ min. 1 konkretny fakt (liczbÄ™, stawkÄ™, wymiar, termin).
Zdania bez informacji ("Ta sytuacja...", "Ten problem...") = DO USUNIÄ˜CIA.
KAÅ»DY h3: na OSOBNEJ linii z pustÄ… liniÄ… powyÅ¼ej i poniÅ¼ej.
Å»ADEN nagÅ‚Ã³wek NIE moÅ¼e byÄ‡ wklejony w Å›rodek akapitu.
Zero markdown (**, __, #). Zero tagÃ³w HTML (<h2>, <h3>, <b>).
NIE dodawaj komentarzy. TYLKO treÅ›Ä‡ artykuÅ‚u."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEW v2 FORMATTERS (article only)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_entity_context_v2(pre_batch):
    """v2.3: Smart S1 context â€” per-H2 filtered data from _build_batch_s1_context."""
    parts = []
    s1_ctx = pre_batch.get("_s1_context") or {}

    _raw_main = pre_batch.get("main_keyword") or {}
    main_name = _raw_main.get("keyword", "") if isinstance(_raw_main, dict) else str(_raw_main)
    _entity_seo = (pre_batch.get("s1_data") or {}).get("entity_seo") or \
        pre_batch.get("entity_seo") or {}

    # â”€â”€ Block 1: Synonyms (static â€” needed for anaphora in every batch) â”€â”€
    if main_name:
        synonyms = _entity_seo.get("entity_synonyms", [])[:5]
        if synonyms:
            parts.append(f"â•â•â• ENCJE â•â•â•\nSynonimy: {', '.join(str(s) for s in synonyms)}")
        else:
            parts.append("â•â•â• ENCJE â•â•â•")

    # â”€â”€ Block 2: Lead entity + concepts for THIS section â”€â”€
    lead = s1_ctx.get("lead_entity")
    concepts = s1_ctx.get("concepts", [])
    e_gaps = s1_ctx.get("entity_gaps", [])

    concept_parts = []
    if lead and lead.lower() != main_name.lower():
        concept_parts.append(f"ğŸ¯ Encja wiodÄ…ca sekcji: {lead}")
    all_to_weave = concepts[:]
    for g in e_gaps:
        if g not in all_to_weave:
            all_to_weave.append(f"{g} [luka]")
    if all_to_weave:
        concept_parts.append(f"WpleÄ‡: {', '.join(all_to_weave[:6])}")
    if concept_parts:
        parts.append("\n".join(concept_parts))

    # â”€â”€ Block 3: EAV facts (filtered per H2) â”€â”€
    eav = s1_ctx.get("eav", [])
    if eav:
        eav_lines = ["Fakty (wpleÄ‡ w zdania, nie listuj):"]
        for e in eav[:5]:
            marker = "ğŸ¯" if e.get("is_primary") else "â€¢"
            eav_lines.append(f'  {marker} {e.get("entity","")} â†’ {e.get("attribute","")} â†’ {e.get("value","")}')
        parts.append("\n".join(eav_lines))

    # â”€â”€ Block 4: SVO relations (filtered per H2 â€” NEW in article prompt) â”€â”€
    svo = s1_ctx.get("svo", [])
    if svo:
        svo_lines = ["Relacje (opisz swoimi sÅ‚owami):"]
        for t in svo[:3]:
            ctx = f' [{t.get("context","")}]' if t.get("context") else ""
            svo_lines.append(f'  â€¢ {t.get("subject","")} â†’ {t.get("verb","")} â†’ {t.get("object","")}{ctx}')
        parts.append("\n".join(svo_lines))

    # â”€â”€ Block 5: Causal chains (NEW â€” first time in article prompt) â”€â”€
    causal = s1_ctx.get("causal", [])
    if causal:
        causal_lines = ["ÅaÅ„cuchy przyczynowe (uÅ¼yj do wyjaÅ›niania DLACZEGO):"]
        for c in causal[:2]:
            if isinstance(c, dict):
                text = c.get("chain", c.get("text", str(c)))
            else:
                text = str(c)
            causal_lines.append(f"  â›“ï¸ {_word_trim(text, 150)}")
        parts.append("\n".join(causal_lines))

    # â”€â”€ Block 6: Content gaps for THIS section (NEW) â”€â”€
    gaps = s1_ctx.get("gaps", [])
    if gaps:
        parts.append(f"Luki TOP10 (information gain): {', '.join(gaps[:3])}")

    # â”€â”€ Block 7: Co-occurrence pairs for THIS section â”€â”€
    cooc = s1_ctx.get("cooc", [])
    if cooc:
        parts.append(f"Encje razem w akapicie: {' | '.join(cooc[:4])}")

    # â”€â”€ Block 8: Information gain (from master API, per-batch) â”€â”€
    enhanced = pre_batch.get("enhanced") or {}
    info_gain = enhanced.get("information_gain", "")
    if info_gain:
        parts.append(f"Przewaga nad konkurencjÄ…: {_word_trim(info_gain, 200)}")

    # â”€â”€ Block 9: Semantic angle (from master API, per-batch) â”€â”€
    plan = pre_batch.get("semantic_batch_plan") or {}
    if plan:
        h2_coverage = plan.get("h2_coverage") or {}
        for h2_name, info in h2_coverage.items():
            if isinstance(info, dict):
                angle = info.get("semantic_angle", "")
                if angle:
                    parts.append(f"KÄ…t sekcji: {angle}")
                    break

    # â”€â”€ Fallback: if _s1_context empty, use old static fields â”€â”€
    if not s1_ctx:
        must_concepts = pre_batch.get("_must_cover_concepts") or []
        old_eav = pre_batch.get("_eav_triples") or []
        old_gaps = pre_batch.get("_entity_gaps") or []
        if must_concepts:
            names = [c.get("text", c) if isinstance(c, dict) else str(c) for c in must_concepts[:8]]
            parts.append(f"WpleÄ‡: {', '.join(n for n in names if n)}")
        if old_eav:
            eav_lines = ["Fakty (wpleÄ‡ w zdania):"]
            for e in old_eav[:4]:
                eav_lines.append(f'  â€¢ {e.get("entity","")} â†’ {e.get("attribute","")} â†’ {e.get("value","")}')
            parts.append("\n".join(eav_lines))
        if old_gaps:
            gap_names = [g.get("entity", "") for g in old_gaps if g.get("priority") == "high"][:3]
            if gap_names:
                parts.append(f"Luki: {', '.join(gap_names)}")

    return "\n\n".join(parts) if parts else ""


def _fmt_serp_enrichment_v2(pre_batch):
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}

    paa = serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or []
    lsi = serp.get("lsi_keywords") or []
    chips = serp.get("refinement_chips") or []

    if not paa and not lsi and not chips:
        return ""

    parts = ["â•â•â• SERP â•â•â•"]
    if chips:
        parts.append(f"Podtematy Google: {', '.join(str(c) for c in chips[:8])}")
    if paa:
        q_strs = []
        for q in paa[:4]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                q_strs.append(str(q_text))
        if q_strs:
            parts.append("Pytania PAA (odpowiedz na 1-2):\n  " + "\n  ".join(q_strs))
    if lsi:
        # Deduplicate: skip LSI keywords already in EXTENDED
        _ext_kws = pre_batch.get("keywords", {}).get("extended_this_batch", [])
        _ext_names = {(k.get("keyword", k) if isinstance(k, dict) else str(k)).lower().strip()
                      for k in _ext_kws}
        lsi_names = []
        for l in lsi[:8]:
            name = l.get("keyword", l) if isinstance(l, dict) else l
            if str(name).lower().strip() not in _ext_names:
                lsi_names.append(str(name))
        if lsi_names:
            parts.append(f"LSI: {', '.join(lsi_names)}")

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_intro_guidance_v2(pre_batch, batch_type):
    if batch_type not in ("INTRO", "intro"):
        return ""

    main_kw = pre_batch.get("main_keyword") or {}
    kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
    serp = pre_batch.get("serp_enrichment") or {}

    parts = ["â•â•â• LEAD (WSTÄ˜P) â•â•â•", "120-200 sÅ‚Ã³w. NIE zaczynaj od h2:."]
    if kw_name:
        parts.append(f'Zacznij od sedna: czym jest "{kw_name}" i dlaczego czytelnik powinien czytaÄ‡ dalej.')
    parts.append("Kontekst praktyczny + konkretny fakt. NIE zapowiadaj co bÄ™dzie dalej.")

    search_intent = serp.get("search_intent", "")
    if search_intent:
        parts.append(f"Intencja: {search_intent}")

    guidance = pre_batch.get("intro_guidance", "")
    if guidance:
        if isinstance(guidance, dict):
            hook = guidance.get("hook", "")
            if hook:
                parts.append(f"Hak: {hook}")
        elif isinstance(guidance, str) and len(str(guidance)) > 10:
            parts.append(str(guidance)[:300])

    return "\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LEGAL / MEDICAL (used by article v2 â€” kept in full)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_legal_medical(pre_batch):
    legal_ctx = pre_batch.get("legal_context") or {}
    medical_ctx = pre_batch.get("medical_context") or {}
    ymyl_enrich = pre_batch.get("_ymyl_enrichment") or {}
    ymyl_intensity = pre_batch.get("_ymyl_intensity", "full")

    parts = []

    if ymyl_intensity == "light":
        light_note = pre_batch.get("_light_ymyl_note", "")
        if light_note:
            parts.append("â•â•â• ASPEKT REGULACYJNY (peryferyjny) â•â•â•")
            parts.append(f"  {light_note}")
            parts.append("  âš ï¸ Wspomnij o regulacjach MAX 1-2 razy w CAÅYM artykule.")
        return "\n".join(parts) if parts else ""

    if legal_ctx and legal_ctx.get("active"):
        parts.append("â•â•â• KONTEKST PRAWNY (YMYL) â•â•â•")
        parts.append("NIE wymyÅ›laj sygnatur, dat orzeczeÅ„ ani numerÃ³w artykuÅ‚Ã³w.")
        parts.append("Placeholder 'odpowiednich przepisÃ³w' â†’ zawsze podaj konkretny art.")

        wiki_arts = pre_batch.get("legal_wiki_articles") or []
        if wiki_arts:
            parts.append("\nWIKIPEDIA:")
            for w in wiki_arts[:4]:
                if w.get("found"):
                    parts.append(f"  [{w['article_ref']}] {w['title']}:")
                    parts.append(f"  {w['extract'][:300]}")
                    parts.append(f"  Å¹rÃ³dÅ‚o: {w['url']}")
                    parts.append("")

        legal_enrich = ymyl_enrich.get("legal", {})
        if legal_enrich.get("articles"):
            parts.append("\nPODSTAWA PRAWNA:")
            for art in legal_enrich["articles"][:5]:
                parts.append(f"  â€¢ {art}")
        if legal_enrich.get("acts"):
            parts.append(f"  Ustawy: {', '.join(legal_enrich['acts'][:4])}")
        if legal_enrich.get("key_concepts"):
            parts.append(f"  PojÄ™cia: {', '.join(legal_enrich['key_concepts'][:6])}")

        instruction = legal_ctx.get("legal_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        judgments = legal_ctx.get("top_judgments") or []
        if judgments:
            parts.append("\nOrzeczenia (dostÄ™pne, ale NIE musisz cytowaÄ‡):")
            parts.append("  âš ï¸ UÅ¼yj MAX 1 orzeczenia i TYLKO gdy bezpoÅ›rednio dotyczy tematu sekcji.")
            parts.append("  âš ï¸ NIE cytuj wyroku cywilnego (sygn. I C, III RC) w tekÅ›cie o odpowiedzialnoÅ›ci karnej.")
            parts.append("  âš ï¸ Lepiej pominÄ…Ä‡ orzeczenie niÅ¼ wcisnÄ…Ä‡ nieadekwatne.")
            for j in judgments[:3]:
                if isinstance(j, dict):
                    sig = j.get("signature", j.get("caseNumber", ""))
                    court = j.get("court", j.get("courtName", ""))
                    date = j.get("date", j.get("judgmentDate", ""))
                    matched = j.get("matched_article", "")
                    line = f'  â€¢ {sig}, {court} ({date})'
                    if matched:
                        line += f' [dot. {matched}]'
                    parts.append(line)

        citation_hint = legal_ctx.get("citation_hint", "")
        if citation_hint:
            parts.append(f'\n{citation_hint}')

    if medical_ctx and medical_ctx.get("active"):
        if parts:
            parts.append("")
        parts.append("â•â•â• KONTEKST MEDYCZNY (YMYL) â•â•â•")
        parts.append("MUSISZ:")
        parts.append("  1. CytowaÄ‡ ÅºrÃ³dÅ‚a naukowe (podane niÅ¼ej)")
        parts.append("  2. NIE wymyÅ›laÄ‡ statystyk ani nazw badaÅ„")

        med_enrich = ymyl_enrich.get("medical", {})
        if med_enrich.get("specialization"):
            parts.append(f"\n  Specjalizacja: {med_enrich['specialization']}")
        if med_enrich.get("condition"):
            cond = med_enrich["condition"]
            latin = med_enrich.get("condition_latin", "")
            icd = med_enrich.get("icd10", "")
            parts.append(f"  Choroba/stan: {cond}" + (f" ({latin})" if latin else "") + (f" [ICD-10: {icd}]" if icd else ""))
        if med_enrich.get("key_drugs"):
            parts.append(f"  Leki: {', '.join(med_enrich['key_drugs'][:5])}")
        if med_enrich.get("evidence_note"):
            parts.append(f"\n  âš ï¸ WYTYCZNE: {med_enrich['evidence_note']}")

        parts.append("")
        parts.append("HIERARCHIA DOWODÃ“W:")
        parts.append("  1. Meta-analiza > 2. RCT > 3. Kohortowe > 4. Opis przypadku > 5. Opinia")

        instruction = medical_ctx.get("medical_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        publications = medical_ctx.get("top_publications") or []
        if publications:
            parts.append("\nPublikacje:")
            for p in publications[:5]:
                if isinstance(p, dict):
                    title = p.get("title", "")[:80]
                    authors = p.get("authors", "")[:40]
                    year = p.get("year", "")
                    pmid = p.get("pmid", "")
                    parts.append(f'  â€¢ {authors} ({year}): "{title}" PMID:{pmid}')

    return "\n".join(parts) if parts else ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATEGORY-ONLY FORMATTERS
# (used by build_category_user_prompt â€” NOT by article v2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_smart_instructions(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    smart = enhanced.get("smart_instructions_formatted", "")
    if smart:
        return f"â•â•â• INSTRUKCJE DLA TEGO BATCHA â•â•â•\n{smart[:1000]}"
    return ""


def _fmt_semantic_plan(pre_batch, h2):
    plan = pre_batch.get("semantic_batch_plan") or {}
    if not plan:
        return ""
    parts = ["â•â•â• CO PISAÄ† W TEJ SEKCJI â•â•â•"]
    h2_coverage = plan.get("h2_coverage") or {}
    for h2_name, info in h2_coverage.items():
        if isinstance(info, dict):
            angle = info.get("semantic_angle", "")
            must = info.get("must_phrases", [])
            if angle:
                parts.append(f'KÄ…t: {angle}')
            if must:
                parts.append(f'Frazy: {", ".join(f"{p}" for p in must[:5])}')
    direction = plan.get("content_direction") or plan.get("writing_direction", "")
    if direction:
        parts.append(f'Kierunek: {direction}')
    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_coverage_density(pre_batch):
    coverage = pre_batch.get("coverage") or {}
    density = pre_batch.get("density") or {}
    main_kw = pre_batch.get("main_keyword") or {}
    if not coverage and not density and not main_kw:
        return ""
    parts = ["â•â•â• STATUS POKRYCIA FRAZ â•â•â•"]
    if main_kw:
        kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
        synonyms = main_kw.get("synonyms", []) if isinstance(main_kw, dict) else []
        if kw_name:
            parts.append(f'HasÅ‚o gÅ‚Ã³wne: "{kw_name}"')
        if synonyms:
            parts.append(f'Synonimy: {", ".join(synonyms[:5])}')
    current_cov = coverage.get("current", coverage.get("current_coverage"))
    target_cov = coverage.get("target", coverage.get("target_coverage"))
    if current_cov is not None and target_cov is not None:
        parts.append(f'Pokrycie: {current_cov}% z {target_cov}%')
    missing = coverage.get("missing_phrases") or coverage.get("uncovered") or []
    if missing:
        parts.append("âš ï¸ BRAKUJÄ„CE:")
        for m in missing[:8]:
            name = m.get("keyword", m) if isinstance(m, dict) else m
            parts.append(f'  â†’ "{name}"')
    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_style(pre_batch):
    style = pre_batch.get("style_instructions") or pre_batch.get("style_instructions_v39") or {}
    if not style:
        return ""
    parts = ["â•â•â• STYL (dodatkowy) â•â•â•"]
    if isinstance(style, dict):
        # Skip 'tone' â€” system prompt already sets tone to avoid conflicts
        forbidden = style.get("forbidden_phrases") or style.get("avoid_phrases") or []
        if forbidden:
            parts.append(f'Unikaj teÅ¼: {", ".join(f"{f}" for f in forbidden[:8])}')
    elif isinstance(style, str):
        parts.append(_word_trim(style, 500))
    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_entity_salience(pre_batch):
    """Entity salience â€” used by category prompt. Full version kept."""
    parts = []

    local_instructions = pre_batch.get("_entity_salience_instructions", "")
    if local_instructions:
        parts.append(local_instructions)

    backend_placement = pre_batch.get("_backend_placement_instruction", "")
    if backend_placement:
        parts.append("â•â•â• ROZMIESZCZENIE ENCJI â•â•â•")
        parts.append("âš ï¸ WskazÃ³wki techniczne â€” NIE kopiuj dosÅ‚ownie.")
        parts.append(backend_placement)

    FLEXION_NOTE = (
        "\nâš ï¸ FLEKSJA: PojÄ™cia w mianowniku â€” odmieniaj przez przypadki."
    )
    concept_instr = pre_batch.get("_concept_instruction", "")
    must_concepts = pre_batch.get("_must_cover_concepts", [])
    if concept_instr:
        parts.append(concept_instr + FLEXION_NOTE)
    elif must_concepts:
        concept_names = [c.get("text", c) if isinstance(c, dict) else str(c) for c in must_concepts[:10]]
        parts.append(
            "â•â•â• POJÄ˜CIA TEMATYCZNE â•â•â•\n"
            f"WpleÄ‡ naturalnie: {', '.join(concept_names)}"
            + FLEXION_NOTE
        )

    cooc_pairs = pre_batch.get("_cooccurrence_pairs") or []
    if cooc_pairs:
        cooc_lines = []
        for pair in cooc_pairs[:8]:
            if isinstance(pair, dict):
                e1 = pair.get("entity1", pair.get("source", ""))
                e2 = pair.get("entity2", pair.get("target", ""))
                if e1 and e2:
                    cooc_lines.append(f'  â€¢ "{e1}" + "{e2}"')
        if cooc_lines:
            parts.append("â•â•â• WSPÃ“ÅWYSTÄ˜POWANIE â•â•â•\n" + "\n".join(cooc_lines))

    first_para_ents = pre_batch.get("_first_paragraph_entities") or []
    if first_para_ents:
        fp_names = [ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent) for ent in first_para_ents[:6]]
        fp_names = [f'"{n}"' for n in fp_names if n]
        if fp_names:
            parts.append(f"PIERWSZY AKAPIT: {', '.join(fp_names)}")

    h2_ents = pre_batch.get("_h2_entities") or []
    if h2_ents:
        h2_names = [ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent) for ent in h2_ents[:8]]
        h2_names = [f'"{n}"' for n in h2_names if n]
        if h2_names:
            parts.append(f"ENCJE H2: {', '.join(h2_names)}")

    eav_triples = pre_batch.get("_eav_triples") or []
    if eav_triples:
        eav_lines = ["â•â•â• CECHY ENCJI (EAV) â•â•â•"]
        for e in eav_triples[:10]:
            eav_lines.append(f'  â€¢ "{e.get("entity","")}": {e.get("attribute","")} â†’ {e.get("value","")}')
        parts.append("\n".join(eav_lines))

    svo_triples = pre_batch.get("_svo_triples") or []
    if svo_triples:
        svo_lines = ["â•â•â• RELACJE (SVO) â•â•â•"]
        for t in svo_triples[:12]:
            svo_lines.append(f'  {t.get("subject","")} â†’ {t.get("verb","")} â†’ {t.get("object","")}')
        parts.append("\n".join(svo_lines))

    entity_gaps = pre_batch.get("_entity_gaps") or []
    if entity_gaps:
        high_gaps = [g for g in entity_gaps if g.get("priority") == "high"]
        if high_gaps:
            gap_lines = ["â•â•â• LUKI ENCYJNE â•â•â•"]
            for g in high_gaps[:5]:
                reason = f" â€” {g['why']}" if g.get("why") else ""
                gap_lines.append(f'  ğŸ”´ "{g["entity"]}"{reason}')
            parts.append("\n".join(gap_lines))

    return "\n\n".join(parts) if parts else ""


def _fmt_natural_polish(pre_batch):
    """Anti-stuffing + fleksja â€” consolidated v2.2.
    Removed per-keyword spacing table (LLM can't count words).
    Kept: fleksja, anaphora, FAQ rotation, stuffing test.
    """
    parts = ["â•â•â• ANTY-STUFFING â•â•â•"]

    parts.append(
        "FLEKSJA: Odmiany = jedno uÅ¼ycie. Max 2Ã— ta sama fraza w jednym akapicie.\n"
        "RozkÅ‚adaj frazy RÃ“WNOMIERNIE po tekÅ›cie â€” nie skupiaj w jednym akapicie."
    )

    # Dynamic anaphora ban for main entity
    _raw_main = pre_batch.get("main_keyword") or {}
    _main_name = _raw_main.get("keyword", "") if isinstance(_raw_main, dict) else str(_raw_main)
    if _main_name:
        _entity_seo = (pre_batch.get("s1_data") or {}).get("entity_seo") or pre_batch.get("entity_seo") or {}
        _dynamic_synonyms = _entity_seo.get("entity_synonyms", [])
        if _dynamic_synonyms and len(_dynamic_synonyms) >= 2:
            synonyms = ", ".join(str(s) for s in _dynamic_synonyms[:5])
        else:
            synonyms = "konkretny podmiot z kontekstu (inwestor, ekipa, wykonawca, sÄ…d)"
        parts.append(f"ANTY-ANAPHORA [{_main_name}] MAX 2 ZDANIA Z RZÄ˜DU â†’ zmieÅ„ na: {synonyms}")

    parts.append(
        "FAQ: kaÅ¼de pytanie zaczynaj INNYM sÅ‚owem (Czy, Kiedy, Jak, Co, Ile, Dlaczego).\n"
        "TEST STUFFINGU: usuniÄ™cie frazy NIE zmienia sensu = stuffing â†’ usuÅ„ powtÃ³rzenie."
    )

    return "\n".join(parts)


def _fmt_serp_enrichment(pre_batch):
    """Old SERP enrichment â€” used by category prompt."""
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}
    paa = serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or []
    lsi = serp.get("lsi_keywords") or []
    chips = serp.get("refinement_chips") or []
    if not paa and not lsi and not chips:
        return ""
    parts = ["â•â•â• WZBOGACENIE Z SERP â•â•â•"]
    if chips:
        parts.append(f"Refinement Chips: {', '.join(str(c) for c in chips[:8])}")
    if paa:
        parts.append("PAA:")
        for q in paa[:5]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                parts.append(f'  â“ {q_text}')
    if lsi:
        lsi_names = [l.get("keyword", l) if isinstance(l, dict) else l for l in lsi[:8]]
        parts.append(f'LSI: {", ".join(str(n) for n in lsi_names)}')
    return "\n".join(parts) if len(parts) > 1 else ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FAQ PROMPT BUILDER (unchanged)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_faq_system_prompt(pre_batch=None):
    base = (
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO. "
        "Piszesz sekcjÄ™ FAQ: zwiÄ™zÅ‚e, konkretne odpowiedzi. "
        "KaÅ¼da odpowiedÅº ma szansÄ™ trafiÄ‡ do Google Featured Snippet."
    )
    gpt_instructions = ""
    if pre_batch:
        gpt_instructions = pre_batch.get("gpt_instructions_v39", "")
    if gpt_instructions:
        return base + "\n\n" + gpt_instructions
    return base


def build_faq_user_prompt(paa_data, pre_batch=None):
    if isinstance(paa_data, list):
        paa_data = {"serp_paa": paa_data}
    elif not isinstance(paa_data, dict):
        paa_data = {}
    paa_questions = paa_data.get("serp_paa") or []
    unused = paa_data.get("unused_keywords") or {}
    avoid = paa_data.get("avoid_in_faq") or []
    if isinstance(avoid, dict):
        avoid = avoid.get("topics") or []
    elif isinstance(avoid, str):
        avoid = [avoid] if avoid.strip() else []
    elif not isinstance(avoid, list):
        avoid = []
    instructions_raw = paa_data.get("instructions", "")
    if isinstance(instructions_raw, dict):
        instr_parts = []
        for k, v in instructions_raw.items():
            if isinstance(v, str):
                instr_parts.append(f"â€¢ {v}")
            elif isinstance(v, dict):
                for sk, sv in v.items():
                    if isinstance(sv, str):
                        instr_parts.append(f"â€¢ {sk}: {sv}")
        instructions = "\n".join(instr_parts)
    elif isinstance(instructions_raw, str):
        instructions = instructions_raw
    else:
        instructions = ""

    enhanced_paa = []
    if pre_batch:
        enhanced = pre_batch.get("enhanced") or {}
        if not isinstance(enhanced, dict):
            enhanced = {}
        enhanced_paa = enhanced.get("paa_from_serp") or []
        if not isinstance(enhanced_paa, list):
            enhanced_paa = []

    keyword_limits = {}
    if pre_batch:
        keyword_limits = pre_batch.get("keyword_limits") or {}
        if not isinstance(keyword_limits, dict):
            keyword_limits = {}
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_names = [s.get("keyword", s) if isinstance(s, dict) else s for s in stop_raw]

    style = {}
    if pre_batch:
        style = pre_batch.get("style_instructions") or {}

    sections = []
    sections.append("â•â•â• SEKCJA FAQ â•â•â•\nNapisz sekcjÄ™ FAQ. Zaczynaj od:\nh2: NajczÄ™Å›ciej zadawane pytania")

    all_paa = list(dict.fromkeys(paa_questions + enhanced_paa))
    if all_paa:
        sections.append("Pytania z Google (PAA):")
        for i, q in enumerate(all_paa[:8], 1):
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text and q_text.strip():
                sections.append(f'  {i}. {q_text}')
        sections.append("Wybierz 4-6 najlepszych.")

    if unused:
        if isinstance(unused, dict):
            unused_list = []
            for cat, items in unused.items():
                if isinstance(items, list):
                    unused_list.extend(items[:5])
                elif isinstance(items, str):
                    unused_list.append(items)
            if unused_list:
                names = ", ".join(f'"{u}"' if isinstance(u, str) else f'"{u.get("keyword", "")}"' for u in unused_list[:8])
                sections.append(f'\nFrazy nieuÅ¼yte: {names}')
        elif isinstance(unused, list):
            names = ", ".join(f'"{u}"' for u in unused[:8])
            sections.append(f'\nFrazy nieuÅ¼yte: {names}')

    if avoid:
        topics = ", ".join(f'"{a}"' if isinstance(a, str) else f'"{a.get("topic", "")}"' for a in avoid[:8])
        sections.append(f'\nNIE powtarzaj: {topics}')

    if stop_names:
        sections.append(f'\nğŸ›‘ STOP: {", ".join(f"{s}" for s in stop_names[:5])}')

    if style:
        forbidden = style.get("forbidden_phrases") or []
        if forbidden:
            sections.append(f'ZAKAZANE: {", ".join(forbidden[:5])}')

    if pre_batch and pre_batch.get("article_memory"):
        mem = pre_batch["article_memory"]
        if isinstance(mem, dict):
            topics = mem.get("topics_covered") or []
            if topics:
                topic_names = [t if isinstance(t, str) else t.get("topic", "") for t in topics[:6]]
                sections.append(f'\nTematy z artykuÅ‚u: {", ".join(topic_names)}')

    if instructions:
        sections.append(f'\n{instructions}')

    sections.append("""
â•â•â• FORMAT â•â•â•
h2: NajczÄ™Å›ciej zadawane pytania

h3: [Pytanie, 5-10 sÅ‚Ã³w]
[OdpowiedÅº 60-120 sÅ‚Ã³w]
â†’ Zdanie 1: BEZPOÅšREDNIA odpowiedÅº
â†’ Zdanie 2-3: rozwiniÄ™cie
â†’ Zdanie 4: praktyczna wskazÃ³wka

Zero markdown (**, __, #). Zero tagÃ³w HTML (<h3>, <b>, <strong>).
KaÅ¼dy h3: na OSOBNEJ linii z pustÄ… liniÄ… powyÅ¼ej.
Napisz 4-6 pytaÅ„. TYLKO treÅ›Ä‡.""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# H2 PLAN PROMPT BUILDER (unchanged)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_h2_plan_system_prompt():
    return (
        "JesteÅ› ekspertem SEO z 10-letnim doÅ›wiadczeniem w planowaniu architektury treÅ›ci. "
        "Tworzysz logiczne, wyczerpujÄ…ce struktury nagÅ‚Ã³wkÃ³w H2."
    )


def build_h2_plan_user_prompt(main_keyword, mode, s1_data, all_user_phrases, user_h2_hints=None):
    s1_data = s1_data or {}
    competitor_h2 = s1_data.get("competitor_h2_patterns") or []
    suggested_h2s = (s1_data.get("content_gaps") or {}).get("suggested_new_h2s", [])
    content_gaps = s1_data.get("content_gaps") or {}
    causal_triplets = s1_data.get("causal_triplets") or {}
    paa = s1_data.get("paa") or s1_data.get("paa_questions") or []
    serp_analysis = s1_data.get("serp_analysis") or {}
    related_searches = s1_data.get("related_searches") or serp_analysis.get("related_searches") or []

    sections = []
    mode_desc = "standard = peÅ‚ny artykuÅ‚" if mode == "standard" else "fast = krÃ³tki, max 3 sekcje"
    sections.append(f"HASÅO GÅÃ“WNE: {main_keyword}\nTRYB: {mode} ({mode_desc})")

    if competitor_h2:
        def _h2_count(h):
            return h.get("count", h.get("sources", 0)) if isinstance(h, dict) else 0
        sorted_h2 = sorted(competitor_h2[:30], key=_h2_count, reverse=True)
        lines = ["â•â•â• WZORCE H2 KONKURENCJI â€” posortowane po popularnoÅ›ci â•â•â•",
                 "Liczba przy H2 = ilu konkurentÃ³w uÅ¼ywa tego tematu.",
                 "H2 z wysokÄ… liczbÄ… = MUST HAVE w Twoim artykule (uÅ¼ytkownicy tego szukajÄ…)."]
        for i, h in enumerate(sorted_h2[:20], 1):
            if isinstance(h, dict):
                pattern = h.get("text", h.get("pattern", h.get("h2", str(h))))
                count = _h2_count(h)
                bar = "â–ˆ" * min(count, 8)
                lines.append(f"  {i:2}. [{bar:<8}] {count}Ã— â€” {pattern}")
            elif isinstance(h, str):
                lines.append(f"  {i:2}. {h}")
        sections.append("\n".join(lines))

    if suggested_h2s:
        lines = ["â•â•â• SUGEROWANE NOWE H2 (luki, tego NIKT z konkurencji nie pokrywa) â•â•â•"]
        for h in suggested_h2s[:10]:
            h_text = h if isinstance(h, str) else h.get("h2", h.get("title", str(h)))
            lines.append(f"  â€¢ {h_text}")
        sections.append("\n".join(lines))

    gap_priority_map = {
        "paa_unanswered": ("ğŸ”´ HIGH", "PAA bez odpowiedzi"),
        "depth_missing": ("ğŸŸ¡ MED-HIGH", "Brak gÅ‚Ä™bi"),
        "subtopic_missing": ("ğŸŸ¢ MED", "BrakujÄ…cy podtemat"),
        "gaps": ("", "Luka"),
    }
    all_gaps = []
    for key in ("paa_unanswered", "depth_missing", "subtopic_missing", "gaps"):
        priority, label = gap_priority_map.get(key, ("", ""))
        items = content_gaps.get(key) or []
        for item in items[:5]:
            gap_text = item if isinstance(item, str) else item.get("gap", item.get("topic", str(item)))
            if gap_text and gap_text not in [g[0] for g in all_gaps]:
                all_gaps.append((gap_text, priority, label))
    if all_gaps:
        lines = ["â•â•â• LUKI TREÅšCIOWE (tematy do pokrycia, priorytet od najwyÅ¼szego) â•â•â•"]
        for gap_text, priority, label in all_gaps[:10]:
            prefix = f"[{priority}] " if priority else ""
            lines.append(f"  â€¢ {prefix}{gap_text}")
        sections.append("\n".join(lines))

    if paa:
        lines = ["â•â•â• PAA â•â•â•"]
        for q in paa[:8]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                lines.append(f"  â“ {q_text}")
        sections.append("\n".join(lines))

    if related_searches:
        rs_texts = []
        for rs in related_searches[:12]:
            rs_t = rs if isinstance(rs, str) else (rs.get("query", "") or rs.get("text", ""))
            if rs_t:
                rs_texts.append(rs_t)
        if rs_texts:
            lines = ["â•â•â• RELATED SEARCHES (Google podpowiada po main_keyword) â•â•â•",
                     "UÅ¼yj tych fraz jako wskazÃ³wek tematycznych przy tworzeniu H2.",
                     "Wiele z nich to podtematy ktÃ³rych BRAK u konkurencji â€” Twoja szansa:"]
            for rs_t in rs_texts:
                lines.append(f"  ğŸ” {rs_t}")
            sections.append("\n".join(lines))

    triplet_list = (causal_triplets.get("chains") or causal_triplets.get("singles")
                    or causal_triplets.get("triplets") or [])[:8]
    if triplet_list:
        lines = ["â•â•â• PRZYCZYNOWE ZALEÅ»NOÅšCI (causeâ†’effect z konkurencji) â•â•â•",
                 "Confidence: ğŸ”´ â‰¥0.9 UÅ»YJ | ğŸŸ¡ â‰¥0.6 gdy pasuje | ğŸŸ¢ <0.6 opcjonalnie",
                 "is_chain=True (Aâ†’Bâ†’C) = najcenniejsze. Buduj logiczny przepÅ‚yw"]
        for t in triplet_list:
            if isinstance(t, dict):
                cause = t.get("cause", t.get("subject", ""))
                effect = t.get("effect", t.get("object", ""))
                conf = t.get("confidence", 0)
                is_chain = t.get("is_chain", False)
                ind = "ğŸ”´" if conf >= 0.9 else ("ğŸŸ¡" if conf >= 0.6 else "ğŸŸ¢")
                chain_tag = " [CHAIN]" if is_chain else ""
                lines.append(f"  {ind} {cause} â†’ {effect}{chain_tag}")
            elif isinstance(t, str):
                lines.append(f"  â€¢ {t}")
        sections.append("\n".join(lines))

    if user_h2_hints:
        h2_hints_list = "\n".join(f'  â€¢ "{h}"' for h in user_h2_hints[:10])
        sections.append(f"""â•â•â• FRAZY H2 UÅ»YTKOWNIKA â•â•â•

UÅ¼ytkownik podaÅ‚ te frazy z myÅ›lÄ… o nagÅ‚Ã³wkach H2.
Wykorzystaj je w nagÅ‚Ã³wkach tam, gdzie brzmiÄ… naturalnie po polsku.
Nie musisz uÅ¼yÄ‡ kaÅ¼dej, ale nie ignoruj ich. Dopasuj z wyczuciem.

FRAZY H2:
{h2_hints_list}""")

    if all_user_phrases:
        phrases_text = ", ".join(f'"{p}"' for p in all_user_phrases[:15])
        sections.append(f"""â•â•â• KONTEKST TEMATYCZNY (frazy BASIC/EXTENDED) â•â•â•

PoniÅ¼sze frazy bÄ™dÄ… uÅ¼yte W TREÅšCI artykuÅ‚u (nie w nagÅ‚Ã³wkach).
Zaplanuj H2 tak, by kaÅ¼da fraza miaÅ‚a naturalnÄ… sekcjÄ™:

{phrases_text}""")

    # H2 scaling â€” driven by target length, not arbitrary thresholds
    length_analysis = s1_data.get("length_analysis") or {}
    rec_length = length_analysis.get("recommended") or s1_data.get("recommended_length") or 0
    median_length = length_analysis.get("median") or s1_data.get("median_length") or 0

    if mode == "fast":
        fast_note = "Tryb fast: DOKÅADNIE 3 sekcje + FAQ."
    else:
        target = rec_length or (median_length * 2) or 1500
        # ~250 words per H2 section + intro â†’ derive count from length
        _raw_h2 = max(3, min(12, target // 250))
        h2_min = max(3, _raw_h2 - 1)
        h2_max = _raw_h2 + 1
        h2_range = f"{h2_min}-{h2_max}"
        fast_note = f"Tryb standard: {h2_range} sekcji + FAQ. Max {h2_max + 1} H2 Å‚Ä…cznie."

    h2_hint_rule = ("UwzglÄ™dnij frazy H2 uÅ¼ytkownika." if user_h2_hints
                    else "Dobierz nagÅ‚Ã³wki na podstawie S1 i luk.")

    sections.append(f"""â•â•â• ZASADY â•â•â•
1. LICZBA H2: {fast_note}
2. OSTATNI H2: "NajczÄ™Å›ciej zadawane pytania"
3. Pokryj wzorce konkurencji + luki
4. {h2_hint_rule}
5. Logiczna narracja
6. NIE powtarzaj hasÅ‚a gÅ‚Ã³wnego w kaÅ¼dym H2
7. Naturalna polszczyzna

â•â•â• FORMAT â•â•â•
JSON array: ["H2 pierwszy", ..., "NajczÄ™Å›ciej zadawane pytania"]""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATEGORY PROMPT BUILDERS (unchanged)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_category_system_prompt(pre_batch, batch_type, category_data=None):
    pre_batch = pre_batch or {}
    category_data = category_data or {}
    parts = []

    store_name = category_data.get("store_name") or "sklep"
    store_desc = category_data.get("store_description") or ""
    brand_voice = category_data.get("brand_voice") or ""

    store_ctx = f" dla {store_name}" if store_name != "sklep" else ""
    store_desc_line = f"\n{store_desc}" if store_desc else ""
    parts.append(f"""<role>
JesteÅ› doÅ›wiadczonym copywriterem e-commerce{store_ctx}{store_desc_line}.
Specjalizujesz siÄ™ w opisach kategorii sklepÃ³w internetowych.
Nie jesteÅ› blogerem â€” piszesz tekst sprzedaÅ¼owy.
</role>""")

    parts.append("""<goal>
Opis kategorii e-commerce, ktÃ³ry:
  â€¢ wspiera intencjÄ™ transakcyjnÄ…,
  â€¢ naturalnie zawiera sÅ‚owa kluczowe (gÄ™stoÅ›Ä‡ 1,0â€“2,0%),
  â€¢ buduje entity salience >0,30,
  â€¢ uÅ¼ywa konkretnych nazw produktÃ³w, cen, cech,
  â€¢ pomaga kupujÄ…cemu podjÄ…Ä‡ decyzjÄ™.
80% transakcyjnych, 20% informacyjnych.
</goal>""")

    target = category_data.get("target_audience") or ""
    target_line = f"\nGrupa docelowa: {target}" if target else ""
    parts.append(f"""<audience>
KupujÄ…cy z intencjÄ… zakupowÄ….{target_line}
</audience>""")

    voice_line = f"\nBrand voice: {brand_voice}" if brand_voice else ""
    parts.append(f"""<tone>
Ton: autorytatywny, pomocny, zwiÄ™zÅ‚y.{voice_line}
Unikaj: â€szeroki wybÃ³r", â€coÅ› dla kaÅ¼dego", â€nie szukaj dalej".
</tone>""")

    parts.append("""<epistemology>
Å¹RÃ“DÅA: dane wejÅ›ciowe, konkurencja z SERP, wiedza produktowa.
âŒ ZAKAZ: nie wymyÅ›laj produktÃ³w, cen, recenzji, certyfikatÃ³w.
</epistemology>""")

    cat_type = category_data.get("category_type", "subcategory")
    if cat_type == "parent":
        struct_desc = """KATEGORIA NADRZÄ˜DNA (200â€“500 sÅ‚Ã³w):
  Blok 1 â€” INTRO (50â€“100 sÅ‚Ã³w): keyword + opis + USP + linki podkategorii
  Blok 2 â€” SEO (100â€“300 sÅ‚Ã³w): 1â€“2 H2, przeglÄ…d, dlaczego u nas
  Blok 3 â€” FAQ (2â€“3 pytania)"""
    else:
        struct_desc = """PODKATEGORIA (500â€“1200 sÅ‚Ã³w):
  Blok 1 â€” INTRO (50â€“150 sÅ‚Ã³w): keyword + opis + USP
  Blok 2 â€” SEO (400â€“800 sÅ‚Ã³w): 2â€“4 H2 (jak wybraÄ‡, rodzaje, dlaczego u nas)
  Blok 3 â€” FAQ (3â€“6 pytaÅ„)"""

    parts.append(f"<category_structure>\n{struct_desc}\n</category_structure>")

    parts.append("""<rules>
KEYWORD DENSITY: 1,0â€“2,0%.
ENTITY SALIENCE: cel >0,30. Entity-rich: typy, materiaÅ‚y, technologie, marki.
PASSAGE-FIRST: intro = standalone summary.
LISTY HTML: 3+ elementÃ³w â†’ lista.
SPACING: MAIN ~60 sÅ‚Ã³w, BASIC ~80, EXTENDED ~120.
ANTI-AI: zakaz fraz kliszowych.
LINKI: 3â€“8 kontekstowych na 300â€“500 sÅ‚Ã³w.
FORMAT: h2:/h3:. Zero markdown (**, __, #). Zero tagÃ³w HTML (<h2>, <h3>).
  KaÅ¼dy h2:/h3: na OSOBNEJ linii z pustÄ… liniÄ… powyÅ¼ej.
</rules>""")

    parts.append("""<examples>
PRZYKÅAD DOBRY:
<example_good>
Damskie buty do biegania od Nike, ASICS i Brooks â€” od 299 do 1 199 zÅ‚.
Bestseller sezonu: Nike Air Zoom Pegasus 41 (4,7â˜…, 312 recenzji)
Å‚Ä…czy responsywnÄ… piankÄ™ React z siateczkÄ… Flyknit.
Darmowy zwrot 30 dni, wysyÅ‚ka w 24h.
</example_good>
</examples>""")

    return "\n\n".join(parts)


def build_category_user_prompt(pre_batch, h2, batch_type, article_memory=None, category_data=None):
    pre_batch = pre_batch or {}
    category_data = category_data or {}
    sections = []

    sections.append(
        "Piszesz opis kategorii e-commerce â€” ton pomocny, "
        "konkretny, wspierajÄ…cy decyzjÄ™ zakupowÄ…. "
        "Zasady w system prompcie."
    )

    # Opening pattern rotation for category (commercial variants)
    _CAT_PATTERNS = [
        ("A", "KONKRET PRODUKTOWY",
         "Zacznij od konkretnego produktu, ceny lub cechy. "
         "Np: 'Nike Pegasus 41 od 549 zÅ‚ â€” bestseller z 312 recenzjami...'"),
        ("B", "ZAKRES/STATYSTYKA",
         "Zacznij od zakresu, liczby lub faktu. "
         "Np: 'Ponad 200 modeli butÃ³w do biegania od 15 marek...'"),
        ("C", "POTRZEBA KUPUJÄ„CEGO",
         "Zacznij od potrzeby klienta. "
         "Np: 'Szukasz buta na maraton z amortyzacjÄ… na twardym podÅ‚oÅ¼u?'"),
        ("D", "USP/WYRÃ“Å»NIK",
         "Zacznij od przewagi sklepu. "
         "Np: 'Darmowy zwrot 30 dni i dobÃ³r rozmiaru z ekspertem...'"),
    ]
    batch_num = pre_batch.get("batch_number", 1) or 1
    pattern_idx = (batch_num - 1) % len(_CAT_PATTERNS)
    p_letter, p_name, p_desc = _CAT_PATTERNS[pattern_idx]
    sections.append(
        f"OTWARCIE â€” wzorzec {p_letter} ({p_name}):\n{p_desc}"
    )

    # Category context
    cat_ctx_parts = []
    cat_name = category_data.get("category_name") or pre_batch.get("main_keyword", "")
    if isinstance(cat_name, dict):
        cat_name = cat_name.get("keyword", "")
    cat_type = category_data.get("category_type", "subcategory")
    hierarchy = category_data.get("hierarchy") or ""
    store_name = category_data.get("store_name") or ""
    usp = category_data.get("usp") or ""
    products = category_data.get("products") or ""
    bestseller = category_data.get("bestseller") or ""
    price_range = category_data.get("price_range") or ""

    cat_ctx_parts.append(f"Kategoria: {cat_name}")
    cat_ctx_parts.append(f"Typ: {'nadrzÄ™dna' if cat_type == 'parent' else 'podkategoria'}")
    if hierarchy: cat_ctx_parts.append(f"Hierarchia: {hierarchy}")
    if store_name: cat_ctx_parts.append(f"Sklep: {store_name}")
    if usp: cat_ctx_parts.append(f"USP: {usp}")
    if products: cat_ctx_parts.append(f"Produkty:\n{products}")
    if bestseller: cat_ctx_parts.append(f"Bestseller: {bestseller}")
    if price_range: cat_ctx_parts.append(f"Ceny: {price_range}")
    sections.append("â•â•â• DANE KATEGORII â•â•â•\n" + "\n".join(cat_ctx_parts))

    _schema_guard(pre_batch)

    formatters = [
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_keywords(pre_batch),
        lambda: _fmt_smart_instructions(pre_batch),
        lambda: _fmt_semantic_plan(pre_batch, h2),
        lambda: _fmt_coverage_density(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_h2_remaining(pre_batch),
        lambda: _fmt_entity_salience(pre_batch),
        lambda: _fmt_serp_enrichment(pre_batch),
        lambda: _fmt_natural_polish(pre_batch),
        lambda: _fmt_style(pre_batch),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception:
            pass

    return "\n\n".join(sections)
