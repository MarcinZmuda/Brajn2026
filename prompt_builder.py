"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BRAJEN PROMPT BUILDER v1.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Converts raw pre_batch data into optimized, readable prompts.

v1.1 changes:
  - _fmt_keywords(): calculates remaining from actual + target_total
    (backend sends these but NOT remaining directly)
  - Shows hard_max_this_batch so Claude knows per-batch limits
  - Clearer MUST/EXTENDED/STOP formatting

Architecture:
  SYSTEM PROMPT = Expert persona + Writing techniques
  USER PROMPT   = Structured instructions from data
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_system_prompt(pre_batch, batch_type):
    """
    Build system prompt = expert persona + writing rules.
    Uses gpt_instructions_v39 (writing techniques from API) as the core,
    with a proper persona wrapper.
    """
    pre_batch = pre_batch or {}
    gpt_instructions = pre_batch.get("gpt_instructions_v39", "")
    gpt_prompt = pre_batch.get("gpt_prompt", "")

    parts = []

    # â”€â”€ Expert persona â”€â”€
    parts.append(
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO z 10-letnim staÅ¼em. "
        "Piszesz naturalnie, merytorycznie i angaÅ¼ujÄ…co. "
        "TwÃ³j tekst nie brzmi jak AI â€” brzmi jak ekspert piszÄ…cy dla ludzi."
    )

    # â”€â”€ Writing techniques from API (if available) â”€â”€
    if gpt_instructions:
        parts.append(gpt_instructions)

    # â”€â”€ Batch context from API (structure, lengths) â”€â”€
    if gpt_prompt:
        parts.append(gpt_prompt)

    # â”€â”€ Core rules (always) â”€â”€
    parts.append("""ZASADY PISANIA:
â€¢ PASSAGE-FIRST: KaÅ¼dy akapit zaczynaj od konkretnej odpowiedzi, potem rozwijaj.
â€¢ BURSTINESS: Mieszaj dÅ‚ugoÅ›Ä‡ zdaÅ„ â€” krÃ³tkie (8 sÅ‚Ã³w) z dÅ‚uÅ¼szymi (20-25 sÅ‚Ã³w).
â€¢ ANTI-AI: Unikaj fraz-klisz: "warto zauwaÅ¼yÄ‡", "naleÅ¼y podkreÅ›liÄ‡", "w dzisiejszych czasach", "kluczowe jest", "nie ulega wÄ…tpliwoÅ›ci". Brzmi to sztucznie.
â€¢ NATURALNOÅšÄ†: Pisz jak ekspert tÅ‚umaczÄ…cy temat znajomemu â€” konkretnie, bez lania wody.
â€¢ FORMAT: UÅ¼ywaj wyÅ‚Ä…cznie formatu h2:/h3: dla nagÅ‚Ã³wkÃ³w. Å»adnego markdown, HTML ani gwiazdek.""")

    return "\n\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USER PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_user_prompt(pre_batch, h2, batch_type, article_memory=None):
    """
    Main user prompt builder.
    Converts ALL pre_batch fields into readable, actionable instructions.
    Each section is wrapped in try/except so one bad field won't crash generation.
    """
    pre_batch = pre_batch or {}
    sections = []

    formatters = [
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_intro_guidance(pre_batch, batch_type),
        lambda: _fmt_smart_instructions(pre_batch),
        lambda: _fmt_keywords(pre_batch),
        lambda: _fmt_semantic_plan(pre_batch, h2),
        lambda: _fmt_entities(pre_batch),
        lambda: _fmt_ngrams(pre_batch),
        lambda: _fmt_serp_enrichment(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_coverage_density(pre_batch),
        lambda: _fmt_style(pre_batch),
        lambda: _fmt_legal_medical(pre_batch),
        lambda: _fmt_experience_markers(pre_batch),
        lambda: _fmt_causal_context(pre_batch),
        lambda: _fmt_h2_remaining(pre_batch),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception:
            pass

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION FORMATTERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_batch_header(pre_batch, h2, batch_type):
    batch_number = pre_batch.get("batch_number", 1)
    total_batches = pre_batch.get("total_planned_batches", 1)
    batch_length = pre_batch.get("batch_length") or {}

    min_w = batch_length.get("min_words", 350)
    max_w = batch_length.get("max_words", 500)

    section_length = pre_batch.get("section_length_guidance") or {}
    length_hint = ""
    if section_length:
        suggested = section_length.get("suggested_words") or section_length.get("target_words")
        if suggested:
            length_hint = f"\nSugerowana dÅ‚ugoÅ›Ä‡ tej sekcji: ~{suggested} sÅ‚Ã³w."

    return f"""â•â•â• BATCH {batch_number}/{total_batches} â€” {batch_type} â•â•â•
Sekcja H2: "{h2}"
DÅ‚ugoÅ›Ä‡: {min_w}-{max_w} sÅ‚Ã³w{length_hint}
Zaczynaj DOKÅADNIE od: h2: {h2}"""


def _fmt_intro_guidance(pre_batch, batch_type):
    if batch_type not in ("INTRO", "intro"):
        return ""
    guidance = pre_batch.get("intro_guidance", "")
    if not guidance:
        return ""

    if isinstance(guidance, dict):
        hook = guidance.get("hook", "")
        angle = guidance.get("angle", "")
        parts = []
        if hook:
            parts.append(f"Hak otwierajÄ…cy: {hook}")
        if angle:
            parts.append(f"KÄ…t artykuÅ‚u: {angle}")
        return "â•â•â• WPROWADZENIE â•â•â•\n" + "\n".join(parts) if parts else ""

    return f"â•â•â• WPROWADZENIE â•â•â•\n{guidance}"


def _fmt_smart_instructions(pre_batch):
    """Smart instructions from enhanced_pre_batch â€” THE most valuable field."""
    enhanced = pre_batch.get("enhanced") or {}
    smart = enhanced.get("smart_instructions_formatted", "")
    if smart:
        return f"â•â•â• INSTRUKCJE DLA TEGO BATCHA â•â•â•\n{smart[:1000]}"
    return ""


def _parse_target_max(target_total_str):
    """
    Parse target_max from backend's target_total field.
    Backend sends target_total as "min-max" string (e.g., "2-6").
    Returns max value as int, or 0 if unparseable.
    """
    if not target_total_str:
        return 0
    if isinstance(target_total_str, (int, float)):
        return int(target_total_str)
    try:
        parts = str(target_total_str).replace("x", "").split("-")
        if len(parts) >= 2:
            return int(parts[-1].strip())
        return int(parts[0].strip())
    except (ValueError, IndexError):
        return 0


def _fmt_keywords(pre_batch):
    """
    Format keywords section with CALCULATED remaining_max.
    
    v1.1: Backend sends actual (current uses) and target_total ("min-max")
    but NOT remaining. We calculate: remaining = target_max - actual.
    Also shows hard_max_this_batch so Claude knows per-batch limits.
    """
    keywords_info = pre_batch.get("keywords") or {}
    keyword_limits = pre_batch.get("keyword_limits") or {}
    soft_caps = pre_batch.get("soft_cap_recommendations") or {}

    # â”€â”€ MUST USE (with calculated remaining) â”€â”€
    must_raw = keywords_info.get("basic_must_use", [])
    must_lines = []
    for kw in must_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            
            # Calculate remaining from actual + target_total
            actual = kw.get("actual", kw.get("actual_uses", kw.get("current_count", 0)))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            hard_max = kw.get("hard_max_this_batch", "")
            use_range = kw.get("use_this_batch", "")
            
            # Explicit remaining from backend (if sent), otherwise calculate
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            
            # Build descriptive line
            parts_line = [f'"{name}"']
            if remaining:
                parts_line.append(f"zostaÅ‚o {remaining}Ã— ogÃ³Å‚em")
            if hard_max:
                parts_line.append(f"max {hard_max}Ã— w tym batchu")
            elif use_range:
                parts_line.append(f"cel: {use_range}Ã— w tym batchu")
            
            must_lines.append(f'  â€¢ {" â€” ".join(parts_line)}')
        else:
            must_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ EXTENDED (with remaining) â”€â”€
    ext_raw = keywords_info.get("extended_this_batch", [])
    ext_lines = []
    for kw in ext_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            actual = kw.get("actual", kw.get("actual_uses", 0))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            
            line = f'  â€¢ "{name}"'
            if remaining:
                line += f" â€” zostaÅ‚o {remaining}Ã—"
            ext_lines.append(line)
        else:
            ext_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ STOP â”€â”€
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_lines = []
    for s in stop_raw:
        if isinstance(s, dict):
            name = s.get("keyword", "")
            current = s.get("current_count", s.get("current", s.get("actual", "?")))
            max_c = s.get("max_count", s.get("max", s.get("target_max", "?")))
            stop_lines.append(f'  â€¢ "{name}" (juÅ¼ {current}Ã—, limit {max_c}) â€” STOP!')
        else:
            stop_lines.append(f'  â€¢ "{s}"')

    # â”€â”€ CAUTION â”€â”€
    caution_raw = keyword_limits.get("caution_keywords") or []
    caution_lines = []
    for c in caution_raw:
        if isinstance(c, dict):
            name = c.get("keyword", "")
            current = c.get("current_count", c.get("current", c.get("actual", "")))
            max_c = c.get("max_count", c.get("max", c.get("target_max", "")))
            line = f'  â€¢ "{name}"'
            if current and max_c:
                line += f" ({current}/{max_c})"
            line += " â€” max 1Ã— w tym batchu"
            caution_lines.append(line)
        else:
            caution_lines.append(f'  â€¢ "{c}" â€” max 1Ã—')

    # â”€â”€ SOFT CAPS â”€â”€
    soft_notes = []
    if soft_caps:
        for kw_name, info in soft_caps.items():
            if isinstance(info, dict):
                action = info.get("action", "")
                if action and action != "OK":
                    soft_notes.append(f'  â„¹ï¸ "{kw_name}": {action}')

    # â”€â”€ Build section â”€â”€
    parts = ["â•â•â• FRAZY KLUCZOWE â•â•â•"]

    if must_lines:
        parts.append("ğŸ”´ OBOWIÄ„ZKOWE (wpleÄ‡ naturalnie w tekst):")
        parts.extend(must_lines)

    if ext_lines:
        parts.append("\nğŸŸ¡ ROZSZERZONE (uÅ¼yj jeÅ›li pasujÄ… do kontekstu):")
        parts.extend(ext_lines)

    if stop_lines:
        parts.append("\nğŸ›‘ STOP â€” NIE UÅ»YWAJ (przekroczone limity!):")
        parts.extend(stop_lines)

    if caution_lines:
        parts.append("\nâš ï¸ OSTROÅ»NIE â€” uÅ¼yj max 1Ã— lub pomiÅ„:")
        parts.extend(caution_lines)

    if soft_notes:
        parts.append("")
        parts.extend(soft_notes)

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_semantic_plan(pre_batch, h2):
    plan = pre_batch.get("semantic_batch_plan") or {}
    if not plan:
        return ""

    parts = ["â•â•â• CO PISAÄ† W TEJ SEKCJI â•â•â•"]

    h2_coverage = plan.get("h2_coverage") or {}
    for h2_name, info in h2_coverage.items():
        if isinstance(info, dict):
            angle = info.get("semantic_angle", "")
            must = info.get("must_phrases", [])
            if angle:
                parts.append(f'KÄ…t semantyczny: {angle}')
            if must:
                phrases = ", ".join(f'"{p}"' for p in must[:5])
                parts.append(f'ObowiÄ…zkowe frazy w tej sekcji: {phrases}')

    density_targets = plan.get("density_targets") or {}
    overall = density_targets.get("overall")
    if overall:
        parts.append(f'Docelowa gÄ™stoÅ›Ä‡ fraz: {overall}%')

    direction = plan.get("content_direction") or plan.get("writing_direction", "")
    if direction:
        parts.append(f'Kierunek treÅ›ci: {direction}')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_entities(pre_batch):
    entities_for_batch = pre_batch.get("entities_for_batch") or {}
    entity_seo = pre_batch.get("entity_seo") or {}
    enhanced = pre_batch.get("enhanced") or {}
    entities_to_define = enhanced.get("entities_to_define") or []
    relations = enhanced.get("relations_to_establish") or []

    if not entities_for_batch and not entity_seo.get("enabled") and not entities_to_define:
        return ""

    parts = ["â•â•â• ENCJE (budujÄ… autorytet tematyczny) â•â•â•"]

    introduce = entities_for_batch.get("introduce") or []
    if introduce:
        parts.append("WPROWADÅ¹ w tym batchu (pierwsza wzmianka):")
        for ent in introduce[:5]:
            if isinstance(ent, dict):
                name = ent.get("entity", ent.get("text", ""))
                etype = ent.get("type", "")
                context = ent.get("context", "")
                line = f'  â€¢ "{name}"'
                if etype:
                    line += f" ({etype})"
                if context:
                    line += f" â€” {context}"
                parts.append(line)
            else:
                parts.append(f'  â€¢ "{ent}"')

    if entities_to_define:
        parts.append("\nZDEFINIUJ (wyjaÅ›nij czytelnikowi):")
        for ent in entities_to_define[:5]:
            if isinstance(ent, dict):
                name = ent.get("entity", ent.get("text", ""))
                hint = ent.get("definition_hint", ent.get("hint", ""))
                line = f'  â€¢ "{name}"'
                if hint:
                    line += f" â€” {hint}"
                parts.append(line)
            else:
                parts.append(f'  â€¢ "{ent}"')

    maintain = entities_for_batch.get("maintain") or []
    if maintain:
        names = ", ".join(f'"{m}"' if isinstance(m, str) else f'"{m.get("entity", "")}"' for m in maintain[:5])
        parts.append(f"\nUTRZYMUJ (juÅ¼ wprowadzone wczeÅ›niej): {names}")

    if relations:
        parts.append("\nPOWIÄ„Å» ze sobÄ…:")
        for rel in relations[:4]:
            if isinstance(rel, dict):
                subj = rel.get("subject", "")
                verb = rel.get("verb", rel.get("relation", "â†’"))
                obj = rel.get("object", "")
                parts.append(f'  â€¢ {subj} {verb} {obj}')
            elif isinstance(rel, str):
                parts.append(f'  â€¢ {rel}')

    must_mention = entity_seo.get("must_mention") or []
    if must_mention and not introduce:
        parts.append("WSPOMNIJ w tekÅ›cie:")
        for ent in must_mention[:5]:
            if isinstance(ent, dict):
                name = ent.get("text", ent.get("entity", ""))
                parts.append(f'  â€¢ "{name}"')
            else:
                parts.append(f'  â€¢ "{ent}"')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_ngrams(pre_batch):
    ngrams = pre_batch.get("ngrams_for_batch") or []
    ngram_guidance = pre_batch.get("ngram_guidance") or {}

    if not ngrams and not ngram_guidance:
        return ""

    parts = ["â•â•â• POPULARNE FRAZY Z TOP10 (n-gramy) â•â•â•",
             "Te frazy czÄ™sto pojawiajÄ… siÄ™ u najlepszych wynikÃ³w. WpleÄ‡ naturalnie:"]

    for ng in ngrams[:10]:
        if isinstance(ng, dict):
            text = ng.get("ngram", ng.get("text", ""))
            count = ng.get("count", ng.get("frequency", ""))
            if text:
                parts.append(f'  â€¢ "{text}"' + (f" ({count}Ã— u konkurencji)" if count else ""))
        elif isinstance(ng, str):
            parts.append(f'  â€¢ "{ng}"')

    if ngram_guidance:
        overused = ngram_guidance.get("overused") or []
        if overused:
            over_list = ", ".join(f'"{o}"' if isinstance(o, str) else f'"{o.get("ngram", "")}"' for o in overused[:5])
            parts.append(f"\nâš ï¸ NaduÅ¼ywane n-gramy (uÅ¼yj zamiennikÃ³w): {over_list}")

        synonyms = ngram_guidance.get("suggested_synonyms") or ngram_guidance.get("synonyms") or {}
        if synonyms and isinstance(synonyms, dict):
            parts.append("Sugerowane zamienniki:")
            for orig, alts in list(synonyms.items())[:5]:
                if isinstance(alts, list):
                    parts.append(f'  â€¢ "{orig}" â†’ {", ".join(alts[:3])}')

    return "\n".join(parts) if len(parts) > 2 else ""


def _fmt_serp_enrichment(pre_batch):
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}

    paa = (serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or [])
    lsi = (serp.get("lsi_keywords") or [])

    if not paa and not lsi:
        return ""

    parts = ["â•â•â• WZBOGACENIE Z SERP â•â•â•"]

    if paa:
        parts.append("Pytania ktÃ³re ludzie zadajÄ… w Google (PAA) â€” odpowiedz na 1-2 w tekÅ›cie:")
        for q in paa[:5]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                parts.append(f'  â“ {q_text}')

    if lsi:
        lsi_names = [l.get("keyword", l) if isinstance(l, dict) else l for l in lsi[:8]]
        parts.append(f'\nFrazy LSI (bliskoznaczne, wpleÄ‡ naturalnie): {", ".join(lsi_names)}')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_continuation(pre_batch):
    continuation = pre_batch.get("continuation_v39") or {}
    enhanced = pre_batch.get("enhanced") or {}
    cont_ctx = enhanced.get("continuation_context") or {}

    last_h2 = cont_ctx.get("last_h2") or continuation.get("last_h2", "")
    last_ending = cont_ctx.get("last_paragraph_ending") or continuation.get("last_paragraph_ending", "")
    last_topic = cont_ctx.get("last_topic") or continuation.get("last_topic", "")
    transition_hint = continuation.get("transition_hint", "")

    if not last_h2 and not last_ending:
        return ""

    parts = ["â•â•â• KONTYNUACJA â•â•â•",
             "Poprzedni batch zakoÅ„czyÅ‚ siÄ™ na:"]

    if last_h2:
        parts.append(f'  Ostatni H2: "{last_h2}"')
    if last_ending:
        ending_preview = last_ending[:150] + ("..." if len(last_ending) > 150 else "")
        parts.append(f'  Ostatnie zdanie: "{ending_preview}"')
    if last_topic:
        parts.append(f'  Temat: {last_topic}')

    parts.append("\nZacznij PÅYNNIE â€” nawiÄ…Å¼ do poprzedniego wÄ…tku, ale nie powtarzaj zakoÅ„czenia.")
    if transition_hint:
        parts.append(f'Sugerowane przejÅ›cie: {transition_hint}')

    return "\n".join(parts)


def _fmt_article_memory(article_memory):
    if not article_memory:
        return ""

    parts = ["â•â•â• PAMIÄ˜Ä† ARTYKUÅU (nie powtarzaj!) â•â•â•"]

    if isinstance(article_memory, dict):
        topics = article_memory.get("topics_covered") or article_memory.get("covered_topics") or []
        if topics:
            parts.append("Tematy juÅ¼ omÃ³wione w artykule:")
            for t in topics[:10]:
                if isinstance(t, str):
                    parts.append(f'  âœ“ {t}')
                elif isinstance(t, dict):
                    parts.append(f'  âœ“ {t.get("topic", t.get("h2", ""))}')

        facts = article_memory.get("key_facts_used") or article_memory.get("facts", [])
        if facts:
            parts.append("\nFakty juÅ¼ uÅ¼yte (nie powtarzaj):")
            for f in facts[:8]:
                parts.append(f'  â€¢ {f}' if isinstance(f, str) else f'  â€¢ {json.dumps(f, ensure_ascii=False)[:100]}')

        phrases_used = article_memory.get("phrases_used") or {}
        if phrases_used:
            high_use = [(k, v) for k, v in phrases_used.items()
                        if isinstance(v, (int, float)) and v >= 3]
            if high_use:
                parts.append("\nFrazy juÅ¼ czÄ™sto uÅ¼yte (ogranicz):")
                for name, count in high_use[:8]:
                    parts.append(f'  â€¢ "{name}" â€” juÅ¼ {count}Ã—')
    elif isinstance(article_memory, str):
        parts.append(article_memory[:1500])

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_coverage_density(pre_batch):
    coverage = pre_batch.get("coverage") or {}
    density = pre_batch.get("density") or {}
    main_kw = pre_batch.get("main_keyword") or {}
    keyword_tracking = pre_batch.get("keyword_tracking") or {}

    if not coverage and not density and not main_kw:
        return ""

    parts = ["â•â•â• STATUS POKRYCIA FRAZ â•â•â•"]

    if main_kw:
        kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
        synonyms = main_kw.get("synonyms", []) if isinstance(main_kw, dict) else []
        if kw_name:
            parts.append(f'HasÅ‚o gÅ‚Ã³wne: "{kw_name}"')
        if synonyms:
            parts.append(f'Synonimy (uÅ¼ywaj zamiennie): {", ".join(synonyms[:5])}')

    current_cov = coverage.get("current", coverage.get("current_coverage"))
    target_cov = coverage.get("target", coverage.get("target_coverage"))
    if current_cov is not None and target_cov is not None:
        parts.append(f'\nPokrycie fraz: {current_cov}% z docelowych {target_cov}%')

    missing = coverage.get("missing_phrases") or coverage.get("uncovered") or []
    if missing:
        parts.append("âš ï¸ BRAKUJÄ„CE FRAZY â€” wpleÄ‡ w tym batchu:")
        for m in missing[:8]:
            name = m.get("keyword", m) if isinstance(m, dict) else m
            parts.append(f'  â†’ "{name}"')

    if density:
        current_d = density.get("current")
        target_range = density.get("target_range") or []
        if current_d is not None:
            range_str = f'{target_range[0]}-{target_range[1]}%' if len(target_range) >= 2 else "1.5-2.5%"
            status = "âœ… w normie" if target_range and len(target_range) >= 2 and target_range[0] <= current_d <= target_range[1] else "âš ï¸ do korekty"
            parts.append(f'\nGÄ™stoÅ›Ä‡ fraz: {current_d}% (cel: {range_str}) {status}')

        overused_d = density.get("overused") or []
        if overused_d:
            over_names = ", ".join(f'"{o}"' if isinstance(o, str) else f'"{o.get("keyword", "")}"' for o in overused_d[:5])
            parts.append(f'NaduÅ¼ywane: {over_names} â€” uÅ¼yj synonimÃ³w')

    if keyword_tracking:
        total_kw = keyword_tracking.get("total_keywords", 0)
        covered_kw = keyword_tracking.get("covered", 0)
        if total_kw and covered_kw:
            parts.append(f'\nTracking: {covered_kw}/{total_kw} fraz pokrytych')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_style(pre_batch):
    style = pre_batch.get("style_instructions") or pre_batch.get("style_instructions_v39") or {}

    if not style:
        return ""

    parts = ["â•â•â• STYL â•â•â•"]

    if isinstance(style, dict):
        tone = style.get("tone", "")
        if tone:
            parts.append(f'Ton: {tone}')

        para_len = style.get("paragraph_length", "")
        if para_len:
            parts.append(f'DÅ‚ugoÅ›Ä‡ akapitÃ³w: {para_len} sÅ‚Ã³w')

        forbidden = style.get("forbidden_phrases") or style.get("avoid_phrases") or []
        if forbidden:
            parts.append(f'ZAKAZANE zwroty: {", ".join(f"{f}" for f in forbidden[:8])}')

        preferred = style.get("preferred_phrases") or style.get("use_phrases") or []
        if preferred:
            parts.append(f'Preferowane zwroty: {", ".join(preferred[:5])}')

        persona = style.get("persona", "")
        if persona:
            parts.append(f'Perspektywa: {persona}')
    elif isinstance(style, str):
        parts.append(style[:500])

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_legal_medical(pre_batch):
    legal_ctx = pre_batch.get("legal_context") or {}
    medical_ctx = pre_batch.get("medical_context") or {}

    parts = []

    if legal_ctx and legal_ctx.get("active"):
        parts.append("â•â•â• KONTEKST PRAWNY (YMYL) â•â•â•")
        parts.append("Ten artykuÅ‚ dotyczy tematyki prawnej. MUSISZ:")
        parts.append("  1. CytowaÄ‡ realne przepisy i orzeczenia (podane niÅ¼ej)")
        parts.append("  2. DodaÄ‡ disclaimer o konsultacji z prawnikiem")
        parts.append("  3. NIE wymyÅ›laÄ‡ sygnatur ani dat orzeczeÅ„")

        instruction = legal_ctx.get("legal_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        judgments = legal_ctx.get("top_judgments") or []
        if judgments:
            parts.append("\nOrzeczenia do zacytowania:")
            for j in judgments[:3]:
                if isinstance(j, dict):
                    sig = j.get("signature", j.get("caseNumber", ""))
                    court = j.get("court", j.get("courtName", ""))
                    date = j.get("date", j.get("judgmentDate", ""))
                    parts.append(f'  â€¢ {sig} â€” {court} ({date})')

        citation_hint = legal_ctx.get("citation_hint", "")
        if citation_hint:
            parts.append(f'\n{citation_hint}')

    if medical_ctx and medical_ctx.get("active"):
        if parts:
            parts.append("")
        parts.append("â•â•â• KONTEKST MEDYCZNY (YMYL) â•â•â•")
        parts.append("Ten artykuÅ‚ dotyczy tematyki zdrowotnej. MUSISZ:")
        parts.append("  1. CytowaÄ‡ ÅºrÃ³dÅ‚a naukowe (podane niÅ¼ej)")
        parts.append("  2. NIE wymyÅ›laÄ‡ statystyk ani nazw badaÅ„")
        parts.append("  3. DodaÄ‡ informacjÄ™ o konsultacji z lekarzem")

        instruction = medical_ctx.get("medical_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        publications = medical_ctx.get("top_publications") or []
        if publications:
            parts.append("\nPublikacje do zacytowania:")
            for p in publications[:5]:
                if isinstance(p, dict):
                    title = p.get("title", "")[:80]
                    authors = p.get("authors", "")[:40]
                    year = p.get("year", "")
                    pmid = p.get("pmid", "")
                    parts.append(f'  â€¢ {authors} ({year}): "{title}" PMID:{pmid}')

    return "\n".join(parts) if parts else ""


def _fmt_experience_markers(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    markers = enhanced.get("experience_markers") or []

    if not markers:
        return ""

    parts = ["â•â•â• SYGNAÅY DOÅšWIADCZENIA (E-E-A-T) â•â•â•",
             "WpleÄ‡ min 1 sygnaÅ‚, Å¼e autor MA doÅ›wiadczenie z tematem:"]

    for m in markers[:5]:
        if isinstance(m, str):
            parts.append(f'  â€¢ {m}')
        elif isinstance(m, dict):
            parts.append(f'  â€¢ {m.get("marker", m.get("text", ""))}')

    return "\n".join(parts)


def _fmt_causal_context(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    causal = enhanced.get("causal_context", "")
    info_gain = enhanced.get("information_gain", "")

    parts = []

    if causal:
        parts.append("â•â•â• KONTEKST PRZYCZYNOWO-SKUTKOWY â•â•â•")
        parts.append(f'{causal[:500]}')

    if info_gain:
        if parts:
            parts.append("")
        parts.append("â•â•â• INFORMATION GAIN (przewaga nad konkurencjÄ…) â•â•â•")
        parts.append(f'{info_gain[:500]}')

    return "\n".join(parts) if parts else ""


def _fmt_h2_remaining(pre_batch):
    h2_remaining = pre_batch.get("h2_remaining") or []
    if not h2_remaining:
        return ""

    h2_list = ", ".join(f'"{h}"' for h in h2_remaining[:6])
    return f"â•â•â• PLAN â•â•â•\nPozostaÅ‚e sekcje H2 w artykule: {h2_list}\nNie zachodÅº na ich tematy â€” zostanÄ… pokryte pÃ³Åºniej."


def _fmt_output_format(h2, batch_type):
    return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ tego batcha. Zaczynaj dokÅ‚adnie od:

h2: {h2}

Potem: akapity tekstu (40-150 sÅ‚Ã³w kaÅ¼dy), opcjonalnie h3: [podsekcja].
NIE dodawaj komentarzy, wyjaÅ›nieÅ„, podsumowaÅ„ â€” TYLKO treÅ›Ä‡ artykuÅ‚u."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FAQ PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_faq_system_prompt(pre_batch=None):
    """System prompt for FAQ generation."""
    base = (
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO. "
        "Piszesz sekcjÄ™ FAQ â€” zwiÄ™zÅ‚e, konkretne odpowiedzi na pytania uÅ¼ytkownikÃ³w. "
        "KaÅ¼da odpowiedÅº ma szansÄ™ trafiÄ‡ do Google Featured Snippet â€” pisz bezpoÅ›rednio i merytorycznie."
    )

    gpt_instructions = ""
    if pre_batch:
        gpt_instructions = pre_batch.get("gpt_instructions_v39", "")

    if gpt_instructions:
        return base + "\n\n" + gpt_instructions
    return base


def build_faq_user_prompt(paa_data, pre_batch=None):
    """User prompt for FAQ generation."""
    # Normalize: if paa_data is a list (raw PAA questions), wrap it
    if isinstance(paa_data, list):
        paa_data = {"serp_paa": paa_data}
    elif not isinstance(paa_data, dict):
        paa_data = {}
    paa_questions = paa_data.get("serp_paa") or []
    unused = paa_data.get("unused_keywords") or {}
    avoid = paa_data.get("avoid_in_faq") or []
    if isinstance(avoid, dict):
        avoid = list(avoid.values()) if avoid else []
    elif not isinstance(avoid, list):
        avoid = []
    instructions = paa_data.get("instructions", "")

    enhanced_paa = []
    if pre_batch:
        enhanced = pre_batch.get("enhanced") or {}
        enhanced_paa = enhanced.get("paa_from_serp") or []

    keyword_limits = {}
    if pre_batch:
        keyword_limits = pre_batch.get("keyword_limits") or {}
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_names = [s.get("keyword", s) if isinstance(s, dict) else s for s in stop_raw]

    style = {}
    if pre_batch:
        style = pre_batch.get("style_instructions") or {}

    sections = []

    sections.append("""â•â•â• SEKCJA FAQ â•â•â•
Napisz sekcjÄ™ FAQ. Zaczynaj DOKÅADNIE od:
h2: NajczÄ™Å›ciej zadawane pytania""")

    all_paa = list(dict.fromkeys(paa_questions + enhanced_paa))
    if all_paa:
        sections.append("Pytania z Google (People Also Ask) â€” to NAPRAWDÄ˜ pytajÄ… uÅ¼ytkownicy:")
        for i, q in enumerate(all_paa[:8], 1):
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text and q_text.strip():
                sections.append(f'  {i}. {q_text}')
        sections.append("Wybierz 4-6 najlepszych. MoÅ¼esz przeformuÅ‚owaÄ‡, ale zachowaj sens.")

    if unused:
        if isinstance(unused, dict):
            unused_list = []
            for cat, items in unused.items():
                if isinstance(items, list):
                    unused_list.extend(items[:5])
                elif isinstance(items, str):
                    unused_list.append(items)
            if unused_list:
                names = ", ".join(f'"{u}"' if isinstance(u, str) else f'"{u.get("keyword", "")}"' for u in unused_list[:8])
                sections.append(f'\nFrazy jeszcze nieuÅ¼yte â€” wpleÄ‡ w odpowiedzi: {names}')
        elif isinstance(unused, list):
            names = ", ".join(f'"{u}"' for u in unused[:8])
            sections.append(f'\nFrazy jeszcze nieuÅ¼yte â€” wpleÄ‡ w odpowiedzi: {names}')

    if avoid:
        topics = ", ".join(f'"{a}"' if isinstance(a, str) else f'"{a.get("topic", "")}"' for a in avoid[:8])
        sections.append(f'\nNIE powtarzaj tematÃ³w juÅ¼ pokrytych w artykule: {topics}')

    if stop_names:
        sections.append(f'\nğŸ›‘ STOP â€” NIE UÅ»YWAJ: {", ".join(f"{s}" for s in stop_names[:5])}')

    if style:
        forbidden = style.get("forbidden_phrases") or []
        if forbidden:
            sections.append(f'ZAKAZANE zwroty: {", ".join(forbidden[:5])}')

    if pre_batch and pre_batch.get("article_memory"):
        mem = pre_batch["article_memory"]
        if isinstance(mem, dict):
            topics = mem.get("topics_covered") or []
            if topics:
                topic_names = [t if isinstance(t, str) else t.get("topic", "") for t in topics[:6]]
                sections.append(f'\nTematy z artykuÅ‚u (nie powtarzaj): {", ".join(topic_names)}')

    if instructions:
        sections.append(f'\n{instructions}')

    sections.append("""
â•â•â• FORMAT â•â•â•
h2: NajczÄ™Å›ciej zadawane pytania

h3: [Pytanie â€” 5-10 sÅ‚Ã³w, zaczynaj od Jak/Czy/Co/Dlaczego/Ile]
[OdpowiedÅº 60-120 sÅ‚Ã³w]
â†’ Zdanie 1: BEZPOÅšREDNIA odpowiedÅº
â†’ Zdanie 2-3: rozwiniÄ™cie z konkretem
â†’ Zdanie 4: praktyczna wskazÃ³wka lub wyjÄ…tek

Napisz 4-6 pytaÅ„. Pisz TYLKO treÅ›Ä‡, bez komentarzy.""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# H2 PLAN PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_h2_plan_system_prompt():
    """System prompt for H2 plan generation."""
    return (
        "JesteÅ› ekspertem SEO z 10-letnim doÅ›wiadczeniem w planowaniu architektury treÅ›ci. "
        "Tworzysz logiczne, wyczerpujÄ…ce struktury nagÅ‚Ã³wkÃ³w H2, ktÃ³re pokrywajÄ… temat kompleksowo "
        "i dajÄ… przewagÄ™ nad konkurencjÄ… dziÄ™ki pokryciu luk treÅ›ciowych."
    )


def build_h2_plan_user_prompt(main_keyword, mode, s1_data, all_user_phrases, user_h2_hints=None):
    """Build readable H2 plan prompt from S1 analysis data."""
    s1_data = s1_data or {}
    competitor_h2 = s1_data.get("competitor_h2_patterns") or []
    suggested_h2s = (s1_data.get("content_gaps") or {}).get("suggested_new_h2s", [])
    content_gaps = s1_data.get("content_gaps") or {}
    causal_triplets = s1_data.get("causal_triplets") or {}
    paa = s1_data.get("paa") or s1_data.get("paa_questions") or []

    sections = []

    mode_desc = "standard = peÅ‚ny artykuÅ‚" if mode == "standard" else "fast = krÃ³tki artykuÅ‚, max 3 sekcje"
    sections.append(f"""HASÅO GÅÃ“WNE: {main_keyword}
TRYB: {mode} ({mode_desc})""")

    if competitor_h2:
        lines = ["â•â•â• WZORCE H2 KONKURENCJI (najczÄ™stsze tematy sekcji) â•â•â•"]
        for i, h in enumerate(competitor_h2[:20], 1):
            if isinstance(h, dict):
                pattern = h.get("pattern", h.get("h2", str(h)))
                count = h.get("count", "")
                lines.append(f"  {i}. {pattern}" + (f" ({count}Ã—)" if count else ""))
            elif isinstance(h, str):
                lines.append(f"  {i}. {h}")
        sections.append("\n".join(lines))

    if suggested_h2s:
        lines = ["â•â•â• SUGEROWANE NOWE H2 (luki â€” tego NIKT z konkurencji nie pokrywa) â•â•â•"]
        for h in suggested_h2s[:10]:
            h_text = h if isinstance(h, str) else h.get("h2", h.get("title", str(h)))
            lines.append(f"  â€¢ {h_text}")
        sections.append("\n".join(lines))

    all_gaps = []
    for key in ("paa_unanswered", "subtopic_missing", "depth_missing", "gaps"):
        items = content_gaps.get(key) or []
        for item in items[:5]:
            gap_text = item if isinstance(item, str) else item.get("gap", item.get("topic", str(item)))
            if gap_text and gap_text not in all_gaps:
                all_gaps.append(gap_text)
    if all_gaps:
        lines = ["â•â•â• LUKI TREÅšCIOWE (tematy do pokrycia) â•â•â•"]
        for g in all_gaps[:10]:
            lines.append(f"  â€¢ {g}")
        sections.append("\n".join(lines))

    if paa:
        lines = ["â•â•â• PYTANIA PAA (People Also Ask z Google) â•â•â•"]
        for q in paa[:8]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                lines.append(f"  â“ {q_text}")
        sections.append("\n".join(lines))

    triplet_list = (causal_triplets.get("chains") or causal_triplets.get("singles")
                    or causal_triplets.get("triplets") or [])[:5]
    if triplet_list:
        lines = ["â•â•â• PRZYCZYNOWE ZALEÅ»NOÅšCI (causeâ†’effect z konkurencji) â•â•â•"]
        for t in triplet_list:
            if isinstance(t, dict):
                cause = t.get("cause", t.get("subject", ""))
                effect = t.get("effect", t.get("object", ""))
                lines.append(f"  â€¢ {cause} â†’ {effect}")
            elif isinstance(t, str):
                lines.append(f"  â€¢ {t}")
        sections.append("\n".join(lines))

    if user_h2_hints:
        h2_hints_list = "\n".join(f'  â€¢ "{h}"' for h in user_h2_hints[:10])
        sections.append(f"""â•â•â• FRAZY H2 UÅ»YTKOWNIKA â•â•â•

UÅ¼ytkownik podaÅ‚ te frazy z myÅ›lÄ… o nagÅ‚Ã³wkach H2.
Wykorzystaj je w nagÅ‚Ã³wkach tam, gdzie brzmiÄ… naturalnie po polsku.
Nie musisz uÅ¼yÄ‡ kaÅ¼dej â€” ale nie ignoruj ich. Dopasuj z wyczuciem.

JeÅ›li fraza brzmi sztucznie jako nagÅ‚Ã³wek â€” przeformuÅ‚uj lub pomiÅ„ (trafi do treÅ›ci).

FRAZY H2:
{h2_hints_list}""")

    if all_user_phrases:
        phrases_text = ", ".join(f'"{p}"' for p in all_user_phrases[:15])
        sections.append(f"""â•â•â• KONTEKST TEMATYCZNY (frazy BASIC/EXTENDED) â•â•â•

PoniÅ¼sze frazy bÄ™dÄ… uÅ¼yte W TREÅšCI artykuÅ‚u (nie w nagÅ‚Ã³wkach).
PodajÄ™ je Å¼ebyÅ› wiedziaÅ‚ jaki zakres tematyczny artykuÅ‚ musi pokryÄ‡
i zaplanowaÅ‚ H2 tak, by kaÅ¼da fraza miaÅ‚a naturalnÄ… sekcjÄ™:

{phrases_text}""")

    fast_note = "Tryb fast: max 3 sekcje + FAQ." if mode == "fast" else "Typowo 5-10 sekcji â€” tyle ile wymaga temat."
    h2_hint_rule = ("UwzglÄ™dnij frazy H2 uÅ¼ytkownika w nagÅ‚Ã³wkach, o ile brzmiÄ… naturalnie."
                    if user_h2_hints else "Dobierz nagÅ‚Ã³wki na podstawie S1 i luk treÅ›ciowych.")

    sections.append(f"""â•â•â• ZASADY â•â•â•

1. LICZBA H2 wynika z analizy â€” ile sekcji potrzeba, by wyczerpujÄ…co pokryÄ‡ temat. {fast_note}
2. OSTATNI H2 MUSI byÄ‡: "NajczÄ™Å›ciej zadawane pytania"
3. Pokryj najwaÅ¼niejsze wzorce z konkurencji + luki treÅ›ciowe (przewaga nad konkurencjÄ…)
4. {h2_hint_rule}
5. Logiczna narracja â€” od ogÃ³Å‚u do szczegÃ³Å‚u, chronologicznie, lub problemowo
6. NIE powtarzaj hasÅ‚a gÅ‚Ã³wnego dosÅ‚ownie w kaÅ¼dym H2
7. H2 muszÄ… brzmieÄ‡ naturalnie po polsku â€” Å¼adnego keyword stuffingu

â•â•â• FORMAT ODPOWIEDZI â•â•â•

Odpowiedz TYLKO JSON array, bez markdown, bez komentarzy:
["H2 pierwszy", "H2 drugi", ..., "NajczÄ™Å›ciej zadawane pytania"]""")

    return "\n\n".join(sections)
